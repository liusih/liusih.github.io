{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Adversarial Multi-task Learning for Text Classification","text":"Reinforcement Learning中的 AC 和 A3C 总结Actor Critic在RL中的位置： 一句话概括 Actor Critic 方法: 强化学习的分类可以分为以值函数为中心的和以策略为中心的算法，交叉部分就是actor-critic。 结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率(即策略). Actor Critic 方法的优势: 可以进行单步更新, 比传统的 Policy Gradient 要快.（sih: PG是回合更新） Actor Critic 方法的劣势: 取决于 Critic 的价值判断, 但是 Critic 难收敛, 再加上 Actor 的更新, 就更难收敛. 为了解决收敛问题, Google Deepmind 提出了 Actor Critic 升级版 Deep Deterministic Policy Gradient. 后者融合了 DQN 的优势, 解决了收敛难的问题. Deep Deterministic Policy Gradient (DDPG) 就是以 Actor Critic 为基础. ACActor-Critic的形象解释为：演员在演戏的同时有评论家指点。这样演员就会演得越来越好。同时专家也在不断进步，也就是对演员的动作评价的更加的准确，共同进步，使得效果越来越好。 基于Actor-Critic策略梯度学习分为两部分内容： Critic：参数化动作价值函数 $Q_w(s, a)$ Actor：按照Critic部分得到的价值引导策略函数参数 $\\theta $ 的更新。 Critic做的事情：策略评估，他要告诉个体，在由参数 $\\theta$ 确定的策略 $\\pi_{\\theta}$ 到底表现得怎么样。 A3CA3C-Asynchronous Advantage Actor-Critic 未完待续… Reference： [1] https://zhuanlan.zhihu.com/p/32596470","link":"/2019/10/09/A3C/"},{"title":"Adversarial Multi-task Learning for Text Classification","text":"创新点： Adversial learning + Orthogonality Constraints Abstract 背景： 神经网络模型已经表明了它们在Multi-task Learning中的广阔前景，其重点是学习共享层以提取公共的和随不同任务不变的特征。 存在的问题： 但在大多数现有方法中，提取的 shared features易受到特定于任务的功能或其他任务带来的噪音的污染。 本文的方法： 本文提出了一种adversarial multi-task learning frame-work，以减轻共享的和私有的潜在特征空间的相互干扰。 对16种不同的文本分类任务进行了广泛的实验，这证明了方法的正确性。 此外还表明，提出的模型所学习的共享知识可以视为现成的(off-the-shelf)知识，并且可以轻松地迁移到新任务中。 Related Intro Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. 存在的问题是， 目前多任务学习的方法尝试在不同的task中分离出 private和shared space， 只是基于一些应该shared的components. 图1-(a)所示，一般的共享-私有模型为任何任务提供了两个特性空间:一个用于存储任务相关的特性，另一个用于捕获共享的特性。该框架的主要限制是共享的特性空间可能包含一些不必要的特定于任务的特性，而一些可共享的特性也可能混合在私有空间中，存在特性冗余。 为了解决这个问题，本文提出了一种对抗式多任务框架，其中通过引入正交约束(orthogonality constraints)，使得共享和私有特征空间固有地不相交。 具体来说，我们设计了一个通用的共享-私有学习框架来对文本序列进行建模。 为了防止共享的和私有的潜在特征空间相互干扰，引入了两种策略：对抗训练和正交约束。 对抗训练用于确保共享特征空间简单包含公共和任务不变信息，而正交性约束则用于从私有空间和共享空间中消除冗余特征。 本文主要的contributions: 提出的模型以更精确的方式划分特定于任务的空间和共享的空间，而不是粗略地共享参数。 我们将原来的二元对抗性训练扩展到多类，这样不仅可以联合训练多个任务，还可以利用未标记的数据。 我们可以将多个任务之间的共享知识压缩成一个现成的神经层，它可以很容易地转移到新的任务中。 Method给定一个文本序列x = {x1，x2，···，xT}，我们首先使用lookupup层获取每个单词xi的向量表示（em-bedding）xi。 最后时刻T的输出可以看作是整个序列的表示，它具有一个完全连接的层，后跟一个softmax非线性层，用于预测类别的概率分布： loss 函数： $C$ is the class number. Text Classification with LSTM 多任务学习的目的是利用这些相关任务之间的相关性，通过并行学习任务来改进分类。设置对于task $k$, $D_k$是一个有$N_k$个样本的数据集 $D_k$ . Fully-Shared Model (FS-MTL) 在完全共享模型中，我们使用单个共享LSTM层提取所有任务的功能。 例如，给定两个任务m, n，它认为task m的功能可以完全与task n共享，反之亦然。 该模型忽略了某些特征依赖于task。 图2-a说明了完全共享的模型。 Shared-Private Model (SP-MTL)共享-私有模型为每个任务引入了两个特征空间：一个用于存储 task-dependent 的特征，另一个用于捕获 task-invarian的特征。 因此，我们可以看到每个任务都被分配了一个私有LSTM层和一个共享LSTM层。 正式地，对于task k中的任何句子，我们都可以计算其共享表示( shared rep-resentation) $s_t^k$ ，黄色区域，和任务特定表示(task-specific representation)$h_k$，灰色区域，如下： Task-Specific Output Layer 对于task $k$ 中的一个句子来说，它的特征 $h^{(k)}$ 是由深层的多任务体系结构发出的，最终被送入相应的 task-specific的softmax层 中进行分类或其他任务。损失行数为计算交叉熵， loss $L_{task}$可以计算为: Incorporating Adversarial Training虽然共享-私有模型不能保证共享的特征不在私有特征空间中存在，反之亦然。因此，在共享-私有模型中，一些有用的共享特性可能被忽略，而共享的特性空间也容易受到一些特定于任务的信息的污染。 良好的共享特征空间应包含更多公共信息，而没有特定于任务的信息。 为了解决这个问题，我们将对抗训练引入了多任务框架，如图3（ASP-MTL）所示。 Adversarial Network 对抗网络的目标是学习到符合真实数据 $P{data}(x)$ 分布的生成分布 $p{G}(x)$。GAN 学习一个生成网络 $G$ 和一个判别网络 $D$, $G$ 通过生成分布 $p{G}(x)$ 生成样本，$D$ 学习决策一个样本是来自$p_G(x)$ 还是$P{data}(x)$ The goal is to learn a generative distribution $p{G}(x)$ that matches the real data distribution $P{data}(x)$Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk.","link":"/2019/10/09/Adversarial Multi-task Learning for Text Classification/"},{"title":"Curiosity-driven Exploration by Self-supervised Prediction","text":"Abstract在现实世界中自监督预测的好奇心驱动的探索，能够提供给agent的奖励非常少，或者根本不存在。在这种情况下，好奇心可以作为一种内在的奖励信号，使agent能够探索环境并学习可能在以后的生活中有用的技能。本文把好奇心表述为一个agent在视觉特征空间预测自身行为结果的能力时的误差，并把这种error作为内在好奇心reward提供给agent进行学习，该视觉特征空间是由一个自监督逆动态模型学习的。 我们的公式扩展到像图像这样的高维连续状态空间，绕过了直接预测像素的困难，并且，关键的是，忽略了不能影响代理的环境方面。所提出的方法在两个环境中进行评估（VizDoom和Super Mario Bros）。研究了三个广泛的环境:1)稀疏的外部奖励，在这种奖励中好奇心使得与环境的互动更少以达到目标；2)没有外在回报的探索，好奇心驱使代理人更有效地探索；和3)推广到看不见的sce-narios(例如，同一游戏的新级别)，在这里，从早期经验中获得的知识帮助代理人比从头开始更快地探索新的地方。 Fig2 看图是两个loss，一个是Forword model 用$at$和$φ(s_t)$ 去预测$\\hat{φ}(s{t+1})$， 一个是Inverse model 用$st$和$s{t+1}$预测所采取的动作$a_t$。 Self-supervised prediction for exploration自监督的探索目标不是手动设计每个环境的特征表示，而是提出一种学习特征表示的通用模型，使得学习特征空间中的预测误差提供良好的内在奖励信号。我们提出这样一个特征空间可以通过训练具有两个子模块的深层神经网络来学习:第一子模块将原始状态编码为特征向量φ(st)，第二子模块将编码两个后续状态的$φ(st)$、$φ(s{t+1})$的特征作为输入，并预测代理从状态$st$到$s{t+1}$移动所采取的动作($a_t$)。训练这个神经网络相当于学习功能，定义为： 其中$\\hat{a_t}$是对动作$a_t$的预测估计值，并且训练了神经网络参数θ来进行优化。 其中，$L_I$是损失函数，用于衡量预测的action与实际action之间的差异。在$a_t$离散情况下，输出g是在所有可能的动作上均具有最大柔和的分布，并且在多项式分布下将θ的最大似然估计的数量最小化。学习的功能也称为逆动力学模型，元组（st，at，st + 1）要求在代理使用其当前策略与环境交互时获得的学习信息。 除了逆动力学模型(inverse dynamics model)外，还训练了另一个神经网络，该神经网络以$a_t$和$φ(s_t)$作为输入，并预测时间$t + 1$处状态的特征编码。 其中，$\\hat{φ}(s{t+1})$是通过最小化损失函数$L_F$而预测的$φ(s{t+1})$ ,应用神经网络参数$θ$预测估计值: 学习的函数$f$也称为正向动力学模型, 内在奖励信号$r^i_t$计算为: 其中$η&gt; 0$是比例因子。 为了生成基于好奇心的内在奖励信号，我们共同优化了方程3和5中分别描述的正向f和逆向g动力学损失。 逆模型学习一个特征空间，该特征空间对只与预测代理的行为有关的信息进行编码，而正向模型则在该特征空间中进行预测。 The inverse model learns a fea-ture space that encodes information relevant for predictingthe agent’s actions only and the forward model makes pre-dictions in this feature space. 本文将提出的好奇心函数称为 Intrinsic Curiosity Module（ICM）。由于没有动力诱使该特征空间对那些不受代理行为影响的任何环境特征进行编码，因此该代理将不会因无法预测的环境状态而获得任何奖励，它的探测策略将对干扰物的存在、光照的变化或环境中其他讨厌的变化来源具有很强的适应性。 参见图2的说明，Agrawal 在2016年构造了一个joint inverse-forward model 来学习推物体的特征表示。 但是，他们只是将forword model用作训练inverse model 特征的正则化函数，而本文将forword model预测中的error用作训练agent policy的好奇心奖励。agent所解决的总体优化问题是由公式1、3和5组成，可以写成： 其中0≤β≤1是权重，其权重是逆模型损失与正向模型损失之间的关系，而λ&gt; 0 标量是权衡策略梯度损失的重要性与学习内在奖励信号的重要性之间的关系。 Experiment本文定性和定量地评估了在有和没有内在好奇心信号的情况下，在两种环境下（VizDoom和Super Mario Bros）学习策略的性能。 评估了三个广泛的设置：a）达成目标时稀疏的外在奖励； b） 无外部奖励的探索； c）本文提出的新的场景。 Future work未来研究的一个有趣方向是将学习到的探索行为/技能用作更复杂的分层系统中的原始/低级策略。 例如，本文的VizDoomagent学会沿着走廊行走，而不是撞到墙壁上。 这对于导航系统可能是一个有用的原语。尽管丰富多样的现实世界为交互提供了充足的机会，但奖励信号却很少。 本文的方法在这种情况下表现出色，并将影响代理的意外互动转化为内在奖励。 但是，本文的方法并未直接扩展到“互动机会”都很少见的情况。 在理论中，可以将此类事件保存在 replay memory中，并使用它们来指导探索。 但是，本文将此扩展留给以后的工作。","link":"/2019/10/25/Curiosity-driven Exploration by Self-supervised Prediction/"},{"title":"ConvLab - Multi-Domain End-to-End Dialog System Platform","text":"微软和清华在对话系统领域的合作：ConvLab - Multi-Domain End-to-End Dialog System Platform 提出ConvLab—多领域端到端对话系统平台，它一方面提供一些可复用的组件来帮助实验人员快速实验，另一方面还可基于它在通用环境中对比大量不同方法（包括从pipeline到完全端到端好几个层级的方法，后面会细说）。另外，ConvLab还提供了一些完全标注的数据集和一些相关的预训练模型。作为一个演示，我们使用用户对话行为标注扩展了multiwoz数据集(记得好像是EMNLP2018 best paper提出来的，不仅有数据还有相应的任务)用来训练系统的所有组件，同时演示convlab如何使在多领域的端到端对话中进行复杂实验变得简单和高效。 本文主要贡献包括：ConvLab是第一个涵盖多个可训练模型和相关标注数据集的开源多领域端到端对话系统；ConvLab提供一套丰富的工具和组件用于开发不同类型的对话系统，使研究人员能够在相同条件下比较不同的方法；ConvLab通过人工和仿真器提供端到端的评估；ConvLab作为DSTC8的标准平台。 Anyway， 期待！","link":"/2019/04/25/ConvLab/"},{"title":"Curriculum Learning and self-paced learning","text":"1. Curriculum LearningBengio提出了课程学习的概念，来自于人类从易到难的学习过程：将数据看作模型所学习的内容的话，我们自然需要一个合理的课程(curriculum)来指导模型学习这些内容的方式。在课程学习中，数据被按照从易到难的顺序逐渐加入模型的训练集，在这个过程中训练集的熵也在不断提高(包含更多的信息)。 A model is learned by gradually including from easy to complex samples in training so as to increase the entropy of training samples. 课程学习的核心问题是得到一个ranking function，该函数能够对每一条数据样本给出其learning priority (学习该样本的优先程度)。一个确定的ranking function便能够确定一个“课程”，有更高rank的数据会被较早地学习。 一般特定问题的ranking function是靠直觉来定义的，这样的定义方法可能会存在偏差，不够科学，所以引出了self-paced learning。 2. Self-paced learningKumar在2010年提出self-paced learning，通过模拟认知机制，通过先学习简单的、普适性的知识，然后逐渐增加难度，学习更复杂、更专业化的知识课程学习。自步学习从选取最简单的数据子集开始，逐渐加入复杂的数据，从而减少熵值，训练出潜在的权重参数。基本思路是利用损失大小与难易程度之间的对偶关系进行对所学样本的加权。这种加权格式类似于引入隐含变量后的估计推断格式和EM算法格式，使得模型对于数据分布的学习更加稳健 SPL 通过在损失函数中引入变量 来表示样本是否被选择： SPL将curriculum设计为学习的目标函数中的一个正则化项，SPL的优化目标即优化得到一组模型参数w与权重向量v，使得所有数据的加权Loss之和与负的权重向量v的L1范数最小(也存在同时使用L1L2范数的变体[3])。其中 λ 是一个控制参数，反映了当前学习所进行到的阶段(learning pace)， 是调整步速的关键变量， 越小，表明模型还在训练的初始阶段，训练相对简单的样本，随着 增大，模型学习越来越多的样本。 直观来解释一下权重向量v，最常用的是binary (0/1)的形式：其实就是是否选择该数据进入训练/验证集。note that在Loss项前乘了因子vi，所以对于第i条数据，若vi=0则其产生的Loss对于该步的模型参数w更新没有影响。 Reference Bengio, Yoshua, et al. “Curriculum learning.” Proceedings of the 26th annual international conference on machine learning. ACM, 2009. Kumar, M. Pawan, Benjamin Packer, and Daphne Koller. “Self-paced learning for latent variable models.” Advances in Neural Information Processing Systems. 2010. https://zhuanlan.zhihu.com/p/55720313 https://zhuanlan.zhihu.com/p/54025612","link":"/2019/11/01/Curriculum Learning/"},{"title":"Domain Adaptive Dialog Generation via Meta Learning","text":"应用meta learning进行对话系统的领域适配，在MAML基础上提出DAML，未开源代码，MAML作为meta learning的具体方法之一，值得学习。 来源： ACL 2019 原文链接： https:// arxiv.org/abs/1906.0352 0 有些格式没来得及转换，可以直接看我的知乎 https://zhuanlan.zhihu.com/p/81715745 Introduction问题领域适配是对话系统中的一项重要任务，因为根据不同的需求，每天都有许多新的对话任务产生，为这些新任务收集和注释训练数据的成本很高，因为它涉及到与真实用户的交互。因此将现有的丰富资源领域的数据适应于资源有限的新领域是对话系统研究中的一项重要任务。 现有解决方法 ：Transfer learning、few-shot learning、meta-learning等介绍了如何解决机器学习中的数据稀缺问题。由于每个对话域之间的差异很大，因此将信息从富资源域泛化到另一个低资源域非常困难。 本文的方法 ：所以本文提出了一种基于元学习（meta-learning）的领域自适应对话生成方法DAML。DAML是一个端到端可训练的对话系统模型，它从具有丰富资源的领域任务中学习，然后用最少的训练样本适应到新的领域。通过将模型无关的元学习算法（ model-agnostic meta-learning MAML）应用于对话域，利用多个单域对话数据和丰富的资源来训练对话系统模型，将训练中的多个对话任务结合起来，学习适用于新领域的通用和可转移信息。使得该模型仅通过少量的训练实例，就能在一个新的领域有效地学习一个具有竞争力的对话系统。 meta-learning 即元学习，也可以称为“ learning to learn”。常见的深度学习模型，目的是学习一个用于预测的数学模型。而元学习面向的不是学习的结果，而是学习的过程。其学习的不是一个直接用于预测的数学模型，而是学习“如何更快更好地学习一个数学模型”。 MAML （ model-agnostic meta-learning ）的思想在于学习一个好的初始化权重，从而在新任务上实现fast adaptation，即训练好的模型能够在小规模的训练样本上迅速收敛并完成fine-tune。MAML算法试图建立多任务的通用内部表示，并在应用于新任务时最大限度地提高损失函数的灵敏度，从而当参数有微小的更新能够极大地提高新任务的损失值。这也使得对话系统不仅是只需要较少的目标域数据，而且能够以更有效的方式成功地适应新的领域。 徐安言：Model-Agnostic Meta-Learning （MAML）模型介绍及算法详解 具体来说，本文使用三个域: 餐馆、天气和公共汽车信息搜索 作为源数据，目标域则为 电影信息搜索 。通过更改seq2seq编解码器网络模型 Sequicity ，对其两阶段的CopyNet进行改进，通过实现MAML算法，使用源域的对话数据来实现最优初始化。然后使用常规梯度下降，用最少的对话框数据对目标域的初始化进行微调。最后，利用目标域的测试数据对自适应模型进行了评价。本文超越了最先进的zeroshot的基线， ZSDG (Zhao andEskenazi, 2018)，以及其他迁移学习方法。 Problem Formulation本文基于seq2seq的对话模型将对话上下文 $\\displaystyle c$ 作为输入，并生成一个句子 $\\displaystyle r$作为响应。 K个不同的source domain有丰富的数据, 则我们在每个源域 $\\displaystyle S_k$ 中都有训练数据，记作： 我们也将target domain T中的数据表示为： 在训练过程中，生成模型： 其中 $\\displaystyle C$ 是上下文， $\\displaystyle R$ 是系统响应。 为了进行领域的适应，对模型 $\\displaystyle M{source}$ 进行微调 ,利用的是目标域训练数据 $\\displaystyleD{train}^{T}$ ，得到一个新的模型 $\\displaystyle M_{target}$。目标是学习到一个模型，可以在新的目标领域表现良好: Proposed Methods 首先介绍如何将MAML算法与序列模型相结合。如图1所示，典型的梯度下降包括 : (1)初始化模型上训练更多的数据 (2)计算目标损失函数 (3)利用损失更新模型参数。 然而，对于MAML，有两个梯度更新步骤。 图中的序号表示执行的先后顺序， $\\displaystyle M$ 为初始化模型，k=1，2，3分别代表不同的sourcedomain，可以看到MAML采用了两次梯度更新（local和global）： 1）先结合源域训练数据和初始模型，更新一步，计算loss, 得到一个临时model $\\displaystyle M_{k}^{‘}$ ； 2）再从每个域及其对应的临时更新的域模型 $\\displaystyle M_{k}^{‘}$ 中计算出每个域新的meta-learning loss 3）然后将所有的新域损失求和得到最终的损失； 4）最后，使用final loss来更新原始模型M。 不同于常见的梯度,在MAML中,用来更新模型的目标损失函数并不直接从当前模型 $\\displaystyle M$ 计算, 而是从临时$\\displaystyle M_{k}^{‘}$ 模型计算。此操作背后的idea是,从更新的模型计算损失函数对原始域的变化更敏感,可以了解更多关于源域的常见的内部表示，而不是每个领域的独有的特征。然后在自适应阶段，由于基本的内部表示已经被捕获，模型对新域的独特特征就会非常敏感。因此，只需要一个或几个梯度步骤和最少的数据量来优化模型到新的领域。 接下来具体描述MAML算法和Sequicity模型结合的实现细节。如算法1所示，Sequicity模型以seq2seq的方式将自然语言理解、对话管理和响应生成结合起来，而元学习是一种调整损失函数值以实现更好优化的方法。 Sequcity模型 是在单一的seq2seq模型的基础上构造的，该模型结合了copying mechanism和 belief span 来记录对话状态。为了使用Sequcity模型，在 $\\displaystyle t$ 时刻将 $\\displaystyle c$ 用 $\\displaystyle\\{B{t - 1},R{t - 1},Ut\\}$ 来表示，其中 $\\displaystyle B{t - 1}$ 为在$\\displaystyle t - 1$ 时刻的belief span， $\\displaystyle R_{t - 1}$ 为上一次的系统响应，$\\displaystyle U_t$ 是当前用户的话语。Sequicity模型引入了belief span来存储所有informslot的值，并在历史中记录requestslot的名称。通过这种方式，不需要将所有的历史语句放入一个RNN中来提取上下文特征，而是直接将存储在beliefspan中的slot作为所有历史上下文的表示。belief span更准确、更简单地代表历史语境，需要在每一个环节进行更新。 Sequcity模型的具体过程是,给定上下文 \\displaystyle c- >\\\\{B_{t - 1},R_{t - 1},U_t\\\\} ,$\\displaystyle t$ 时刻的belief span $\\displaystyle Bt$ 则是根据在 $\\displaystylet - 1$ 时刻的 $\\displaystyle B{t - 1}$ , $\\displaystyle R_{t - 1}$ ，以及$\\displaystyle t$ 时刻的用户话语 $\\displaystyle U_t$ 提取的： 然后，根据之前提取的上下文和 $\\displaystyle t$ 时刻belief span生成系统响应 $\\displaystyle R_t$ : $\\displaystyle m_t$ 是一个帮助生成响应的简单标签。它检查存储在Bt是否有在数据库中有可用的信息。 将Sequcity模型与MAML算法结合： 由于MAML与任何基于梯度下降的模型兼容，所以将当前生成对话模型表示为 $\\displaystyle M$ , 可以随机初始化。根据该算法，对每个源域$\\displaystyle S_k$ 进行一定大小的训练数据采样 $\\displaystyle (c^{(k)}, r^{(k)}) $。在Sequicity模型中，输入训练数据 $\\displaystyle (c^{(k)}, r^{(k)})$ ，得到生成的系统响应,采用交叉熵作为所有域的损失函数： 对于每个源域 $\\displaystyle Sk$ ，使用梯度下降来更新并得到一个临时模型 $\\displaystyle M{k}^{‘}$ ： 为了与(Finn et al.， 2017)文章保持一致，本文只更新了一步，这样在每个源域中都有一个更新的模型，且距离 $\\displaystyle M$只有一步。在以后的工作中，可以考虑多个步骤 的梯度更新。然后，根据更新的模型计算损失函数（其中每个源域中训练数据相同）: 在这一步之后，在每个域中都有元损失值。将所有源域的更新损失值汇总为meta learning的目标函数: 最后对模型进行更新，使元目标函数最小化： Experiment为了与最先进的领域自适应算法ZSDG (Zhao和 Eskenazi ,2018)相比较，本文使用SimDial数据集，该数据集首次用于评估ZSDG。 SimDial中总共有6个对话域:restaurant、weather、bus、movie、restaurant-slot和restaurant-style，本文选择restaurant、weather、bus作为源域，对于每个源域，有900、100、500个对话分别对应训练、验证和测试，每个会话有9个回合。其余三个域用于评估，它们被视为目标域。生成9个对话(占源域的1%)对每个域进行适应训练(finetune)，平均每个域包含约8.4轮。对于测试，为每个目标模型使用500个对话,其中选择Movie作为新的评价目标域，是因为电影有完全不同的NLG模板和对话结构，几乎没有共同的特点。 Metrics 实验中有三个主要指标: BLEU分数，entity F1 score和adapting time。 前两个指标是Finn等人(2017)使用的具有说服力的指标，证明了MAML对新任务的快速适应速度。 使用BLEU评分, 是因为生成自然语言也是任务的一部分，因此评估生成的反应句的质量。 每个对话的实体F1得分为，比较生成的belief span，因为belief span包含所有限制响应的槽，所以这个分数也会检查任务的完整性。 领域适应时间，在适应训练过程中，计算epochs的数量。 表1: 该模型在训练中对餐厅领域进行了额外的微调。“In Domain”使用所有三个源域 (餐厅、天气、公交车)，而“NewDomain”指的是电影领域。当给定相似的目标域数据时，DAML的性能优于ZSDG和迁移学习。 Conclusion and Future Work本文提出了一种基于元学习的领域自适应对话生成方法。构建了一个端到端可训练的对话系统，该系统利用两步梯度更新来获得对新领域更敏感的模型。在一个具有多个独立域的模拟数据集上评估模型。与zero-shot学习方法和迁移学习方法相比，DAML在实体F1中达到了最先进的性能。因此认为DAML是一种有效的、鲁棒的低资源对话系统训练方法。 DAML还提供了很有潜力的扩展，例如将DAML应用于基于增强学习的对话系统。文中还计划使DAML适应多域对话任务。 Reference Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks https:// arxiv.org/pdf/1703.0340 0.pdf Zero-Shot Dialog Generation with Cross-Domain Latent Actions https:// arxiv.org/pdf/1805.0480 3 Sequicity: Simplifying Task-oriented Dialogue Systems with SingleSequence-to-Sequence Architectures https://www. comp.nus.edu.sg/~kanmy/ papers/acl18-sequicity.pdf 再次post一下我滴某乎~https://zhuanlan.zhihu.com/p/81715745","link":"/2019/09/09/Domain Adaptive Dialog Generation via Meta Learning/"},{"title":"MSR任务型对话系统论文列表","text":"The paper list of MSR in task-oriented dialog system Budgeted Policy Learning for Task-Oriented Dialogue Systems ACL 2019 https://arxiv.org/pdf/1906.00499 在DDQ基础上的改进，增加一个BSC模块，DDQ中的world model模块没有改进，sih认为也是用于改进训练数据的质量。 ConvLab: Multi-Domain End-to-End Dialog System Platform 2019.4 提出ConvLab对话系统模块集成的平台 GitHub链接：https://github.com/ConvLab/ConvLab https://arxiv.org/pdf/1904.08637 Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy Learning AAAI209 https://arxiv.org/pdf/1811.07550 加一个Switch，也是在生成质量上的改进，调整用于训练的数据的比例。 Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning ACL2018 https://arxiv.org/pdf/1801.06176 基于model based的模型Dyna-Q (DQN的一种改进), 引入world model用来生成模拟的用户经验。 Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning EMNLP 2018 https://arxiv.org/pdf/1808.09442 D3Q在DDQ基础上的改进, 在world model 后加判别器，模拟器生成的经验质量好的话再给agent作为训练数据 Subgoal Discovery for Hierarchical Dialogue Policy Learning EMNLP 2018 https://arxiv.org/pdf/1804.07855 应用层级神经网络，提出SDN网络模型，把对话任务分解成subgoal，完成travel planning。 Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning https://arxiv.org/pdf/1704.03084 上一篇文章的开端，EMNLP是在这一篇基础改进的。 Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning ICASSP 2018 https://arxiv.org/pdf/1710.11277 A3C, Adversarial 的 AC End-to-End Task-Completion Neural Dialogue Systems https://arxiv.org/pdf/1703.01008 Xiujun Li最早提出的任务型对话系统的pipeline 模型。 A User Simulator for Task-Completion Dialogues https://arxiv.org/pdf/1612.05688 Xiujun Li 最早提出的User simulator.","link":"/2019/11/01/MSR任务型对话系统论文列表/"},{"title":"Today's RL algorithm","text":"Double DQN : Deep Reinforcement Learning with Double Q-learning Deep Deterministic Policy Gradient (DDPG): Continuous control with deep reinforcement learning Continuous DQN (CDQN or NAF): Continuous Deep Q-Learning with Model-based Acceleration Dueling network DQN (Dueling DQN) ： Dueling Network Architectures for Deep Reinforcement Learning Deep SARSA ： [10] Asynchronous Advantage Actor-Critic (A3C) ： Asynchronous Methods for Deep Reinforcement Learning Proximal Policy Optimization Algorithms (PPO)： Proximal Policy Optimization Algorithms Cross-Entropy Method（CEM）: Learning Tetris Using the Noisy Cross-Entropy Method 1. Double DQNQ值函数的定义:在策略π下，状态s，采取动作a， 所得到的Q值为： 最优值就是: Q^*(s, a) = max_π Q_π(s, a)最优策略就可以通过在每个状态选择最高值的行动给出。则目标 DQN 算法DQN 算法的两个最重要的特点是目标网络 (target network) 和经验回顾 (experience replay)。目标网络，其参数为 θ^-，和 online的网络一样，除了其参数是从 online network 经过某些steps之后拷贝下来的。 θ^-_t = θ_t目标网络是： Double Q:在标准的 Q-学习和 DQN 中的 max 操作用相同的值来选择和评价一个 action，这实际上更可能选择过高的估计值，从而导致过于乐观的值估计overestimated values。为了避免这种情况的出现，我们可以对选择selection 和衡量evaluation进行解耦。这就是 Double Q。 reference: Double DQN Background论文（Hasselt等人）发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。作者在他2010年提出的Double Q-Learning的基础上，将该方法引入了DQN中。具体操作是对要学习的Target Q值生成方式进行修改，原版的DQN中是使用TargetNet产生Target Q值，即 Q_T =r+\\gamma\\max_{a'} Q (s',a';\\theta_{i}^-)其中$θ^-$是TargetNet的参数。 Steps 在DDQN中，先用MainNet找到 $\\max{a’}Q(s’,a’;\\theta{i})$的Action ($θ_i$是MainNet的参数) 再去TargetNet中找到这个Action的Q值来去构成Target Q值，这个Q值在TargetNet中不一定是最大的，因此可以避免选到被高估的次优Action。最终要学习的Loss Function为： L(\\theta)=E[(Q_T-Q(s,a;\\theta_i))^2此时的Target Q 变为: Q_T = r+\\gamma Q (s',\\max_{a'} Q(s',a';\\theta_i);\\theta_{i}^-)除此之外，其他设置与DQN一致。实验表明，DDQN能够估计出更准确出Q值，在一些Atari2600游戏中可获得更稳定有效的策略。 reference: Doule DQN 2. Dueling-DQN Background Wang等人提出了一种竞争网络结构（dueling network）作为DQN的网络模型。 Structure 第一个模型是一般的DQN网络模型，即输入层接三个卷积层后，接两个全连接层，输出为每个动作的Q值。 第二个模型—竞争网络（dueling net）将卷积层提取的抽象特征分流到两个支路中： 上路代表状态值函数 $V(s)$，表示静态的状态环境本身具有的价值 下路代表依赖状态的动作优势函数$A(a)$（advantage function），表示选择某个Action额外带来的价值。 最后这两路再聚合再一起得到每个动作的Q值。 论文作者认为，V(s)更专注于环境，A(a)更专注与动作本身对结果的影响，以车的游戏做了举例说明 公式 状态价值函数表示为 V(s;θ,β)动作优势函数表示为 A(s,a;θ,α)动作Q值为两者相加 Q(s,a;θ,α,β)=V(s;θ,β)+A(s,a;θ,α) 其中 θ 是卷积层参数，β 和 α 是两支路全连接层参数。 而在实际中，一般要将动作优势流设置为单独动作优势函数减去某状态下所有动作优势函数的平均值 Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta)+(A(s,a;\\theta,\\alpha)-\\frac{1}{|A|}\\sum_{a'}A(s,a';\\theta,\\alpha)) 这样做可以保证该状态下各动作的优势函数相对排序不变，而且可以缩小 Q 值的范围，去除多余的自由度，提高算法稳定性。 reference：Dueling-DQN 3. Prioritized Experience Replay简单来说，经验池中TD误差 r+\\gamma\\max_{a'}Q(s',a';\\theta^-)-Q(s,a;\\theta)绝对值越大的样本被抽取出来训练的概率越大，加快了最优策略的学习。 4. H-DQNIntroduction hierarchical deep reinforcement learning, model-free, value-based 当环境反馈是sparse和delayed时，解决方法是构造一个两个层级的算法，将困难问题分解为多个小目标，逐个完成。 Procedure以一个两层级的算法为例： 一个层级叫做meta-controller，它负责获取当前状态$st$ ，然后从可能的子任务里面选取一个子任务 g_t \\in \\mathcal{G} 交代给下一个层级的控制器去完成。它是一个强化学习算法，其目标是最大化实际得到的extrinsic reward之和，$F_t = \\sum{t’=t}^\\infty \\gamma^{t’-t} f_{t’}$ 。 在这里，这一层使用的是DQN方法，这一层Q-value的更新目标是: ps: 按照sih的理解，通俗来讲就是，状态s和作为该层级的动作-&gt;子任务$g$,以及外置奖励(此处的外置奖励来源于外部环境)进行一次DQN的学习。 另一个层级叫做controller，它负责接收上一个层级的子任务 $g$ 以及当前的状态 $st$ ，然后选择一个可能的行动 $a_t$ 去执行。它也是一个强化学习算法，其目标是最大化一个人为规定的critic给出的intrinsic reward之和，$R_t(g) = \\sum{t’=t}^\\infty \\gamma^{t’-t} r_{t’}(g)$。这里也使用DQN方法，更新目标为: ps: 此步则是以上一层级的子任务g作为先验，将s,a,g,以及内置奖励（此处的内置奖励来源于critic）进行DQN的学习。 算法框架","link":"/2018/11/23/RL-algrithm/"},{"title":"Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue","text":"EMNLP 2019 任务型对话整理 创新点：将NLG的响应生成与语言模型结合，共享encoder，作为多任务学习。 Abstract在面向任务的对话中，自然语言生成(NLG)是生成面向用户系统话语的最后一步。NLG的结果直接关系到对话系统的质量和可用性。尽管大多数现有的系统在给出目标时都能给出正确的响应，但它们很难与人类语言的多样性和流利性相匹配。在本文中提出了一个新的多任务学习框架，NLG-LM，用于自然语言生成。除了生成传递所需信息的高质量响应外，它还通过一个未配置的语言模型明确地针对生成响应的自然相关性。这对提高人类语言的风格和多样性有重要意义。实验结果表明，该多任务学习框架在多数据集的学习效果优于已有的多任务学习框架。例如，它将以前E2E-NLG数据集上的最佳BLEU评分提高了2.2%，将笔记本数据集上的最佳BLEU评分提高了6.1%。 IntroChallenge: 由于NLG直接面向用户，其可读性和信息量直接影响用户对整个对话系统的感知。一方面，为了提供或请求用户的信息，响应必须包含所需的信息，即meaning representation(MR)。另一方面，系统响应需要模仿人类语言的流动性和变异性来提高用户体验。早期的工作主要采用预定义的规则语法，尽管这些框架可以提供足够的信息，但是它们缺乏语言的自然性和变化性，使得响应非常僵化。此外，这些方法通常需要费力气的手工工作来创建模板，从而使它们无法跨域扩展。 Method: 由于面向任务的对话数据通常包含丰富的人类响应，因此利用语言建模技术提高NLG模型模拟人类语言的能力具有很大的潜力。 meaning representation(MR) 本文方法： 本文提出了一个多任务方案来解决面向任务的对话中的自然语言生成问题。对于NLG任务，我们使用了一个equence-to-sequence的框架。解码器使用一种attention机制传送信息，从编码器处编码的一个MR序列。因此，NLG任务生成响应的条件是输入MR。 我们工作的主要贡献是将语言建模任务整合到人为生成的响应中，作为无条件的补充过程，引入更多与语言相关的元素，而无需干预MR信息。此外，语言建模的非监督性质意味着我们不需要额外的标记数据。因此，在多任务学习框架下，我们同时训练NLG和语言建模任务。为了促进多任务学习，我们在解码器中执行语言建模任务，并且部分共享来自NLG任务的参数。 为了评估模型NLG-LM的有效性，本文对5个任务导向的对话任务进行了评估: E2E-NLG、电视、笔记本电脑、酒店和餐厅数据集。NLG-LM在所有5个数据集上达到了新的SOTA。例如，它超过了最优秀的E2E-NLG竞争对手Slug，在BLEUscore的成绩为2.2%。消融研究表明，在训练过程中引入语言建模任务可以使BLEU分数平均提高2.4%。 Problem setting Pipeline 模型，根据DM生成的action（槽值对），本文叫MR（。Q_Q。），来生成自然语言。 在面向任务的对话中，自然语言生成(NLG)过程是将系统话语作为自然语言生成，给定系统生成的意义表示MR，这些表示来自pipeline模型之前的步骤。每个MR是一个槽值对，槽表示要传递的信息的类别，值表示内容。 Dialogue acts被用来区分不同类型的系统动作。典型的对话行为包括inform, request, confirm。对于给定的意义表示，NLG过程应该为不同的对话行为生成不同的话语。例如，confirm dialogue act通常会导致系统以“Let me confirm” or “Correct me if I’m wrong” 开始。 给定训练数据${（di，ri，ui）}$，其中di是对话行为，$ri = {（s1，v1），（s2，v2），…，（sk，vk）}$是含义表示的集合，ui是样本是由人类标记者产生的话语，目的是通过一对新的对话行为d和意义表示r来产生话语u。 地点等实体用特殊token代替。 Model先将对话动作d和意义表示r连接在一起作为单个输入序列I，有m个token。从给定的话语u中获得输出序列O，有n个token。两个序列都被去词化。在每个序列周围放置特殊的句子标记和。 模型训练的目标是在给定先前预测的token和输入序列的前提下，一次生成一个输出token。 可以将其建模为最大化条件概率分布： Encoder:我们训练一个dictionary D来将每个token映射到一维定长向量。 输入的embedding序列然后进入双向RNN层以生成contextualized embedding。 我们使用GRU作为RNN单元，并汇总前向和后向RNN输出。 编码器的输出用$（u1，…，um）∈R^{d_h×m}$表示，其中$d_h$是RNN的输出尺寸，m是输入序列长度。 Decoder：解码器采用具有注意力机制的RNN，一次生成一个令牌。它从 beginning-of-sentence 的 token开始，并使用encoder RNN的最终隐藏状态作为初始隐藏状态。 在第t步中，我们使用编码器中相同的字典D，将第t个输出token映射到向量$s_t$, 并且dropout。 然后，给定先前的隐藏状态$h_{t-1}$，解码器首先计算编码器输出的注意力权重： 然后将权重$α$应用于编码器的输出,以获得上下文向量 $c ＝ α_iu_i$。 然后将上下文向量 c 和嵌入向量 $s_t$连接起来，并发送到解码器GRU中，输出为g和新的隐藏状态ht. 为了生成下一个token，我们重用了dictionary D及其转置的权重 $W_{D^1}$。我们再次集成上下文向量来融合上下文信息： 其中$W_2$是参数化矩阵。$p_t$下一个token在字典中所有token上的概率分布。 损失函数是交叉熵。假设第t步的one-hot向量是 $y_t$ (谁的one-hot向量？？), 那么每个训练样本序列对的损失函数是: 与语言模型耦合 上面介绍了在NLG中如何在字典中选token，下面介绍如何融入语言模型。 上面的编码器-解码器方法通过attention机制，在每个步骤中将对话动作d和含义表示r的信息合并在一起。 但是，由于这种机制，所产生的话语在很大程度上不可避免地依赖于输入顺序，而较少地关注人类语言的流畅性和变化性，但是这一点与在面向任务的对话中传达所需信息一样重要。另一方面，语言建模通常用于刻画单词，短语和句子的自然性。 一个训练有素的语言模型可以在自然和语义上比僵硬和不自然的句子获得更高的分数。 在深度学习中，语言模型任务通常由递归神经网络解决，但是，与(1)式不同的是，语言模型中出现下一个token的概率只依赖于前面的单词，而不依赖于输入序列： 我们认为，通过将语言建模作为一个额外的目标集成到NLG过程中，生成的句子将更好地接近人类反应的风格和变化。为此，我们将另一个GRU单元$GRU^{LM}$添加到解码器中，该解码器具有自己的隐藏状态 $h^{LM}{t-1}$， 并接受嵌入向量$s_t$作为输入。输出是 $g^{LM}$和新的隐藏状态 $h^{LM}{t}$。在语言模型中，下一个token的概率分布为: 其中W2[:dh]为W2的第dh列。可以看出，上下文c并不影响语言建模的概率计算。语言模型的损失函数为: 最后，我们将这两个损失函数线性地合并成一个单一的多任务损失函数: L(θ) =L^{NLG}(θ) +αL^{LM}(θ)图1中描述了模型结构。如图所示，词典D在NLG任务和语言建模任务之间共享。 Experiment本文评估了来自不同领域的五个数据集的模型，包括了餐厅预订，酒店预订和零售。最大的数据集来自E2E-NLG任务，由餐厅区域内的51.2K个MR话语对组成。还使用RNN-LG的四个数据集，包括电视零售，笔记本电脑零售，酒店预订和餐厅预订领域中的对话场景，分别为14.1K，26.5K，8.7K和8.5K个样本。 Baseline：TGen, SC-LSTM, RALSTM, Slug Use the multi-taskcoefficient α= 0.5 in all experiments Use the BLEU-4 and NIST metrics. Conclusion 本文提出了一种新的多任务学习方法 NLG-LM。它将语言模型任务作为无条件的补充过程整合到响应生成过程中，以提高生成话语的自然度。在多任务学习模式下，将这两个任务都安排在一个序列到序列的结构中。实验结果表明，该算法在5个大规模数据集上都有较好的性能，计算效率较高。消融实验显示了在多任务模式下使用语言建模任务的有效性。 Sih: 将NLG的响应生成与语言模型结合，一起学习，共享decoder，作为多任务学习（Multi task）， NLG-LM的输入用的是pipeline模型的DM的输出，是one-hot编码（竟然！我们pipeline还是有未来的…）。 和传统多任务学习相似，但和我看到abstract时的想法不太一致， 以为是Multi task learning的应用，呜呜…。","link":"/2019/09/25/Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue/"},{"title":"Switch-based Active Deep Dyna-Q Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning","text":"D3Q的一种新的改进方法。 题目：Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning 作者：Yuexin Wu, Xiujun Li, Jingjing Liu, Jianfeng Gao, Yiming Yang 来源：AAAI 2019 链接： https:// arxiv.org/pdf/1811.0755 0.pdf IntroductionProblem基于强化学习的任务型对话agent在训练时通常需要大量真实的user experience. Dyna-Q算法扩展了Q-learning，引入worldmodel的概念，尝试用 world model 产生的simulated experience来有效促进agent的训练。但是Dyna-Q的质量取决于world model的质量，或者进一步说，取决于在学习过程中使用的real experience和simulatedexperience的比例。本文引入一个switcher 去动态的决定在不同的训练阶段是用真实的还是模拟的experience.同时本文不同于以往的随机生成模拟经验，而采用了active sampling策略，生成状态动作空间中未被agent完全探索的模拟经验。本文也是首次将active learning应用到任务型对话上。 BackgroundDyna算法：Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。Dyna learns a model from real experience, learn and plan value function(policy) from real and simulated experience.(model-based) Dyna 算法 该方向的思路历程： Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning 优点：只使用少量的真实用户交互数据，基于集成规划的方法针对任务完成型对话对对话策略进行学习。 缺点：在使用大量的模拟数据时，因为在开始阶段agent对低质量的experience不敏感，但在后期则会有很大影响，因为agent对噪声更敏感了（之前的解决方法是设置参数K，来控制planning的次数）。并且生成模拟经验时是通过均匀的抽样usergoal，就存在有没被探索的状态动作空间。 Chris：Deep Dyna-Q: 任务型对话策略学习的集成规划 2. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning 3. Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning. Model基于Switch-DDQ的对话系统分为6个模块： 基于LSTM的NLU模块，识别对话意图，提取相关的slot state tracker, 用于追踪对话状态 dialogue policy, 基于当前的对话状态选取下一个对话动作 基于模型的NLG模块，将对话动作转为自然语言 World model, 用于生成模拟的user action和奖励 基于动态选择的user goal 一个基于RNN的 Switcher 具体Switch-DDQ模型分为四个步骤： (1) Direct Reinforcement Learning，即agent与真实用户交互，收集真实对话经验，完善对话策略;(2) Active planning, agent与simulator进行交互，利用模拟经验改进策略(2) World model learning，应用收集的真实对话经验进行world model的学习(4) Switcher training: switcher是通过真实和模拟的经验来学习和改进的进一步解释： Direct Reinforcement Learning 在这个步骤中基于真实对话经验，应用DQN来学习对话策略。在每一步中，agent通过当前的对话状态 ， 通过最大化价值函数，选择要执行的动作 。然后agent会接收到奖励 ,以及用户的反馈 ，更新当前状态到 .存储经验 到经验池 。(line 12 in Algorithm1)Planning的过程也是基于相同的Q-learning算法，只是planning使用的是模拟的对话经验。 Active Planning based on World Model Active user goal sampling module在planning的阶段，world model可以选择性的生成模拟经验—在agent未被完全探索的状态动作空间。假设我们已经从human-human的对话数据中收集了大量的user goal, 然后把usergoal分成不同类别，有不同的限制条件和难度。在训练过程中，当监控验证集上agent的表现时,我们可以通过对话的成功率来收集每一类usergoal对agent性能改进的影响，通过这样的信息来指导world model如何来选择user goal。 - 基于当前agent的表现，对话失败率 ![f_i ](https://www.zhihu.com/equation?tex=f_i+) 越高更意味着于注入了更难的情况（包含了更多需要学习的有用的信息） - 被估计为不稳定的类别（因为它被抽样的数量 ![n_i](https://www.zhihu.com/equation?tex=n_i) 小）可能有较高的失败率，因此值得分配更多的训练实例去降低不确定性。 关于user goal的选择概率 符合高斯分布，假设有 类user goal，每个类别在验证集的失败率为 , 其取样的数量为 ，总体的取样数量为, 设方差为作为 不确定性的度量, 上式则表示每一个user goal i被选中的概率 ，选择具有最大的 作为选择的user goal。 Response generation module 用真实的对话数据来训练world model ，使其能够产生模拟的对话经验。在每个对话回合中，在world model输入当前对话状态 和上一轮的agent action ，从而生成用户的回复动作 、奖励 和一个表示对话是否结束的信号 。网络结构如图所示：两个 shared layer，三个 taskspecific layer用来完成分类任务。 SwitcherSwitcher基于一个二分类器，使用LSTM模型。假定一组对话是由一个序列的对话轮次构成的： 是对话的轮次数，Q-learning中采用 作为一个训练样本，可以从一个对话中的两个连续对话轮次中提取。设计Switcher可以turn-based或者dialogue-based,因为基于轮次比基于对话数据量更大，所以本文选择turn-based，则对于给定的一组对话，对它的每一轮的质量进行打分，然后将每一轮的分数取平均来衡量这组对话的质量，如果对话层次的得分低于阈值，agent则转换成和realuser来交流。 每一轮次的打分，需要把前面的轮次都考虑进去。给定一个对话轮次 ,它的历史信息为，用LSTM在隐藏层去编码历史信息 ，最后通过一个sigmoid层输出基于轮次层次的质量分数。 将交叉熵作为Score(.)训练的损失函数，其中 和 是分别存放真实经验和模拟经验的经验池。 由于 和 中存储的经验会在对话训练过程中发生变化，所以switcher的分数函数也会随之更新，从而自动调整在不同训练阶段的planning的执行情况。 ExperimentDataset本文用到的数据集是电影订票场景中的原始会话数据，如表1所示，表1由11个对话动作和16个slot组成。数据集总共包含280个注释对话，每个对话的平均长度约为11轮。 Comparison 在电影票预订领域，对Switch-DDQ进行了两种评估:模拟和人工评估，在模拟评估中使用了开源的面向任务的用户模拟器(Li et al.2016)。为了测试Switch-DDQ的性能，文中使用了不同版本的任务完成型对话agent。 对于每个agent，文中以成功率、平均奖励和平均轮数展示实验的效果。如图所示，Switch-DDQ在前100个epoch后，在交互轮数较少的情况下，始终能够获得较高的成功率。 从实验结果也可以看出DDQ(5)所采用的策略，虽然在训练的早期帮助更快的更新策略网络，但是在后期由于使用了低质量的训练实例影响了性能，所以DDQ(5)无法获得与DQN类似的性能。然而，在Switch-DDQ中并不会出现这种情况，它没有参数K,因为实际和模拟的经验比例是由switcher模块自动控制。 Conclusion本文主要在上一篇论文DDQ的基础上引入了 一个Switcher, Switch-DDQ能够自适应地从真实用户或world model中选择要使用的数据源，提高了对话策略学习的效率和鲁棒性。Switch-DDQ也可以看作是一种基于模型的通用RL方法，并且很容易扩展到其他RL问题。 Reference [1] Su, S.-Y.; Li, X.; Gao, J.; Liu, J.; andChen, Y.-N. 2018. Discriminativedeep dyna-q: Robust planning for dialogue policy learning. arXiv preprintarXiv:1808.09442. [2]Peng, B.; Li, X.; Gao, J.; Liu, J.; Wong, K.-F.; and Su, S.-Y. 2018.Deep Dyna-Q: Integrating planning for task-completion dialogue policylearning. In ACL [3]Li, X.; Chen, Y.-N.; Li, L.; Gao, J.; and Celikyilmaz, A. 2017. End-to-end task-completion neural dialogue systems. arXiv preprint arXiv:1703.01008. [4]Schatzmann, J.; Thomson, B.;Weilhammer, K.; Ye, H.; and Young, S. 2007.Agenda based user simulation for bootstrapping a pomdp dialogue system. InHuman Language Technologies 2007: The Conference of the North American Chapterof the Association for Computational Linguistics; Companion Volume, ShortPapers, 149–152. Association for Computational Linguistics. [5] Chris：Deep Dyna-Q: 任务型对话策略学习的集成规划 ps: 欢迎大家关注我的知乎~ —&gt;戳这里&lt;—","link":"/2019/03/04/Switch-based-Active-Deep-Dyna-Q-Efficient-Adaptive-Planning-for-Task-Completion-Dialogue-Policy-LearningLearning/"},{"title":"Today's User simulator","text":"Dialogue SimulationSimulation-Based Evaluation 为什么用User simulator: 通常，RL算法需要与用户交互才能学习。但是，在招募的用户或实际用户上运行RL可能是昂贵的，甚至是有风险的。解决这一挑战的一种方法是构建一个模拟用户，与RL算法交互几乎不需要任何成本。本质上，User simulator试图模拟真实用户在对话中的行为: a. 跟踪对话状态，b.并与RL对话系统进行对话 一般的User simulator: such as deterministic vs. stochastic content-based vs. collaboration-based static vs. non-static user goals during the conversations 用户模拟器可以在 dialogue-act level 或在句子级别上操作 可以是基于规则也可以是基于模型从真实的对话数据中学到的。 Agenda-Based Simulation ex:As an example, we describe a popular hidden agenda-based user simulator developed by Schatzmann and Young (2009), as instantiated in Li et al. (2016d) and Ultes et al. (2017c). 每个对话模拟从一个随机生成的用户目标开始，对话管理器不知道这个目标。一般来说，用户目标由两部分组成:inform-slots 包含大量的槽值对，作为用户想要强加于对话的约束;request-slots 是用户最初不知道其值的槽，将在对话期间填写。 此外，为了使用户目标更加实际，还添加了领域特定的约束，以便某些slot需要出现在用户目标中。例如，要求用户知道在电影领域中需要多少张票是有意义的 在对话过程中，模拟用户维护一个堆栈数据结构，称为user agenda。议程中的每个agenda都对应着用户要实现的一个未决意图，它们的优先级由议程堆栈的“后进先出”(first-in-last-out)操作隐式地决定。因此，agenda提供了一种方便的方式编码的历史的对话和用户的“心理状态”。对用户的模拟可以归结为在每次对话结束后，当更多的信息被披露时，如何维护议程。机器学习或专家定义(expert-defined)的规则可用于在堆栈更新(stack-update)过程中设置参数。 Model-Based Simulation Another approach to building user simulators is entirely based on data. ex: (Eckert et al., 1997; Levin et al., 2000; Chandramohan et al., 2011). Asri et al. (2016) 与agenda-based的方法类似，模拟器还会以随机生成的用户目标和约束作为开始。这些是在谈话中是固定的。 在每一个回合中，用户模型都会输入一系列在对话中收集到的上下文，并输出下一个动作。具体来说，对话的语境包括： 最近的机器动作， 机器信息与用户目标不一致， 约束状态和 请求状态。 有了这些上下文，使用LSTM输出下一个用户话语。在实践中，通过结合基于规则和基于模型的方法来创建用户模拟器，可以表现的更好。 Further Remarks on User Simulation 目前的用户模拟器自身的评估也不明朗，实际一些对话策略会因为过拟合一个特定的对话模拟器而不能适用于真实的用户。 用户模拟器和人之间的差距，是优化基于用户模拟器获得的对话策略的最大的限制。 最近推出的一个更大的评估环境的语料库—AirDialogue Reference:[1]Li, X., Lipton, Z. C., Dhingra, B., Li, L., Gao, J., and Chen, Y.-N. (2016d). A user simulator fortask-completion dialogues. CoRR abs/1612.05688 [2]Li, X., Chen, Y.-N., Li, L., Gao, J., and Celikyilmaz, A. (2017d). End-to-end task-completion neuraldialogue systems. In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP), pages 733–743.[3] Gao,Michel Galley,Michel Galley, Neural Approaches to Conversational AI (2018) ​","link":"/2018/11/08/Today's user simulator/"},{"title":"jupyter的安装和配置","text":"1.安装jupyter pip install jupyter 如果安装报错，则安装 sudo apt-get install libsqlite3-dev 2.生成配置文件： jupyter notebook --generate-config (该文件在 /home/username/.jupyter/jupyter_notebook_config.py) 3.生成密码： #ipython (或者Python） from notebook.auth import passwd passwd() Enter password: #在此处输入密码 Verify password： #确认输入密码 然后会生成： Out：‘sha1:5ab...............' #为生成的密码，复制一下，后面会用到 4.修改配置文件 ~/.jupyter/jupyter_notebook_config.py vim ~/.jupyter/jupyter_notebook_config.py 此处需要更改的： c.NotebookApp.ip='*’ #此处默认为localhost,更改为任意ip c.NotebookApp.password = u'sha1:5ab..............' #此处为之前自行设置的密码，粘贴到此处 c.NotebookApp.open_browser = False c.NotebookApp.port =8888 #该处端口号可任意设置，8889，9999等，默认为8888 #ps:注意！！！！一定要去掉前面的注释，才可以有用。 保存退出 esc shift+: wq 启动jupyter jupyter notebook 然后打开浏览器输入： 1.如果是远程登录linux服务器： https://服务器地址:8889/ (此处8889为之前设置的端口号） 2.如果是本机： https://localhost:8889/","link":"/2018/09/25/jupyter的安装和配置/"},{"title":"Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models","text":"Naacl 2019 Abstract 为对话定义action space并通过强化学习来优化其决策的过程是一个长期以来的挑战。通常的做法是使用手工来定义对话动作或者词表作为动作空间，两者都有各自的局限性。本文提出了一种潜在动作框架，将端到端agent的动作空间作为潜在变量，并应用非监督方法，从数据中归纳出自己的动作空间。综合实验研究了连续型和离散型的动作类型以及两种基于随机变分推理的优化方法。结果表明，在DealOrNoDeal和MultiWoz对话框上，所提出的潜在动作比以往的词级策略梯度方法取得了更好的性能改进。 Introduction 本文提出的模型 Latent Action Reinforcement Learning (LaRL)，克服了端到端对话模型中基于word-level RL的限制，以无监督的方式结合了传统模块化方法的优点。关键的想法是开发一种可以自己发现对话动作的E2E模型。这些动作必须具有足够的表达能力，以捕获复杂领域中的回复 (即具有表示大量动作的能力)，从而将话语层面的决策过程与自然语言生成分离开来。然后，任何RL方法都可以应用到这个诱导的动作空间中来代替词级别的输出。所以本文提出了一种提供潜在变量的对话框架，研究了从对话数据中诱导潜在动作空间的几种方法。 本文还进一步提出 (1)一种新的训练目标，其性能优于对话生成中使用的典型证据下界;(2)在解码器中集成离散潜在变量的注意机制，以更好地对长响应建模。 本文在DealOrNoDeal (Lewis et al.， 2017)和MultiWoz (Budzianowski et al.， 2018)两个数据集进行了实验。结果表明，在学习对话策略时，LaRL明显比word level RL更有效，而且不会产生不可理解的语言。本文的模型在MultiWoz上实现了18.2%的改进，并在DealOrNoDeal上发现了新颖多样的谈判策略。 除了较强的实证改进外，本文的模型分析还展示了一些新的见解，例如减少潜在动作空间中的暴露偏差至关重要，离散的潜在动作比连续的更适合作为RL agent的动作空间。 本文的工作不同于以往的工作有两个原因:(1)以前的工作中潜在的动作只是辅助的，小规模的，大多是在监督或半监督的环境中学习的。本文研究的是潜在变量的无监督学习，学习的变量具有足够的表达能力，能够独立捕捉整个动作空间。(2)据本文所知，本文的工作是对对话系统中使用潜在变量进行RL策略优化的首次全面研究。 Baseline端到端的系统响应生成可视为一个基于条件的生成任务，该任务使用神经网络的编解码器对条件分布$p(x|c)$进行建模，其中$c$为观察到的对话上下文，$x$为系统对上下文的响应。对话上下文可以是原始对话历史或者文本上下文. RL训练通常分为2步: 基于监督学习的预训练，和基于强化学习策略梯度算法.具体来说,监督学习这一步来最大化对数似然值 ($\\theta$是模型参数):然后接下来的RL这一步使用策略梯度算法，更新与任务目标相关的模型参数。假设有一个agent可以与之交互的环境，并且在对话的每个轮次$t$处都有一个轮次奖励$r_t$。本文得到在模型参数$\\theta$下，预期的折扣回报$J(θ)= E[\\sum^T_0 \\gamma ^t r_t]$, 折现因子$\\gamma$在[0,1]之间，$T$为对话的长度(总轮数)。 通常在奖励r上减去参数b来降低策略梯度的方差 Rt=E[\\sum^{T-t}_{k = 0} \\gamma ^t (r_{t+k}-b)]. Word-level Reinforcement Learning如图1所示，world-level RL将每个输出单词视为一个action step，其策略梯度为其中$U_t$为第t轮回复中的token数量，j为回复中的单词index。显然Eq 2的动作空间非常大，即|V| (词表size)，学习视野较长，即$TU$。之前的工作发现直接应用该式会导致解码器不收敛。常用的解决方法是按一定比例交替使用带监督学习的Eq 2。将这个比率表示为RL:SL=A:B，这意味着, 对于每A次策略梯度更新，运行B次监督学习进行更新。对于只使用策略梯度而不涉及监督学习的情况，则RL:SL=off。 Model Latent Action Reinforcement Learning本文提出的LARL框架如图2所示，在回复生成过程中引入了一个潜在变量$z$。则现在的条件分布$p (x|c) = p (x|z) p (z|c)$生成的过程为:(1) 给定一个对话上下文 $c$, 首先依照$p{θ_e}(z|c)$抽样一个潜在动作 $z$(2) 基于$p{θd} (x|z)$，即基于z抽样生成的响应x, 其中$p{θe}$是对话编码器网络, $p{θ_d}$是响应的解码器网络。在上述设置下，LaRL将潜在变量$z$作为其动作空间，而不是响应 $x$中输出单词。之后可以在潜在动作空间中应用reinforce:与Eq 2相比，LaRL的不同之处在于: 将范围从$TU$缩短到$T$。 潜在动作空间设计为低维，比V小得多。 策略梯度只更新编码器$θe$, 而解码器$θ_d$保持不变。这些属性减少了对话策略优化的难度，并将高层决策与自然语言生成分离开。$给定上下文$c$, $p{θe}$ 负责选择最好的潜在动作, 而$p{θ_d}$仅负责将$z$转换为表层形式的单词。本文的公式也为实验各种类型的模型学习方法提供了一个灵活的框架。在本文中，本文主要关注两个关键方面:潜在变量$z$的类型, 以及在有监督的预训练阶段, 优化学习$z$的方法。 Types of Latent Actions以往的研究中使用了两类潜在变量:连续各向同性高斯分布(Serban et al.， 2017b)和多元分类分布(Zhao et al.， 2018)。这两种类型都与本文的LaRL框架兼容，定义如下： Gaussian Latent Actions高斯潜在动作遵循$M$维多元对角协方差矩阵的高斯分布, 即$z\\sim N(\\mu ,\\sigma^2I )$。编码器$p{θ_e}$由两部分构成:一个上下文编码器$F$, 用神经网络编码对话上下文$c$为一个向量表示$h$, 用一个前馈网络$pai$将$h$映射为$\\mu$和$\\sigma$ 。流程定义如下：然后将得到的$z$作为解码器的初始状态用于最后响应的生成。使用$p\\theta(z|c) = N(z; \\mu ,\\sigma^2I)$来计算Eq 3中的策略梯度更新. Categorical Latent Actions分类潜在动作是$M$个独立的$k$向分类随机变量。每个$z_m$都有自己的token embedding，将潜在的符号映射到向量空间$E_m \\epsilon R^{K×D}$，其中$m \\ epsilon [1,M]$， D为embedding size。因此 M个潜在动作可以表示指数级的，$K^M$个，不同且唯一的组合， 这使得它具有足够的表现力，可以在复杂的领域中建模对话动作。类似于高斯潜在动作，我们有与高斯潜在动作不同, 分类潜在动作矩阵编码后得到大小为 $R^{M×D}$ 的矩阵, 而解码器的初始状态是一个大小为 $R^D$ 的向量。以前的工作将潜在的emdedding相加，来集成这个矩阵和编码器, 表示作为求和融合。这种方法的一个限制是，它可能会丢失每个潜在维度中的细粒度顺序信息，并且在涉及多个对话动作的长响应方面存在问题。因此，本文提出一种新的方法，Attention Fusion，来结合分类潜在的动作与解码器。 将注意力机制(Luong et al.， 2015)应用于潜在的动作。设i为解码时的步长索引。然后有: Optimization Approaches 现在给定一个训练数据集{x, c}, 常规的优化方法是通过最大化其变分下界的随机变分推理（Full ELBO） 补充：在ELBO其中q是关于z的一个任意概率分布。 对任意分布q的选择来说，L提供了似然函数的一个下界。 越好地近似p(z∣x,c)的分布q(z∣x，c)，得到的下界就越紧。 将推断问题看作是找一个分布q使得L最大的过程。 $q{\\gamma }(z|x, c)$ 为需要训练的神经网络，来逼近 $q(z|x, c)$ 和$q(z|c)$ 和 $p (x|z)$ 的后验分布。本文认为上式有局限性，解码器只看到 $z$ 从 $q(z|x, c)$ 采样，但未经历过从 $p\\theta(z|c)$ 采样, 所以提出了以下优化函数（Lite ELBO）：本质上，简化目标将后验网络设置为与编码器相同，即 $q\\gamma(z|x, c) = p{θe} (z|c)$, 但是这也使得潜在空间缺少正则化，实验中表明，如果只最大化$p{p_{(z|c})}(x|z)$ 会存在过拟合。因此添加了额外的正则化项, 鼓励后验与先验分布能够相似, β是一介于0和1之间的超参数。将分类潜在动作的 $p(z)$ 设为平均值，即 $p(z) = 1/K$ ；将高斯潜在动作的先验设为 $N(0,I)$ . Experiment Dataset DealOrNoDeal是一个谈判数据集，包含基于2236个不同场景的5805个对话。本文提出252个测试环境场景，并从训练集中随机抽取400个场景进行验证。从混乱度(PPL)、奖励、一致性和多样性四个方面对研究结果进行了评价。PPL帮助确定哪个模型产生的与人更相似的响应，同时奖励和一致性评估模型的谈判强度。多样性表示该模型是发现了一种新的语篇级策略，还是只是重复了单调响应。 Multiwoz是一个插槽填充数据集，它包含10438个对话, 涉及6个不同的域中。8438个对话用于训练，各1000个用于验证和测试。由于该数据集之前没有用户模拟器，为了与之前的技术水平进行公平的比较，本文将重点放在中提出的对话-上下文-文本生成任务上。该任务要求在对话中的每个系统轮次生成响应对话。结果从三个方面进行评估:BLEU，Inform Rate和成功率。BLEU score检查响应层次词汇相似性，而Inform rate 和成功率度量模型是否在dialog级别是否回答问题并提供所有请求的信息。 Conclusion最后，本文为E2E dialogue agent中的RL提出了一个潜在变量的动作空间。 并对两种对话任务进行了评估，结果表明，本文的模型取得了优异的性能，并创造了一个MultiWoz最新的成功率。广泛的分析使我们能够深入了解如何正确地训练潜在变量，这些变量可以作为agent的动作空间，能够以无监督的方式创建抽象的动作。","link":"/2019/05/21/naacl2019 Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models/"},{"title":"Visual Dialogue 总结","text":"CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog NAACL 2019 链接： https://arxiv.org/pdf/1903.03166.pdf 摘要：可视对话是一种多模态任务，使用对话历史作为上下文，以图像为基础回答一系列问题。它是视觉、语言、推理和基础方面的挑战。然而，在大型真实数据集上单独研究这些子任务是不可行的，因为它需要对所有图像和对话框的“状态”进行代价高昂的完整注释。我们开发了一个大型诊断数据集，用于研究可视化对话框中的多轮推理。具体地说文章构建了一个基于CLEVR dataset 图像的场景图的对话语法。 其中可视对话的所有方面都得到了完整的标注。 总共包含了5个10轮对话框的实例，用于大约85k个cleveland图像，总共有425万对问答对。我们使用CLEVR-Dialog来测试标准可视对话模型的性能; 特别是视觉上的共参考分辨率(作为共参考距离的函数)。这是对可视对话模型的第一次分析，没有这个数据集是不可能的。文章希望CLEVR-Dialog的发现将有助于开发未来的可视化对话模型。数据集和代码将公开。 Image-Question-Answer Synergistic Network for Visual Dialog CVPR2019 链接： https://arxiv.org/abs/1902.09774 摘要: 图像、问题(结合对话历史)和相应的答案是可视化对话的三个重要组成部分。经典的可视对话系统集成了图像、问题和对话历史搜索或生成最佳匹配的答案，因此，这种方法明显忽略了答案的作用。本文设计了一种新颖的图像-问答协同网络来评价问答在精确的视觉对话中的作用。我们将传统的单阶段解决方案扩展为两阶段解决方案。在第一个阶段，根据候选答案与图像和问题对的相关性对它们进行粗略的评分。然后，在第二阶段，通过与图像和问题的协同作用，对正确率较高的答案进行重新排序。在可视化对话v1.0数据集上，提出的协同网络增强了判别式可视化对话模型，实现了57.88% \\%的归一化折现累积增益。一个生成的可视化对话模型也显示了很好的改进。 Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog ACL 2019 Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, Jianfeng Gao 链接： https://arxiv.org/abs/1902.00579 摘要：本文提出了一种新的视觉对话模型——递归双注意网络(ReDAN)，利用多步推理的方法来解决图像的一系列问题。在对话的每个问答环节中，ReDAN通过多个推理步骤逐步推断出答案。在推理过程的每一步中，根据图像和之前的对话历史，对问题的语义表示进行更新，并在后续的步骤中使用递归细化的表示进行进一步的推理。在VisDial v1.0数据集上，提出的ReDAN模型获得了64.47%的NDCG评分。推理过程的可视化进一步证明了ReDAN可以通过迭代细化找到上下文相关的视觉线索和文本线索，从而一步一步地得到正确的答案。 Sequential Attention GAN for Interactive Image Editing via Dialogue CVPR2019 Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, Jianfeng Gao 链接：https://arxiv.org/abs/1812.08352 摘要：在本文中，我们介绍了一种新的任务——通过会话语言进行交互式图像编辑，用户可以通过多回合的自然语言对话引导agent对图像进行编辑。在每个对话轮中，代理接受来自用户的源图像和自然语言描述作为输入，并在文本描述之后生成一个新图像。为此任务引入了两个新数据集，Zap-Seq和DeepFashion-Seq。本文提出了一种新的顺序注意生成对抗网络(SeqAttnGAN)框架，该框架利用神经状态跟踪器对每个对话轮中的源图像和文本描述进行编码，生成与前面图像和对话上下文一致的高质量新图像。为了更好地实现区域特定的文本到图像的生成，我们还在模型中引入了注意机制。在这两个新的数据集上的实验表明，所提出的SeqAttnGAN模型在基于对话框的图像编辑任务上优于现有的SOTA方法。详细的定量评估和用户研究也表明，我们的模型在图像生成方面比SOTA基线更有效，无论是在视觉质量还是文本到图像的一致性方面。 StoryGAN: A Sequential Conditional GAN for Story Visualization CVPR2019 Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao 链接： https://arxiv.org/abs/1812.02784 摘要：我们提出了一个新的任务，叫做故事可视化。给定一个多句的段落，通过为每个句子生成一个图像序列来可视化故事。与视频生成不同，故事可视化不太关注生成图像(帧)中的连续性，而是更关注动态场景和角色之间的全局一致性——这是任何单一图像或视频生成方法都无法解决的挑战。因此，我们提出了一个新的基于顺序条件GAN框架的故事到图像序列生成模型StoryGAN。我们的模型是独特的，因为它由一个动态跟踪故事流的深层上下文编码器和两个在故事和图像级别的鉴别器组成，以增强图像质量和生成序列的一致性。为了评估模型，我们修改了现有的数据集，以创建CLEVR-SV和Pororo-SV数据集。从经验上看，StoryGAN在图像质量、上下文一致性度量和人类评估方面都优于最先进的模型。 Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation CVPR2019 Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa 链接: https://arxiv.org/abs/1903.02547 摘要：我们提出了带有回溯(FAST)导航器的边界感知搜索，这是一个用于动作解码的通用框架，在Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson et. al. (2018) 挑战中实现了最先进的结果。给定一个自然语言指令和一个以前从未见过的环境的真实感图像视图，该代理的任务是尽可能快地从源导航到目标位置。虽然目前所有的方法都使用波束搜索来做出局部的行动决策或对整个轨迹进行评分，但我们的方法在探索一个未被观测的环境时平衡了局部和全局信号。重要的是，这让我们可以贪婪地行动，但在必要时使用全球信号进行回溯。将快速框架应用于现有的最先进的模型，获得17%的相对增益，绝对6%的成功率加权路径长度(SPL)。 Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation CVPR2019 Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang 链接：https://arxiv.org/abs/1811.10092 摘要：视觉语言导航(VLN)是在真实的三维环境中，通过导航一个具体的代理来执行自然语言指令的任务。在本文中，我们研究如何解决这一任务的三个关键挑战:跨模态接地，病态反馈和泛化问题。首先，我们提出了一种新的增强交叉模态匹配(RCM)方法，该方法通过增强学习(RL)来实现局部和全局的交叉模态接地。特别地，匹配批评家被用来提供一个内在的奖励来鼓励指令和轨迹之间的全局匹配，推理导航仪被用来在局部视觉场景中执行跨模态接地。对VLN基准数据集的评估表明，我们的RCM模型在SPL上的性能显著优于现有方法的10%，达到了最新的性能水平。为了提高学习策略的通用性，我们进一步介绍了一种自我监督模仿学习(SIL)方法，通过模仿自己的过去来探索未知的环境。我们证明SIL可以近似于一个更好和更有效的策略，这极大地减小了可见和不可见环境之间的成功率性能差距(从30.7%到11.7%)。 LARGE-SCALE ANSWERER IN QUESTIONER’S MIND FOR VISUAL DIALOG QUESTION GENERATION ICLR 2019 链接： https://arxiv.org/pdf/1902.08355.pdf 摘要： Answerer in Questioner’s Mind (AQM)是近年来提出的一种面向任务的对话系统的信息理论框架。AQM从提出一个问题中获益，当它被问到的时候，这个问题可以最大限度地获取信息。 然而，由于其本质上是显式计算信息增益， 当解空间很大时，AQM有一定的局限性。为了解决这个问题，我们建议使用AQM+来处理大规模的问题，并提出一个与当前对话上下文更一致的问题。我们对我们的方法进行了评估。 这是一个具有挑战性的面向任务的可视化对话问题，其中候选类的数量接近10K。我们的实验结果和消融研究表明，在合理的近似下，AQM+的性能显著优于最先进的模型。特别地，所提出的AQM+在对话过程中减少了60%以上的错误，而比较算法减少了不到6%的错误。基于我们的研究结果，我们认为AQM+是一种通用的面向任务的对话算法，可以应用于非yes-or-no响应。","link":"/2019/06/09/Visual Dialog 总结/"},{"title":"医疗对话系统总结","text":"Sih对目前医疗诊断领域任务型对话的总结。 End-to-End Knowledge-Routed Relational Dialogue System for Automatic Diagnosis 2019 AAAI https://arxiv.org/abs/1901.10623v2 Task-oriented Dialogue System for Automatic Diagnosis 2018 ACL https://www.aclweb.org/anthology/P18-2033 Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learning 2018 AAAI http://infolab.stanford.edu/~echang/aaai-2018-context.pdf Abstract本文提出了一个自动医疗诊断对话系统，通过与患者对话，收集患者自我报告之外的其他症状，自动做出诊断。现有的对话系统主要依赖于数据学习，无法对额外的专用知识图谱（expert knowledge graph）进行编码。在这项工作中提出了一个端到端知识路由相关对话系统 （KR-DS），将丰富的医学知识图谱整合到话题转换中，使其与NLU，NLG相互协作。本文使用基于Knowledge-routed的dqn网络 （krdqn） 来管理主题转换，它集成了一个编码不同症状和症状-疾病对之间关系的关系细化分支和一个用于主题决策的知识路由图分支。在公共医疗对话数据集上的大量实验表明，本文提出的KR-DS模型明显优于目前最先进的方法（诊断准确率超过8%）。另外，本文从在线医学论坛收集了一个新的医疗数据集用于验证实验结果。 Introduction1. 医学对话系统的难点 对话系统询问的症状应与潜在疾病相关，并与医学知识相一致。然而当前面向任务的对话系统高度依赖于复杂的belief tracker和纯数据驱动的学习, 由于这样的诊断缺乏医学知识的支撑所以它无法直接适用于自动诊断。 2. 现有的自动诊断系统和其缺点 Task-oriented Dialogue System for Automatic Diagnosis(2018 ACL) 率先构建了一个用于自动诊断的对话系统，将对话系统转换为马尔可夫决策过程，并通过reinforcement learning训练对话策略。缺点：这项工作只是利用DQN网络通过数据驱动学习来管理主题转换(确定应该询问哪些症状)，而且结果复杂且重复。此外，本工作仅针对对话管理进行对话状态跟踪和策略学习，使用基于模板的自然语言处理模型，无法很好地匹配实际的自动诊断场景。 3. 本文的解决方法 本文提出了一个完整的端到端知识路由关系（KR-DS）自动诊断的对话系统，结合医学知识图谱和基于症状-疾病关系进行话题转换的对话管理（DM），并使其与NLU和NLG结合，如图1所示。主要提出两个branch：knowledge-routed graph branch和 relational refinement branch。一般来说，医生根据医学知识和诊断经验（medical knowledge and diagnosis experience）来确定疾病类型。受此启发，本文主要提出两个branch：knowledge-routed graph branch和 relational refinement branch。 Relational refinement branch 从历史诊断数据（historical iagnostic data）中学习不同症状和症状-疾病对之间（symptom-disease pairs）的关系，从而细化由basic DQN生成的粗略结果。 Knowledge-routed graph branch 通过基于条件概率的医学知识路由图谱的先验信息来帮助决策。这两个分支通过knowledge guiding 和 relation encoding产生更加合理的决策结果。 4. 本文主要贡献： 本文进行关系建模（relation modeling）来处理相关的症状和疾病之间的关系，还进行了图谱推理（graph reasoning）来指导利用已有的医学知识进行策略学习。并且进一步介绍了一个面向终端任务的自动诊断对话系统和一个新的医疗对话系统数据集。 Adding: 医疗对话是如何进行的？ Proposed Work本文提出的对话系统（krds）如图1所示，作为一个面向任务的对话系统，它包括NLU,DM,NLG。DM根据当前对话状态执行主题转换，agent学习通过询问症状来诊断疾病的任务，最后应告知患者疾病类型。此外本文也使用了用户模拟器来生成基于user goal的对话。 Policy Learning with KR-DQN Natural Language Understanding 中文医疗诊断数据集 在该诊断系统中，假设有$M$种disease，$N$种symptom. 4种agent action:request+symptom, inform+disease, thanks, closing.所以agent的action space的大小$D$可以表示为：$D = num_{greeting}+M +N$. 4种user action:request+disease, confirm/deny/not-sure+symptom, thanks, closing 4种symptom states：positive, negative, not-sure and not mentioned ,在symptom vectors里表示为1, - 1, -2, 0 NLU和RL进行联合训练。 Policy Learning with KR-DQN Basic DQN Branch.先利用一个Basic DQN分支来生成一个粗略的动作结果，如公式1所示。MLP以状态$s_t$作为输入，输出一个粗糙的动作结果，先有一个粗略的DQN结果再对结果进行优化。 a_t^r = MLP(s_t) Relational Refinement Branch关系模块可以通过从一组其他元素的聚合信息来影响单个元素(例如，其他症状及疾病)。由于关系权重（relation weights）是由任务目标驱动自动学习的，因此关系模块可以对元素之间的依赖关系建模。因此本文设计了一个relational refinement module, 通过引进一个关系矩阵$R \\epsilon R^{D\\times{D}}$ 来表示所有动作间的依赖关系。Basic DQN 预测的动作是$a_t^r$, 再乘以学习到的关系矩阵R得到新的action $a^f_t \\epsilon R^D$, 该关系矩阵是不对称的，表示有向加权图。细化的动作向量$a^f_t$的每一个元素为初始预测动作$a_t^r$的加权和，其中权值表示元素之间的依赖关系. a^f_t = a^r_t ·R关系矩阵$R$由数据集的统计量进行条件概率的初始化。每个条目$R_{ij}$表示单位$x_j$以单位$x_i$为条件的概率。关系矩阵通过反向传播学习捕获动作之间的依赖关系。实验表明，这种初始化方法优于随机初始化，因为先验知识可以指导关系矩阵的学习。 Knowledge-routed Graph Branch（与其他对话系统的不同之处：以user的self report 作为对话的基础）设计思路：在拿到患者的自我报告时，医生首先要对患者可能存在几种候选疾病有一个大致的了解。随后，通过询问这些候选疾病（diease）的显著症状（symptoms），医生排除了其他候选病，直到确定诊断。受此启发，本文设计了一个知识路由图谱（knowledge-routed graph）模块来模拟医生的思维过程。首先计算症状和疾病之间的条件概率作为有向医学知识图谱的权值，其中有两种类型的节点，如图2所示。edges只存在于疾病和症状之间。每条edge都有两个weights，其中疾病到症状的条件概率为$P (dis|sym)$，症状到疾病的条件概率为$P (sym|dis)$. 在与病人交流的过程中，医生可能会有几种候选疾病。我们将候选疾病概率表示为与观察到的症状相对应的疾病概率。症状先验概率$P_{prior}(sym)$通过以下规则计算: 对于提及到的症状，患有该症状设置为1，未患有该症状设置为-1 其他症状（不确定或未提及）的概率设置为从数据集中统计计算出来的先验概率。那么这些症状的概率 $P_{prior}(sym)$乘以条件概率$P(dis|sym)$得到疾病概率$P(dis)$，公式为: P (dis) = P (dis| sym)·P_{prior}(sym)考虑到候选疾病，医生经常根据他们的医学知识询问一些值得注意的症状来确诊, 因此由疾病概率$P(dis)$与条件概率矩阵$P (sym|dis)$相乘得到症状概率$P (sym)$: P (sym) = P (sym|dis)·P (dis)将疾病概率$P(dis)$和症状概率$P (sym)$连接在一起，得到 knowledge-routed action的概率$a^k_t\\epsilon D$。有了这三个action $a_t^r, a_t^f , a_t^k$，然后把三个动作相加得到$a_t$, 作为KR-DQN在当前状态$s_t$下的最终结果。 a_t = sigmoid(a_t^r) + sigmoid(a_t^f ) + a^k_t Experiement医疗对话数据集： MZ dataset： （Baidu拇指医生）710个user goal，66个症状，4类疾病。 本文提出的数据集 DX dataset: 527个对话， 41种症状， 5类疾病 评价指标： 使用正确诊断率作为对话的准确性。 匹配率，来评估症状对话系统请求的有效性，成功匹配意味着系统询问用户隐含存在的症状，否则就是失败匹配。具体对话样例（与baseline的比较） Sih conclusion 对话逻辑较为简单，主要是对知识建模，知识推理的过程。 可以将此方法尝试用于multi domain dialog。 利用知识图谱对知识建模值得借鉴。 编码不同症状和症状-疾病对之间的关系，Relation modeling也是值得借鉴的。","link":"/2019/05/12/医疗对话系统/"},{"title":"w2v词向量模型的训练和使用","text":"0.数据处理针对wiki 数据集： 下载opencc把繁体中文转化为简体中文opencc(windows版本)下载地址：http://download.csdn.net/download/adam_zs/10213686 运行命令： opencc -i wiki.zh.fanti.text -o wiki.zh.jianti.text -c t2s.json jieba分词 a. 中文 def cut_words(sentence): # print sentence return \" \".join(jieba.cut(sentence)).encode('utf-8') b. 英文 引入日志配置 import logging logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 引入数据集 #some error here `# raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"] 切分词汇 sentences= [s.encode('utf-8').split() for s in sentences] 1.训练 step1: train a. #LineSentence model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count()) #min_count 最好设置为1，5的话容易匹配不到词典 b. #Text8Corpus sentences=word2vec.Text8Corpus(u'wiki_seg.txt') model=gensim.models.Word2Vec(sentences, , size=200min_count=3,) step2:save #保存.model a model.save(outp1) # outp1 可以为.model,.vect,.txt,.bin #保存.model b model.model.wv.save_word2vec_format(outp2, binary=False) #保存 #保存 c model.save('wiki.model') #.model model.save_word2vec_format('wiki.model.bin', binary=True) #.bin model.model.wv.save_word2vec_format(outp2, binary=False) #.bz step 3 基于模型继续训练 model=gensim.models.load('wiki.model') model.build_vocab(new_sentences, update=True) model.train(x, total_examples=model.corpus_count, epochs=model.iter) model.save('wiki.model') ps:#中间步骤 调用Word2Vec创建模型实际上会对数据执行两次迭代操作，第一轮操作会统计词频来构建内部的词典数结构，第二轮操作会进行神经网络训练，而这两个步骤是可以分步进行的，这样对于某些不可重复的流（譬如 Kafka 等流式数据中）可以手动控制： model = gensim.models.Word2Vec(iter=1) # an empty model, no training yet model.build_vocab(some_sentences) # can be a non-repeatable, 1-pass generator model.train(other_sentences) # can be a non-repeatable, 1-pass generator 2.载入 loadmodel = gensim.models.Word2Vec.load('wiki.model') #载入 .model模型 model1 = word2vec.Word2Vec.load_word2vec_format('wiki.model.bin', binary=True) #载入bin model=gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True) #载入 .bin模型 3.测试a.前几个最相关的词和相关性 res= model.most_similar（‘女孩’） b.两个词之间的相关性 model.similarity('dogs','you') c. 直接获取向量 model['电脑'] 4.Word2Vec 参数- min_count model = Word2Vec(sentences, min_count=10) # default value is 5 在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。 - size size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。 model = Word2Vec(sentences, size=200) # default value is 100 - workers workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用： model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization","link":"/2018/09/25/w2v词向量模型的训练和使用/"}],"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Actor-Critic","slug":"Actor-Critic","link":"/tags/Actor-Critic/"},{"name":"A3C","slug":"A3C","link":"/tags/A3C/"},{"name":"Multi-task Learning","slug":"Multi-task-Learning","link":"/tags/Multi-task-Learning/"},{"name":"Curiosity","slug":"Curiosity","link":"/tags/Curiosity/"},{"name":"reward","slug":"reward","link":"/tags/reward/"},{"name":"Dialogue system","slug":"Dialogue-system","link":"/tags/Dialogue-system/"},{"name":"Curriculum Learning","slug":"Curriculum-Learning","link":"/tags/Curriculum-Learning/"},{"name":"Self-paced Learning","slug":"Self-paced-Learning","link":"/tags/Self-paced-Learning/"},{"name":"meta-learning","slug":"meta-learning","link":"/tags/meta-learning/"},{"name":"User Simulator","slug":"User-Simulator","link":"/tags/User-Simulator/"},{"name":"jupyter","slug":"jupyter","link":"/tags/jupyter/"},{"name":"Latent Variable","slug":"Latent-Variable","link":"/tags/Latent-Variable/"},{"name":"Visual Dialog","slug":"Visual-Dialog","link":"/tags/Visual-Dialog/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"}],"categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/categories/Reinforcement-Learning/"},{"name":"Text Classification - Multi-task Learning","slug":"Text-Classification-Multi-task-Learning","link":"/categories/Text-Classification-Multi-task-Learning/"},{"name":"Dialogue system","slug":"Dialogue-system","link":"/categories/Dialogue-system/"},{"name":"Curriculum Learning","slug":"Curriculum-Learning","link":"/categories/Curriculum-Learning/"},{"name":"Curiosity","slug":"Reinforcement-Learning/Curiosity","link":"/categories/Reinforcement-Learning/Curiosity/"},{"name":"NLG","slug":"NLG","link":"/categories/NLG/"},{"name":"jupyter","slug":"jupyter","link":"/categories/jupyter/"},{"name":"Visual Dialog","slug":"Visual-Dialog","link":"/categories/Visual-Dialog/"},{"name":"word2vec","slug":"word2vec","link":"/categories/word2vec/"},{"name":"Multi-task Learning","slug":"NLG/Multi-task-Learning","link":"/categories/NLG/Multi-task-Learning/"}]}