{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"2019-2020 Conference about Curriculum Learning","text":"目前对Curriculum Learning的研究进展 会议：ACL 2019 Sentence Level Curriculum Learning for Improved Neural Conversational Models Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives EMNLP2019 Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction IJCAI2019 Curriculum Learning for Cumulative Return Maximization AAAI2019 Transferable Curriculum for Weakly-­Supervised Domain Adaptation AAAI2020 Fine-­Tuning by Curriculum Learning for Non‐Autoregressive Neural Machine Translation Reinforced Curriculum Learning on Pre-­trained Neural Machine Translation Models From Few to More: Large‐scale Dynamic Multiagent Curriculum Learning AAMAS2019 Learning Curriculum Policies for Reinforcement Learning 方向：- 生成任务 Curriculum Learning for Natural Answer Generation ​ IJCAI2018: https://www.ijcai.org/proceedings/2018/0587.pdf - 机器翻译 Competence-based Curriculum Learning for Neural Machine Translation ​ naacl2019：https://arxiv.org/pdf/1903.09848.pdf Curriculum Learning for Domain Adaptation in Neural Machine Translation. ​ NAACL 2019: https://arxiv.org/abs/1905.05816 An empirical exploration of curriculum learning for neural machine translation. ​ https://arxiv.org/abs/1811.00739 Fine-Tuning by Curriculum Learning for Non-­Autoregressive Neural Machine Translation. ​ AAAI 2020:https://arxiv.org/abs/1911.08717 - 信息检索 IR Curriculum Learning Strategies for IR: An Empirical Study on Conversation Response Ranking ​ ECIR’20：https://arxiv.org/abs/1912.08555 -结合强化学习 Reinforcement Learning based Curriculum Optimization for Neural Machine Translation ​ NAACL 2019: https://arxiv.org/abs/1903.00041 ​ 将数据切分为多个bin，利用RL动态选择训练哪个bin Evolutionarily-Curated Curriculum Learning for Deep Reinforcement Learning Agents ​ AAAI2019：https://arxiv.org/pdf/1901.05431.pdf Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments ​ CoRL2019：https://arxiv.org/abs/1910.07224","link":"/2020/02/03/2019-2020 Conference about Curriculum Learning/"},{"title":"ACL 2020 中NLU的研究点","text":"Sih对ACL2020中NLU研究方向的总结，对于NLU的研究内容，欢迎小伙伴一起讨论idea～ 1. 方向总结1) 优化NLU的结构化表示 Recursive Template-based Frame Generation for Task Oriented Dialog Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling 2) 解决NLU中close world的设定 - 将slot filling的任务转化为span抽取：Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations - Novel intent detection：Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification 3) 解决NLU中数据短缺的问题 Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network 2. 具体解析Recursive Template-based Frame Generation for Task Oriented DialogACL2020 Short paper, Amazon: https://www.aclweb.org/anthology/2020.acl-main.186/ 解决的问题：优化NLU的结构化表示。 概述：NLU模块处理用户的语句，并将其转换为结构化信息，此信息通常表示为语义框架，可捕获用户提供的意图和slot标签。本文认为对于复杂的对话场景，这样的浅层表示是不够的，因为它不能捕获许多域中固有的递归性质。 本文提出了一种递归的，基于层次框架的表示形式，文中将frame生成任务公式化为基于模板的树解码任务，其中解码器以递归方式生成模板，然后将slot值填充到模板中。 方法：输入用户话语x, 期望输出上图中括号里的表示y。The translation from x to y is performed using four components that are jointly trained end-to-end, (1)an encoder, (2) a slot decoder, (3) a tree decoder and (4) a pointer network. 数据集：ATIS Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational RepresentationsACL2020 Long paper, PolyAI : https://arxiv.org/abs/2005.08866 Code: https://github.com/PolyAI-LDN/task-specific-datasets 解决的问题：slot filling任务上的对value抽取的改进, 不将value值限制在固定类别上，而是将该任务定义为Span抽取任务。本文提出基Span的slot-filling模型，在slot filling任务中需要给一些slot填充Value值，本文直接从用户对话中抽取一个文段作为value。同时文中基于预训练模型和领域知识，该基于Span的模型可以在小样本条件下实现更好的效果。 Dataset: 文中自己提出了一个数据RESTAURANTS-8K，比ATIS和SNIPS更难。（slot 数量不多，5 个） Coach: A Coarse-to-Fine Approach for Cross-domain Slot FillingACL2020 Short paper: https://arxiv.org/abs/2004.11727 Code: https://github.com/zliucr/coach 本文提出了一种从粗到精的方法进行跨域slot filling的方法，首先通过检测token是否为slot实体来学习slot实体的general pattern, 然后预测slot entity的特定类型。 Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection NetworkACL 2020 哈工大https://arxiv.org/abs/2006.05702 本文研究了少样本槽位提取问题（Few-shot Slot-Tagging）。与其他广泛研究的少样本问题相比，少样本槽位提取面临着“建模标签间依赖关系”的独特挑战。但是，由于不同领域间存在标签集的差异，很难将先前学习的标签依赖应用于新的领域。为了解决这个问题，本文在CRF中引入了折叠的依赖关系迁移机制（Collapsed Dependency Transfer），通过建模抽象的标签依赖关系来实现这种迁移。在小样本和元学习的情景下，CRF的发射概率可以用利用度量学习得到：计算为单词与每个标签类别的相似度。为了计算这种相似性，在近期的图像小样本分类模型TapNet基础上，利用标签名称语义来表示标签，提出了一种标签增强的任务自适应投影网络（L-TapNet）。","link":"/2020/06/15/ACL 2020 中的NLU的研究点--sih/"},{"title":"ACL 2020 最新论文总结","text":"ACL2020 概览目前放出的几篇文章—-sih出品，转载请注明出处。 （呜呜图又挂了，也放在了知乎一份，滴～传送门在这里，欢迎关注我的知乎呀～） https://zhuanlan.zhihu.com/p/127062292 Generating Hierarchical Explanations on Text Classification via Feature Interaction Detectionhttps://arxiv.org/pdf/2004.02015 解释生成式神经网络在现实世界中的应用。在自然语言处理中，现有生成式方法从输入文本中选择单词或短语作为解释，但是忽略了它们之间的相互作用。在本文工作中，通过检测特征交互来构建层次性的解释。 这种解释可视化了单词和短语如何在层次结构的不同级别上组合，这可以帮助用户理解黑盒模型的决策。 通过自动和人工评估，在两个基准数据集上使用三个神经文本分类器（LSTM，CNN和BERT）对所提出的方法进行了评估。 Unsupervised Domain Clusters in Pretrained Language Modelshttps://arxiv.org/pdf/2004.02105 NLP中“in-domain data”的概念通常过于简单和模糊，因为文本数据在许多细微的方面（例如主题，样式或形式级别）都有所不同。 此外，域标签很多时候都不可用，这使得构建特定于域的系统具有挑战性。 本文则表明，大量的预训练语言模型隐式地学习了句子的表示形式，这些表示形式在没有监督的情况下按域聚类-暗含了文本数据中域的data-driven的简单定义。 本文利用此属性，并基于此类模型提出了域数据选择方法，该方法仅需要一小部分域内单语言数据。 文中评估了在五个不同领域中进行神经机器翻译的数据选择方法，它们在性能上均优于通过BLEU以及通过精确度和句子选择召回来衡量的既定方法。 Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight （与sih的相似）https://arxiv.org/abs/2004.02594 ​ ​ 当前的对话模型都是数据驱动人类对话数据中学习。 因此，可靠的训练语料库是构建健壮且well-behaved的对话模型的关键。 然而，由于人类对话的开放性，用户生成的训练数据的质量差异很大，有效的训练样本通常不足，且经常出现噪声样本。 这阻碍了那些数据驱动的神经对话模型的学习。 因此，有效的对话学习不仅需要更可靠的学习样本，而且还需要更少的嘈杂样本。 在本文中提出了一种数据处理框架，通过扩大和突出有效的学习样本并同时减少无效样本的影响，主动地将数据分布重塑为可靠的样本。Data Manipulation模型选择性地增加训练样本并为每个实例分配重要性权重以重新构造训练数据。需要注意的是， 本文建议的数据操作框架是完全由数据驱动的，并且是可学习的。 它不仅可以操纵训练样本来优化对话生成模型，还可以通过使用验证样本进行梯度下降来学习提高其操纵技巧。实验表明，本文的框架可以从13个自动评估指标和人工判断方面提高对话的生成性能。 ​ 具体模型结构： Hooks in the Headline: Learning to Generate Headlines with Controlled Styles​ 当前的摘要系统只能产生简单的基于事实的标题，但不能满足实际需求，去创建令人难忘的标题以增加曝光率。 本文提出了一项新任务，即Stylistic Headline Generation (SHG)，以三种形式（humor,romance and clickbait来丰富标题，以吸引更多读者。本文使用没有样式的文章标题对（只有标准的标题摘要数据集和单样式语料库），具体方法叫做-TitleStylist通过将摘要和重构任务组合到一个多任务框架中来生成样式特定的标题。此外还引入了一种新颖的参数共享方案来进一步从数据中分离文本样式。 通过自动评估和人为评估，证明了TitleStylist可以生成具有与以下三种目标样式相关的标题：humor,romance and clickbait。 本文的模型产生标题的吸引力得分比最新的摘要模型的吸引力得分高9.68％，甚至超过了人工撰写的参考文献。 具体模型结构：（Transformer结构，Multitask training） ​ other recent ACL： An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language modelshttps://arxiv.org/pdf/2004.02451 A Relational Memory-based Embedding Model for Triple Classification and Search Personalization https://arxiv.org/pdf/1907.06080 Hierarchical Entity Typing via Multi-level Learning to Rank https://arxiv.org/pdf/2004.02286 FastBERT: a Self-distilling BERT with Adaptive Inference Time https://arxiv.org/pdf/2004.02178 A Novel Hierarchical Binary Tagging Framework for Relational Triple Extraction https://arxiv.org/pdf/1909.03227","link":"/2020/04/07/ACL 2020 最新论文总结/"},{"title":"Adversarial Multi-task Learning for Text Classification","text":"创新点： Adversial learning + Orthogonality Constraints Abstract 背景： 神经网络模型已经表明了它们在Multi-task Learning中的广阔前景，其重点是学习共享层以提取公共的和随不同任务不变的特征。 存在的问题： 但在大多数现有方法中，提取的 shared features易受到特定于任务的功能或其他任务带来的噪音的污染。 本文的方法： 本文提出了一种adversarial multi-task learning frame-work，以减轻共享的和私有的潜在特征空间的相互干扰。 对16种不同的文本分类任务进行了广泛的实验，这证明了方法的正确性。 此外还表明，提出的模型所学习的共享知识可以视为现成的(off-the-shelf)知识，并且可以轻松地迁移到新任务中。 Related Intro Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. 存在的问题是， 目前多任务学习的方法尝试在不同的task中分离出 private和shared space， 只是基于一些应该shared的components. 图1-(a)所示，一般的共享-私有模型为任何任务提供了两个特性空间:一个用于存储任务相关的特性，另一个用于捕获共享的特性。该框架的主要限制是共享的特性空间可能包含一些不必要的特定于任务的特性，而一些可共享的特性也可能混合在私有空间中，存在特性冗余。 为了解决这个问题，本文提出了一种对抗式多任务框架，其中通过引入正交约束(orthogonality constraints)，使得共享和私有特征空间固有地不相交。 具体来说，我们设计了一个通用的共享-私有学习框架来对文本序列进行建模。 为了防止共享的和私有的潜在特征空间相互干扰，引入了两种策略：对抗训练和正交约束。 对抗训练用于确保共享特征空间简单包含公共和任务不变信息，而正交性约束则用于从私有空间和共享空间中消除冗余特征。 本文主要的contributions: 提出的模型以更精确的方式划分特定于任务的空间和共享的空间，而不是粗略地共享参数。 我们将原来的二元对抗性训练扩展到多类，这样不仅可以联合训练多个任务，还可以利用未标记的数据。 我们可以将多个任务之间的共享知识压缩成一个现成的神经层，它可以很容易地转移到新的任务中。 Method给定一个文本序列x = {x1，x2，···，xT}，我们首先使用lookupup层获取每个单词xi的向量表示（em-bedding）xi。 最后时刻T的输出可以看作是整个序列的表示，它具有一个完全连接的层，后跟一个softmax非线性层，用于预测类别的概率分布： loss 函数： $C$ is the class number. Text Classification with LSTM 多任务学习的目的是利用这些相关任务之间的相关性，通过并行学习任务来改进分类。设置对于task $k$, $D_k$是一个有$N_k$个样本的数据集 $D_k$ . Fully-Shared Model (FS-MTL) 在完全共享模型中，我们使用单个共享LSTM层提取所有任务的功能。 例如，给定两个任务m, n，它认为task m的功能可以完全与task n共享，反之亦然。 该模型忽略了某些特征依赖于task。 图2-a说明了完全共享的模型。 Shared-Private Model (SP-MTL)共享-私有模型为每个任务引入了两个特征空间：一个用于存储 task-dependent 的特征，另一个用于捕获 task-invarian的特征。 因此，我们可以看到每个任务都被分配了一个私有LSTM层和一个共享LSTM层。 正式地，对于task k中的任何句子，我们都可以计算其共享表示( shared rep-resentation) $s_t^k$ ，黄色区域，和任务特定表示(task-specific representation)$h_k$，灰色区域，如下： Task-Specific Output Layer 对于task $k$ 中的一个句子来说，它的特征 $h^{(k)}$ 是由深层的多任务体系结构发出的，最终被送入相应的 task-specific的softmax层 中进行分类或其他任务。损失行数为计算交叉熵， loss $L_{task}$可以计算为: Incorporating Adversarial Training虽然共享-私有模型不能保证共享的特征不在私有特征空间中存在，反之亦然。因此，在共享-私有模型中，一些有用的共享特性可能被忽略，而共享的特性空间也容易受到一些特定于任务的信息的污染。 良好的共享特征空间应包含更多公共信息，而没有特定于任务的信息。 为了解决这个问题，我们将对抗训练引入了多任务框架，如图3（ASP-MTL）所示。 Adversarial Network 对抗网络的目标是学习到符合真实数据 $P_{data}(x)$ 分布的生成分布 $p_{G}(x)$。GAN 学习一个生成网络 $G$ 和一个判别网络 $D$, $G$ 通过生成分布 $p_{G}(x)$ 生成样本，$D$ 学习决策一个样本是来自$p_G(x)$ 还是$P_{data}(x)$ The goal is to learn a generative distribution $p_{G}(x)$ that matches the real data distribution $P_{data}(x)$Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk.","link":"/2019/10/09/Adversarial Multi-task Learning for Text Classification/"},{"title":"Curiosity-driven Exploration by Self-supervised Prediction","text":"Abstract在现实世界中自监督预测的好奇心驱动的探索，能够提供给agent的奖励非常少，或者根本不存在。在这种情况下，好奇心可以作为一种内在的奖励信号，使agent能够探索环境并学习可能在以后的生活中有用的技能。本文把好奇心表述为一个agent在视觉特征空间预测自身行为结果的能力时的误差，并把这种error作为内在好奇心reward提供给agent进行学习，该视觉特征空间是由一个自监督逆动态模型学习的。 我们的公式扩展到像图像这样的高维连续状态空间，绕过了直接预测像素的困难，并且，关键的是，忽略了不能影响代理的环境方面。所提出的方法在两个环境中进行评估（VizDoom和Super Mario Bros）。研究了三个广泛的环境:1)稀疏的外部奖励，在这种奖励中好奇心使得与环境的互动更少以达到目标；2)没有外在回报的探索，好奇心驱使代理人更有效地探索；和3)推广到看不见的sce-narios(例如，同一游戏的新级别)，在这里，从早期经验中获得的知识帮助代理人比从头开始更快地探索新的地方。 Fig2 看图是两个loss，一个是Forword model 用$a_t$和$φ(s_t)$ 去预测$\\hat{φ}(s_{t+1})$， 一个是Inverse model 用$s_t$和$s_{t+1}$预测所采取的动作$a_t$。 Self-supervised prediction for exploration自监督的探索目标不是手动设计每个环境的特征表示，而是提出一种学习特征表示的通用模型，使得学习特征空间中的预测误差提供良好的内在奖励信号。我们提出这样一个特征空间可以通过训练具有两个子模块的深层神经网络来学习:第一子模块将原始状态编码为特征向量φ(st)，第二子模块将编码两个后续状态的$φ(s_t)$、$φ(s_{t+1})$的特征作为输入，并预测代理从状态$s_t$到$s_{t+1}$移动所采取的动作($a_t$)。训练这个神经网络相当于学习功能，定义为： 其中$\\hat{a_t}$是对动作$a_t$的预测估计值，并且训练了神经网络参数θ来进行优化。 其中，$L_I$是损失函数，用于衡量预测的action与实际action之间的差异。在$a_t$离散情况下，输出g是在所有可能的动作上均具有最大柔和的分布，并且在多项式分布下将θ的最大似然估计的数量最小化。学习的功能也称为逆动力学模型，元组（st，at，st + 1）要求在代理使用其当前策略与环境交互时获得的学习信息。 除了逆动力学模型(inverse dynamics model)外，还训练了另一个神经网络，该神经网络以$a_t$和$φ(s_t)$作为输入，并预测时间$t + 1$处状态的特征编码。 其中，$\\hat{φ}(s_{t+1})$是通过最小化损失函数$L_F$而预测的$φ(s_{t+1})$ ,应用神经网络参数$θ$预测估计值: 学习的函数$f$也称为正向动力学模型, 内在奖励信号$r^i_t$计算为: 其中$η&gt; 0$是比例因子。 为了生成基于好奇心的内在奖励信号，我们共同优化了方程3和5中分别描述的正向f和逆向g动力学损失。 逆模型学习一个特征空间，该特征空间对只与预测代理的行为有关的信息进行编码，而正向模型则在该特征空间中进行预测。 The inverse model learns a fea-ture space that encodes information relevant for predictingthe agent’s actions only and the forward model makes pre-dictions in this feature space. 本文将提出的好奇心函数称为 Intrinsic Curiosity Module（ICM）。由于没有动力诱使该特征空间对那些不受代理行为影响的任何环境特征进行编码，因此该代理将不会因无法预测的环境状态而获得任何奖励，它的探测策略将对干扰物的存在、光照的变化或环境中其他讨厌的变化来源具有很强的适应性。 参见图2的说明，Agrawal 在2016年构造了一个joint inverse-forward model 来学习推物体的特征表示。 但是，他们只是将forword model用作训练inverse model 特征的正则化函数，而本文将forword model预测中的error用作训练agent policy的好奇心奖励。agent所解决的总体优化问题是由公式1、3和5组成，可以写成： 其中0≤β≤1是权重，其权重是逆模型损失与正向模型损失之间的关系，而λ&gt; 0 标量是权衡策略梯度损失的重要性与学习内在奖励信号的重要性之间的关系。 Experiment本文定性和定量地评估了在有和没有内在好奇心信号的情况下，在两种环境下（VizDoom和Super Mario Bros）学习策略的性能。 评估了三个广泛的设置：a）达成目标时稀疏的外在奖励； b） 无外部奖励的探索； c）本文提出的新的场景。 Future work未来研究的一个有趣方向是将学习到的探索行为/技能用作更复杂的分层系统中的原始/低级策略。 例如，本文的VizDoomagent学会沿着走廊行走，而不是撞到墙壁上。 这对于导航系统可能是一个有用的原语。尽管丰富多样的现实世界为交互提供了充足的机会，但奖励信号却很少。 本文的方法在这种情况下表现出色，并将影响代理的意外互动转化为内在奖励。 但是，本文的方法并未直接扩展到“互动机会”都很少见的情况。 在理论中，可以将此类事件保存在 replay memory中，并使用它们来指导探索。 但是，本文将此扩展留给以后的工作。 ####","link":"/2019/10/25/Curiosity-driven Exploration by Self-supervised Prediction/"},{"title":"Data Augmentation for NLU -- Sih调研","text":"Sih之前对NLU的数据增强很感兴趣，最近在面试中也遇到很多能够用到DA的方向, 在此整理一些之前的调研内容。（模型图又挂了，后补吧～） Combining active and semi-supervised learning for spoken language understanding2005 Speech Communication https://www.sciencedirect.com/science/article/abs/pii/S0167639304000962 比较早的文章，主要是了解文章提出的半监督学习应用到NLU的架构。 在本文提出应用主动学习和半监督学习方法，减少对自然语言理解的标注工作。主动学习的目的是通过自动选择可能对标签信息最有帮助的话语来最大程度地减少标签话语的数量。 在基于确定性的主动学习的启发下，本文提出的主动学习方法选择了分类器最不自信的示例。 使用两种半监督学习方法来利用具有较高置信度得分（因此不是通过主动学习选择的）分类的示例。 第一种方法通过将机器标记的类用于未标记的话语来扩充训练数据。 第二种方法改为以加权方式用机器标记的话语增强使用人工标记的话语训练的分类模型。 然后使用选择性采样和自动标记的数据将主动学习和半监督学习结合起来，利用所有收集的数据并减轻仅采用主动学习或半监督学习所导致的数据不平衡问题。 Fast Cross-domain Data Augmentation through Neural Sentence Editing应用数据扩充减轻数据短缺。对于自然语言，句子编辑提供了一种解决方案-依靠对原始内容进行微小但有意义的更改，旨在在数据丰富的源域中学习知识，并将其应用于数据稀缺的其他目标域。本文提出了Edit-transformer，这是一个基于Transformer的句子编辑器。 本文认为 Edit-transformer比其基于Edit的规则改写更适合跨域环境。文章中还提出合成数据需要与原始数据足够不同，以使这些变化导致下游方法具有更好的泛化能力。 Controlled Text Generation for Data Augmentation in Intelligent Artificial AgentsEMNLP WNGT workshop https://arxiv.org/abs/1910.03487 本文研究的任务是对NLU做数据增强，通过给定的一个短语集生成语义相似的短语来进行数据增强。给定 signature（ domain+intent+slots ）和少量的 carrier phrases （短句），生成更多的语义相似的 carrier phrases 。 提出了三种生成模型来生成短句： Seq2seq with attention VAE VAE with discriminator:对VAE加入了一个类别鉴别器。先训练一个VAE，然后加上鉴别器用sleep-wake过程再训练整个网络。对于输出类别数量少的情况效果很好，但是对于本文中的上百个 signatures 输出还是有问题的。4. CVAE在VAE的基础上以 signature 的one-hot编码为条件，实验表明该模型能够自动学习与类别无关的隐变量 z 。 CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot Intent Detectionhttps://arxiv.org/abs/2004.01881 放在arxiv上的文章（是Zero-shot user intent detection via capsule neural networks)的作者），模型基本思路和sih最近总结的关于NLU增强idea基本一致，但针对解决的问题有一点不同。 【research topic】 为NLU中的新颖意图生成对应的句子，通过应用基于条件的BERT模型。 本文中为NLU中的意图检测任务制定了一个更现实，更困难的问题设置，即通用的少发意图检测（Generalized Few-Shot Intent Detection GFSID）。 GFSID旨在区分联合标签空间，该联合标签空间包括具有足够标签数据的现有意图和针对每个类别仅具有少数样例的新颖意图。 为了解决这个问题，本文提出了Conditional BERT的文本生成模型（CG-BERT）。 CG-BERT有效地利用了大型的预训练语言模型来生成以意图标签为条件的文本。 通过用变分推理对话语分布进行建模，即使只有少数话语，CG-BERT仍可以为新颖意图生成多种句子。本文通过为新颖的意图生成更多的言语来减轻GFSID任务中标注稀缺的问题，并将generalized few-shot问题转换为监督分类问题。 【method】 通过BERT学习的意图分布没有针对不同的意图进行调节，因此本文采用条件变分自动编码器（CVAE）的思想为BERT添加潜在空间映射并规范化BERT特征空间以形成单位高斯分布，来模拟具有相同意图的各种话语的分布。 具体方法是将intent拼接对应的sentence所为输入，将[CLS]作为隐变量z,对于Bert学习到的p(z|x,y)分布，控制条件y,为intent的类别，x为输入的句子。期望的输出为对于novel的intent，条件控制生成对应的句子，再将生成的句子加入到训练集中，训练NLU model。中间应用很多trick，去缓解在结合变分编码和transformer结合时出现的问题。 【Conclusion】 本文和sih之前的思路非常相似，应用变分推理对话语分布进行建模，再利用预训练语言模型来生成以意图标签为条件的文本，中间加了很多trick来缓解二者融合时候产生的问题，解决的很hard, 有改进的空间。","link":"/2020/07/01/Data Augmentation for NLU/"},{"title":"Domain Adaptive Dialog Generation via Meta Learning","text":"应用meta learning进行对话系统的领域适配，在MAML基础上提出DAML，未开源代码，MAML作为meta learning的具体方法之一，值得学习。 来源： ACL 2019 原文链接： https:// arxiv.org/abs/1906.0352 0 有些格式没来得及转换，可以直接看我的知乎 https://zhuanlan.zhihu.com/p/81715745 Introduction问题领域适配是对话系统中的一项重要任务，因为根据不同的需求，每天都有许多新的对话任务产生，为这些新任务收集和注释训练数据的成本很高，因为它涉及到与真实用户的交互。因此将现有的丰富资源领域的数据适应于资源有限的新领域是对话系统研究中的一项重要任务。 现有解决方法 ：Transfer learning、few-shot learning、meta-learning等介绍了如何解决机器学习中的数据稀缺问题。由于每个对话域之间的差异很大，因此将信息从富资源域泛化到另一个低资源域非常困难。 本文的方法 ：所以本文提出了一种基于元学习（meta-learning）的领域自适应对话生成方法DAML。DAML是一个端到端可训练的对话系统模型，它从具有丰富资源的领域任务中学习，然后用最少的训练样本适应到新的领域。通过将模型无关的元学习算法（ model-agnostic meta-learning MAML）应用于对话域，利用多个单域对话数据和丰富的资源来训练对话系统模型，将训练中的多个对话任务结合起来，学习适用于新领域的通用和可转移信息。使得该模型仅通过少量的训练实例，就能在一个新的领域有效地学习一个具有竞争力的对话系统。 meta-learning 即元学习，也可以称为“ learning to learn”。常见的深度学习模型，目的是学习一个用于预测的数学模型。而元学习面向的不是学习的结果，而是学习的过程。其学习的不是一个直接用于预测的数学模型，而是学习“如何更快更好地学习一个数学模型”。 MAML （ model-agnostic meta-learning ）的思想在于学习一个好的初始化权重，从而在新任务上实现fast adaptation，即训练好的模型能够在小规模的训练样本上迅速收敛并完成fine-tune。MAML算法试图建立多任务的通用内部表示，并在应用于新任务时最大限度地提高损失函数的灵敏度，从而当参数有微小的更新能够极大地提高新任务的损失值。这也使得对话系统不仅是只需要较少的目标域数据，而且能够以更有效的方式成功地适应新的领域。 徐安言：Model-Agnostic Meta-Learning （MAML）模型介绍及算法详解 具体来说，本文使用三个域: 餐馆、天气和公共汽车信息搜索 作为源数据，目标域则为 电影信息搜索 。通过更改seq2seq编解码器网络模型 Sequicity ，对其两阶段的CopyNet进行改进，通过实现MAML算法，使用源域的对话数据来实现最优初始化。然后使用常规梯度下降，用最少的对话框数据对目标域的初始化进行微调。最后，利用目标域的测试数据对自适应模型进行了评价。本文超越了最先进的zeroshot的基线， ZSDG (Zhao andEskenazi, 2018)，以及其他迁移学习方法。 Problem Formulation本文基于seq2seq的对话模型将对话上下文 $\\displaystyle c$ 作为输入，并生成一个句子 $\\displaystyle r$作为响应。 K个不同的source domain有丰富的数据, 则我们在每个源域 $\\displaystyle S_k$ 中都有训练数据，记作： 我们也将target domain T中的数据表示为： 在训练过程中，生成模型： 其中 $\\displaystyle C$ 是上下文， $\\displaystyle R$ 是系统响应。 为了进行领域的适应，对模型 $\\displaystyle M_{source}$ 进行微调 ,利用的是目标域训练数据 $\\displaystyleD_{train}^{T}$ ，得到一个新的模型 $\\displaystyle M_{target}$。目标是学习到一个模型，可以在新的目标领域表现良好: Proposed Methods 首先介绍如何将MAML算法与序列模型相结合。如图1所示，典型的梯度下降包括 : (1)初始化模型上训练更多的数据 (2)计算目标损失函数 (3)利用损失更新模型参数。 然而，对于MAML，有两个梯度更新步骤。 图中的序号表示执行的先后顺序， $\\displaystyle M$ 为初始化模型，k=1，2，3分别代表不同的sourcedomain，可以看到MAML采用了两次梯度更新（local和global）： 1）先结合源域训练数据和初始模型，更新一步，计算loss, 得到一个临时model $\\displaystyle M_{k}^{‘}$ ； 2）再从每个域及其对应的临时更新的域模型 $\\displaystyle M_{k}^{‘}$ 中计算出每个域新的meta-learning loss 3）然后将所有的新域损失求和得到最终的损失； 4）最后，使用final loss来更新原始模型M。 不同于常见的梯度,在MAML中,用来更新模型的目标损失函数并不直接从当前模型 $\\displaystyle M$ 计算, 而是从临时$\\displaystyle M_{k}^{‘}$ 模型计算。此操作背后的idea是,从更新的模型计算损失函数对原始域的变化更敏感,可以了解更多关于源域的常见的内部表示，而不是每个领域的独有的特征。然后在自适应阶段，由于基本的内部表示已经被捕获，模型对新域的独特特征就会非常敏感。因此，只需要一个或几个梯度步骤和最少的数据量来优化模型到新的领域。 接下来具体描述MAML算法和Sequicity模型结合的实现细节。如算法1所示，Sequicity模型以seq2seq的方式将自然语言理解、对话管理和响应生成结合起来，而元学习是一种调整损失函数值以实现更好优化的方法。 Sequcity模型 是在单一的seq2seq模型的基础上构造的，该模型结合了copying mechanism和 belief span 来记录对话状态。为了使用Sequcity模型，在 $\\displaystyle t$ 时刻将 $\\displaystyle c$ 用 $\\displaystyle\\{B_{t - 1},R_{t - 1},U_t\\}$ 来表示，其中 $\\displaystyle B_{t - 1}$ 为在$\\displaystyle t - 1$ 时刻的belief span， $\\displaystyle R_{t - 1}$ 为上一次的系统响应，$\\displaystyle U_t$ 是当前用户的话语。Sequicity模型引入了belief span来存储所有informslot的值，并在历史中记录requestslot的名称。通过这种方式，不需要将所有的历史语句放入一个RNN中来提取上下文特征，而是直接将存储在beliefspan中的slot作为所有历史上下文的表示。belief span更准确、更简单地代表历史语境，需要在每一个环节进行更新。 Sequcity模型的具体过程是,给定上下文 $$\\displaystyle c- &gt;\\{B_{t - 1},R_{t - 1},U_t\\}$$ ,$\\displaystyle t$ 时刻的belief span $\\displaystyle B_t$ 则是根据在 $\\displaystylet - 1$ 时刻的 $\\displaystyle B_{t - 1}$ , $\\displaystyle R_{t - 1}$ ，以及$\\displaystyle t$ 时刻的用户话语 $\\displaystyle U_t$ 提取的： 然后，根据之前提取的上下文和 $\\displaystyle t$ 时刻belief span生成系统响应 $\\displaystyle R_t$ : $\\displaystyle m_t$ 是一个帮助生成响应的简单标签。它检查存储在Bt是否有在数据库中有可用的信息。 将Sequcity模型与MAML算法结合： 由于MAML与任何基于梯度下降的模型兼容，所以将当前生成对话模型表示为 $\\displaystyle M$ , 可以随机初始化。根据该算法，对每个源域$\\displaystyle S_k$ 进行一定大小的训练数据采样 $\\displaystyle (c^{(k)}, r^{(k)}) $。在Sequicity模型中，输入训练数据 $\\displaystyle (c^{(k)}, r^{(k)})$ ，得到生成的系统响应,采用交叉熵作为所有域的损失函数： 对于每个源域 $\\displaystyle S_k$ ，使用梯度下降来更新并得到一个临时模型 $\\displaystyle M_{k}^{‘}$ ： 为了与(Finn et al.， 2017)文章保持一致，本文只更新了一步，这样在每个源域中都有一个更新的模型，且距离 $\\displaystyle M$只有一步。在以后的工作中，可以考虑多个步骤 的梯度更新。然后，根据更新的模型计算损失函数（其中每个源域中训练数据相同）: 在这一步之后，在每个域中都有元损失值。将所有源域的更新损失值汇总为meta learning的目标函数: 最后对模型进行更新，使元目标函数最小化： Experiment为了与最先进的领域自适应算法ZSDG (Zhao和 Eskenazi ,2018)相比较，本文使用SimDial数据集，该数据集首次用于评估ZSDG。 SimDial中总共有6个对话域:restaurant、weather、bus、movie、restaurant-slot和restaurant-style，本文选择restaurant、weather、bus作为源域，对于每个源域，有900、100、500个对话分别对应训练、验证和测试，每个会话有9个回合。其余三个域用于评估，它们被视为目标域。生成9个对话(占源域的1%)对每个域进行适应训练(finetune)，平均每个域包含约8.4轮。对于测试，为每个目标模型使用500个对话,其中选择Movie作为新的评价目标域，是因为电影有完全不同的NLG模板和对话结构，几乎没有共同的特点。 Metrics 实验中有三个主要指标: BLEU分数，entity F1 score和adapting time。 前两个指标是Finn等人(2017)使用的具有说服力的指标，证明了MAML对新任务的快速适应速度。 使用BLEU评分, 是因为生成自然语言也是任务的一部分，因此评估生成的反应句的质量。 每个对话的实体F1得分为，比较生成的belief span，因为belief span包含所有限制响应的槽，所以这个分数也会检查任务的完整性。 领域适应时间，在适应训练过程中，计算epochs的数量。 表1: 该模型在训练中对餐厅领域进行了额外的微调。“In Domain”使用所有三个源域 (餐厅、天气、公交车)，而“NewDomain”指的是电影领域。当给定相似的目标域数据时，DAML的性能优于ZSDG和迁移学习。 Conclusion and Future Work本文提出了一种基于元学习的领域自适应对话生成方法。构建了一个端到端可训练的对话系统，该系统利用两步梯度更新来获得对新领域更敏感的模型。在一个具有多个独立域的模拟数据集上评估模型。与zero-shot学习方法和迁移学习方法相比，DAML在实体F1中达到了最先进的性能。因此认为DAML是一种有效的、鲁棒的低资源对话系统训练方法。 DAML还提供了很有潜力的扩展，例如将DAML应用于基于增强学习的对话系统。文中还计划使DAML适应多域对话任务。 Reference Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks https:// arxiv.org/pdf/1703.0340 0.pdf Zero-Shot Dialog Generation with Cross-Domain Latent Actions https:// arxiv.org/pdf/1805.0480 3 Sequicity: Simplifying Task-oriented Dialogue Systems with SingleSequence-to-Sequence Architectures https://www. comp.nus.edu.sg/~kanmy/ papers/acl18-sequicity.pdf 再次post一下我滴某乎~https://zhuanlan.zhihu.com/p/81715745","link":"/2019/09/09/Domain Adaptive Dialog Generation via Meta Learning/"},{"title":"MSR任务型对话系统论文列表","text":"The paper list of MSR in task-oriented dialog system 任务型对话系统论文列表，总结了Sih一直以来对MSR对话研究的朝拜之路～ Budgeted Policy Learning for Task-Oriented Dialogue Systems ACL 2019 https://arxiv.org/pdf/1906.00499 在DDQ基础上的改进，增加一个BSC模块，DDQ中的world model模块没有改进，sih认为也是用于改进训练数据的质量。 ConvLab: Multi-Domain End-to-End Dialog System Platform 2019.4 提出ConvLab对话系统模块集成的平台 GitHub链接：https://github.com/ConvLab/ConvLab https://arxiv.org/pdf/1904.08637 Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy Learning AAAI209 https://arxiv.org/pdf/1811.07550 加一个Switch，也是在生成质量上的改进，调整用于训练的数据的比例。 Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning ACL2018 https://arxiv.org/pdf/1801.06176 基于model based的模型Dyna-Q (DQN的一种改进), 引入world model用来生成模拟的用户经验。 Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning EMNLP 2018 https://arxiv.org/pdf/1808.09442 D3Q在DDQ基础上的改进, 在world model 后加判别器，模拟器生成的经验质量好的话再给agent作为训练数据 Subgoal Discovery for Hierarchical Dialogue Policy Learning EMNLP 2018 https://arxiv.org/pdf/1804.07855 应用层级神经网络，提出SDN网络模型，把对话任务分解成subgoal，完成travel planning。 Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning https://arxiv.org/pdf/1704.03084 上一篇文章的开端，EMNLP是在这一篇基础改进的。 Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning ICASSP 2018 https://arxiv.org/pdf/1710.11277 A3C, Adversarial 的 AC End-to-End Task-Completion Neural Dialogue Systems https://arxiv.org/pdf/1703.01008 Xiujun Li最早提出的任务型对话系统的pipeline 模型。 A User Simulator for Task-Completion Dialogues https://arxiv.org/pdf/1612.05688 Xiujun Li 最早提出的User simulator.","link":"/2019/11/01/MSR任务型对话系统论文列表/"},{"title":"ConvLab - Multi-Domain End-to-End Dialog System Platform","text":"微软和清华在对话系统领域的合作：ConvLab - Multi-Domain End-to-End Dialog System Platform 提出ConvLab–多领域端到端对话系统平台，它一方面提供一些可复用的组件来帮助实验人员快速实验，另一方面还可基于它在通用环境中对比大量不同方法（包括从pipeline到完全端到端好几个层级的方法，后面会细说）。另外，ConvLab还提供了一些完全标注的数据集和一些相关的预训练模型。作为一个演示，我们使用用户对话行为标注扩展了multiwoz数据集(记得好像是EMNLP2018 best paper提出来的，不仅有数据还有相应的任务)用来训练系统的所有组件，同时演示convlab如何使在多领域的端到端对话中进行复杂实验变得简单和高效。 本文主要贡献包括：ConvLab是第一个涵盖多个可训练模型和相关标注数据集的开源多领域端到端对话系统；ConvLab提供一套丰富的工具和组件用于开发不同类型的对话系统，使研究人员能够在相同条件下比较不同的方法；ConvLab通过人工和仿真器提供端到端的评估；ConvLab作为DSTC8的标准平台。 Anyway， 期待！","link":"/2019/04/25/MSR的ConvLab/"},{"title":"Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue","text":"EMNLP 2019 任务型对话整理 创新点：将NLG的响应生成与语言模型结合，共享encoder，作为多任务学习。 Abstract在面向任务的对话中，自然语言生成(NLG)是生成面向用户系统话语的最后一步。NLG的结果直接关系到对话系统的质量和可用性。尽管大多数现有的系统在给出目标时都能给出正确的响应，但它们很难与人类语言的多样性和流利性相匹配。在本文中提出了一个新的多任务学习框架，NLG-LM，用于自然语言生成。除了生成传递所需信息的高质量响应外，它还通过一个未配置的语言模型明确地针对生成响应的自然相关性。这对提高人类语言的风格和多样性有重要意义。实验结果表明，该多任务学习框架在多数据集的学习效果优于已有的多任务学习框架。例如，它将以前E2E-NLG数据集上的最佳BLEU评分提高了2.2%，将笔记本数据集上的最佳BLEU评分提高了6.1%。 IntroChallenge: 由于NLG直接面向用户，其可读性和信息量直接影响用户对整个对话系统的感知。一方面，为了提供或请求用户的信息，响应必须包含所需的信息，即meaning representation(MR)。另一方面，系统响应需要模仿人类语言的流动性和变异性来提高用户体验。早期的工作主要采用预定义的规则语法，尽管这些框架可以提供足够的信息，但是它们缺乏语言的自然性和变化性，使得响应非常僵化。此外，这些方法通常需要费力气的手工工作来创建模板，从而使它们无法跨域扩展。 Method: 由于面向任务的对话数据通常包含丰富的人类响应，因此利用语言建模技术提高NLG模型模拟人类语言的能力具有很大的潜力。 meaning representation(MR) 本文方法： 本文提出了一个多任务方案来解决面向任务的对话中的自然语言生成问题。对于NLG任务，我们使用了一个equence-to-sequence的框架。解码器使用一种attention机制传送信息，从编码器处编码的一个MR序列。因此，NLG任务生成响应的条件是输入MR。 我们工作的主要贡献是将语言建模任务整合到人为生成的响应中，作为无条件的补充过程，引入更多与语言相关的元素，而无需干预MR信息。此外，语言建模的非监督性质意味着我们不需要额外的标记数据。因此，在多任务学习框架下，我们同时训练NLG和语言建模任务。为了促进多任务学习，我们在解码器中执行语言建模任务，并且部分共享来自NLG任务的参数。 为了评估模型NLG-LM的有效性，本文对5个任务导向的对话任务进行了评估: E2E-NLG、电视、笔记本电脑、酒店和餐厅数据集。NLG-LM在所有5个数据集上达到了新的SOTA。例如，它超过了最优秀的E2E-NLG竞争对手Slug，在BLEUscore的成绩为2.2%。消融研究表明，在训练过程中引入语言建模任务可以使BLEU分数平均提高2.4%。 Problem setting Pipeline 模型，根据DM生成的action（槽值对），本文叫MR（。Q_Q。），来生成自然语言。 在面向任务的对话中，自然语言生成(NLG)过程是将系统话语作为自然语言生成，给定系统生成的意义表示MR，这些表示来自pipeline模型之前的步骤。每个MR是一个槽值对，槽表示要传递的信息的类别，值表示内容。 Dialogue acts被用来区分不同类型的系统动作。典型的对话行为包括inform, request, confirm。对于给定的意义表示，NLG过程应该为不同的对话行为生成不同的话语。例如，confirm dialogue act通常会导致系统以“Let me confirm” or “Correct me if I’m wrong” 开始。 给定训练数据${（di，ri，ui）}$，其中di是对话行为，$ri = {（s1，v1），（s2，v2），…，（sk，vk）}$是含义表示的集合，ui是样本是由人类标记者产生的话语，目的是通过一对新的对话行为d和意义表示r来产生话语u。 地点等实体用特殊token代替。 Model先将对话动作d和意义表示r连接在一起作为单个输入序列I，有m个token。从给定的话语u中获得输出序列O，有n个token。两个序列都被去词化。在每个序列周围放置特殊的句子标记和。 模型训练的目标是在给定先前预测的token和输入序列的前提下，一次生成一个输出token。 可以将其建模为最大化条件概率分布： Encoder:我们训练一个dictionary D来将每个token映射到一维定长向量。 输入的embedding序列然后进入双向RNN层以生成contextualized embedding。 我们使用GRU作为RNN单元，并汇总前向和后向RNN输出。 编码器的输出用$（u1，…，um）∈R^{d_h×m}$表示，其中$d_h$是RNN的输出尺寸，m是输入序列长度。 Decoder：解码器采用具有注意力机制的RNN，一次生成一个令牌。它从 beginning-of-sentence 的 token开始，并使用encoder RNN的最终隐藏状态作为初始隐藏状态。 在第t步中，我们使用编码器中相同的字典D，将第t个输出token映射到向量$s_t$, 并且dropout。 然后，给定先前的隐藏状态$h_{t-1}$，解码器首先计算编码器输出的注意力权重： 然后将权重$α$应用于编码器的输出,以获得上下文向量 $c ＝ α_iu_i$。 然后将上下文向量 c 和嵌入向量 $s_t$连接起来，并发送到解码器GRU中，输出为g和新的隐藏状态ht. 为了生成下一个token，我们重用了dictionary D及其转置的权重 $W_{D^1}$。我们再次集成上下文向量来融合上下文信息： 其中$W_2$是参数化矩阵。$p_t$下一个token在字典中所有token上的概率分布。 损失函数是交叉熵。假设第t步的one-hot向量是 $y_t$ (谁的one-hot向量？？), 那么每个训练样本序列对的损失函数是: 与语言模型耦合 上面介绍了在NLG中如何在字典中选token，下面介绍如何融入语言模型。 上面的编码器-解码器方法通过attention机制，在每个步骤中将对话动作d和含义表示r的信息合并在一起。 但是，由于这种机制，所产生的话语在很大程度上不可避免地依赖于输入顺序，而较少地关注人类语言的流畅性和变化性，但是这一点与在面向任务的对话中传达所需信息一样重要。另一方面，语言建模通常用于刻画单词，短语和句子的自然性。 一个训练有素的语言模型可以在自然和语义上比僵硬和不自然的句子获得更高的分数。 在深度学习中，语言模型任务通常由递归神经网络解决，但是，与(1)式不同的是，语言模型中出现下一个token的概率只依赖于前面的单词，而不依赖于输入序列： 我们认为，通过将语言建模作为一个额外的目标集成到NLG过程中，生成的句子将更好地接近人类反应的风格和变化。为此，我们将另一个GRU单元$GRU^{LM}$添加到解码器中，该解码器具有自己的隐藏状态 $h^{LM}_{t-1}$， 并接受嵌入向量$s_t$作为输入。输出是 $g^{LM}$和新的隐藏状态 $h^{LM}_{t}$。在语言模型中，下一个token的概率分布为: 其中W2[:dh]为W2的第dh列。可以看出，上下文c并不影响语言建模的概率计算。语言模型的损失函数为: 最后，我们将这两个损失函数线性地合并成一个单一的多任务损失函数: $$L(θ) =L^{NLG}(θ) +αL^{LM}(θ)$$ 图1中描述了模型结构。如图所示，词典D在NLG任务和语言建模任务之间共享。 Experiment本文评估了来自不同领域的五个数据集的模型，包括了餐厅预订，酒店预订和零售。最大的数据集来自E2E-NLG任务，由餐厅区域内的51.2K个MR话语对组成。还使用RNN-LG的四个数据集，包括电视零售，笔记本电脑零售，酒店预订和餐厅预订领域中的对话场景，分别为14.1K，26.5K，8.7K和8.5K个样本。 Baseline：TGen, SC-LSTM, RALSTM, Slug Use the multi-taskcoefficient α= 0.5 in all experiments Use the BLEU-4 and NIST metrics. Conclusion 本文提出了一种新的多任务学习方法 NLG-LM。它将语言模型任务作为无条件的补充过程整合到响应生成过程中，以提高生成话语的自然度。在多任务学习模式下，将这两个任务都安排在一个序列到序列的结构中。实验结果表明，该算法在5个大规模数据集上都有较好的性能，计算效率较高。消融实验显示了在多任务模式下使用语言建模任务的有效性。 Sih: 将NLG的响应生成与语言模型结合，一起学习，共享decoder，作为多任务学习（Multi task）， NLG-LM的输入用的是pipeline模型的DM的输出，是one-hot编码（竟然！我们pipeline还是有未来的…）。 和传统多任务学习相似，但和我看到abstract时的想法不太一致， 以为是Multi task learning的应用，呜呜…。","link":"/2019/09/25/Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue/"},{"title":"On The Power of Curriculum Learning in Training Deep Networks","text":"ICML 2019 课程学习在图像识别上的改进。 ICML 2019 :https://arxiv.org/abs/1904.03626 code: https://github.com/GuyHacohen/curriculum_learning 本文主要介绍利用课程学习在图像识别上的改进，如何非均匀采样mini batches，以及做了多组对比实验（包括SPL和 repeat bootstrapping）关注的主要问题也是课程学习中的两个难点： 依据难度对样本进行排序 对一系列的min-batches根据其难度依次进行训练。 对应的解决方式： 与之前的研究有创新的点是引入 transfer learning，也符合图像识别中的特点，引入教师网络和自助法(Bootstrapping) 尝试了不同的pace function,与之前的研究差别不大 具体方法 - 本文尝试了两种Scoring functions ： Transfer scoring function ：利用在imagenet上pretrain的inception网络，为每个样本的在其上得到的特征向量训练一个分类器，利用分类器的confidence score作为每个图片的scoring function。 Self-taught scoring function (Bootstrapping): 对样本进行均匀采样，计算当前网络每个图片confidence score作为scoring function，可重复这个过程，意思是score function可以更新，即在用CL训练网络后，用新的confidence score定义scoring function再retrain。（但这种方法最后的实验效果并理想，作者认为的原因是，如果一开始将某一样本误判为easy，则后续训练则一直认为这个样本是easy，这里不是很理解。） - 本文还尝试了三种pace function： - Fixed exponential pacing ：训练开始时向学习者投入一小部分数据，然后在固定次数的学习迭代中每次以指数方式增加训练样本 ​ Varied exponential pacing ：与上面方法相似，但每次迭代中训练步数是step length是可以变化的。 ​ ​ Single-step pacing ：固定iteration变化一次 ​ Result: 本文的做了一系列的对比实验和分析, ​ 这一组实验里的Anti 指的是CL的逆过程（harder examples are sampled before easier ones），CL的结果是优于Anti以及Random的。 ​ 这一组实验是综合对比本文提出的各种方法，也做了SPL的实验，但是效果是最差的，可以看出SPL在实际应用时还不是很理想。最好的实验结果时是应用Varied exponential pacing作为pace function。 对于提出的transfer scoring function也在不同教师网络，和不同分类器上做了对比实验。 ​ Sih总结 Transfer learning的做法在图像领域上使用的，如果要用在NLP上，之前的尝试是有的工作将BERT作为获取获取句子置信分数的特征提取模型，应用的是QA领域进行课程学习。 Anti的实验证明了CL的效果，repeat bootstrapping 和我之前的想法类似，就是要在一轮训练之后，更新当前的难度评价函数，再retrain，但本文中的实验效果并不好。 Pace function没有创新，但给出了源代码所以可以借鉴尝试。","link":"/2020/03/08/On The Power of Curriculum Learning in Training Deep Networks/"},{"title":"Today's RL algorithm","text":"Double DQN : Deep Reinforcement Learning with Double Q-learning Deep Deterministic Policy Gradient (DDPG): Continuous control with deep reinforcement learning Continuous DQN (CDQN or NAF): Continuous Deep Q-Learning with Model-based Acceleration Dueling network DQN (Dueling DQN) ： Dueling Network Architectures for Deep Reinforcement Learning Deep SARSA ： [10] Asynchronous Advantage Actor-Critic (A3C) ： Asynchronous Methods for Deep Reinforcement Learning Proximal Policy Optimization Algorithms (PPO)： Proximal Policy Optimization Algorithms Cross-Entropy Method（CEM）: Learning Tetris Using the Noisy Cross-Entropy Method 1. Double DQNQ值函数的定义:在策略π下，状态s，采取动作a， 所得到的Q值为： 最优值就是: $$Q^*(s, a) = max_π Q_π(s, a)$$ 最优策略就可以通过在每个状态选择最高值的行动给出。则目标 DQN 算法DQN 算法的两个最重要的特点是目标网络 (target network) 和经验回顾 (experience replay)。目标网络，其参数为 θ^-，和 online的网络一样，除了其参数是从 online network 经过某些steps之后拷贝下来的。$$θ^-_t = θ_t$$ 目标网络是： Double Q:在标准的 Q-学习和 DQN 中的 max 操作用相同的值来选择和评价一个 action，这实际上更可能选择过高的估计值，从而导致过于乐观的值估计overestimated values。为了避免这种情况的出现，我们可以对选择selection 和衡量evaluation进行解耦。这就是 Double Q。 reference: Double DQN Background论文（Hasselt等人）发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。作者在他2010年提出的Double Q-Learning的基础上，将该方法引入了DQN中。具体操作是对要学习的Target Q值生成方式进行修改，原版的DQN中是使用TargetNet产生Target Q值，即 $$Q_T =r+\\gamma\\max_{a’} Q (s’,a’;\\theta_{i}^-)$$ 其中$θ^-$是TargetNet的参数。 Steps 在DDQN中，先用MainNet找到 $\\max_{a’}Q(s’,a’;\\theta_{i})$的Action ($θ_i$是MainNet的参数) 再去TargetNet中找到这个Action的Q值来去构成Target Q值，这个Q值在TargetNet中不一定是最大的，因此可以避免选到被高估的次优Action。最终要学习的Loss Function为： $$L(\\theta)=E[(Q_T-Q(s,a;\\theta_i))^2$$此时的Target Q 变为: $$Q_T = r+\\gamma Q (s’,\\max_{a’} Q(s’,a’;\\theta_i);\\theta_{i}^-)$$ 除此之外，其他设置与DQN一致。实验表明，DDQN能够估计出更准确出Q值，在一些Atari2600游戏中可获得更稳定有效的策略。 reference: Doule DQN 2. Dueling-DQN Background Wang等人提出了一种竞争网络结构（dueling network）作为DQN的网络模型。 Structure 第一个模型是一般的DQN网络模型，即输入层接三个卷积层后，接两个全连接层，输出为每个动作的Q值。 第二个模型–竞争网络（dueling net）将卷积层提取的抽象特征分流到两个支路中： 上路代表状态值函数 $V(s)$，表示静态的状态环境本身具有的价值 下路代表依赖状态的动作优势函数$A(a)$（advantage function），表示选择某个Action额外带来的价值。 最后这两路再聚合再一起得到每个动作的Q值。 论文作者认为，V(s)更专注于环境，A(a)更专注与动作本身对结果的影响，以车的游戏做了举例说明 公式 状态价值函数表示为 $$V(s;θ,β)$$ 动作优势函数表示为 $$A(s,a;θ,α)$$ 动作Q值为两者相加 $$ Q(s,a;θ,α,β)=V(s;θ,β)+A(s,a;θ,α)$$ 其中 θ 是卷积层参数，β 和 α 是两支路全连接层参数。 而在实际中，一般要将动作优势流设置为单独动作优势函数减去某状态下所有动作优势函数的平均值 $$Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta)+(A(s,a;\\theta,\\alpha)-\\frac{1}{|A|}\\sum_{a’}A(s,a’;\\theta,\\alpha))$$ 这样做可以保证该状态下各动作的优势函数相对排序不变，而且可以缩小 Q 值的范围，去除多余的自由度，提高算法稳定性。 reference：Dueling-DQN 3. Prioritized Experience Replay简单来说，经验池中TD误差 $$r+\\gamma\\max_{a’}Q(s’,a’;\\theta^-)-Q(s,a;\\theta)$$绝对值越大的样本被抽取出来训练的概率越大，加快了最优策略的学习。 4. H-DQNIntroduction hierarchical deep reinforcement learning, model-free, value-based 当环境反馈是sparse和delayed时，解决方法是构造一个两个层级的算法，将困难问题分解为多个小目标，逐个完成。 Procedure以一个两层级的算法为例： 一个层级叫做meta-controller，它负责获取当前状态$s_t$ ，然后从可能的子任务里面选取一个子任务 g_t \\in \\mathcal{G} 交代给下一个层级的控制器去完成。它是一个强化学习算法，其目标是最大化实际得到的extrinsic reward之和，$F_t = \\sum_{t’=t}^\\infty \\gamma^{t’-t} f_{t’}$ 。 在这里，这一层使用的是DQN方法，这一层Q-value的更新目标是: ps: 按照sih的理解，通俗来讲就是，状态s和作为该层级的动作-&gt;子任务$g$,以及外置奖励(此处的外置奖励来源于外部环境)进行一次DQN的学习。 另一个层级叫做controller，它负责接收上一个层级的子任务 $g$ 以及当前的状态 $s_t$ ，然后选择一个可能的行动 $a_t$ 去执行。它也是一个强化学习算法，其目标是最大化一个人为规定的critic给出的intrinsic reward之和，$R_t(g) = \\sum_{t’=t}^\\infty \\gamma^{t’-t} r_{t’}(g)$。这里也使用DQN方法，更新目标为: ps: 此步则是以上一层级的子任务g作为先验，将s,a,g,以及内置奖励（此处的内置奖励来源于critic）进行DQN的学习。 算法框架","link":"/2018/11/23/RL算法总结/"},{"title":"Recent Advances and Challenges in Task-oriented Dialog System.","text":"SCIENCE CHINA 2020 的文章，出自黄老师组的 zhangzheng 和 高信～ 结合该论文梳理目前任务型对话系统的最新进展和挑战。 依旧给我的知乎打个广告，传送门～https://zhuanlan.zhihu.com/p/120657306 嘻嘻，某公众号也转载了该篇，sih觉得还是看原文理解更深一点～ 背景本文总结了目前任务型对话系统最新的进展和挑战，讨论了三个相关的关键问题： 如何提高数据使用效率，在资源不足的情况下对任务型对话系统进行建模； （2）如何为多轮对话的对话策略进行建模，使其在任务完成上表现的更好； （3）如何将领域的本体知识集成到pipeline模型和端到端模型中。 同时回顾了当前对话评估的方法和目前广泛使用的语料库的最新进展。 一些介绍 任务型对话系统的目标： 旨在帮助用户完成特定领域中的某些任务，例如餐厅预订，天气查询和航班预订，使其对现实世界中的业务有价值。 任务型对话系统与开放域对话系统相比的区别： 开放域对话系统的主要目的是最大化用户的参与度，而任务型对话系统更针对于完成一个或多个域中的某些特定任务。 通常，面向任务的对话系统以结构化的本体集合为基础，这些结构化的本体集合定义了该任务的领域知识。 任务型对话系统的一般结构： 任务型对话系统从体系结构分类大致可分为两类: pipeline 和 end-to-end 在pipeline的方法中，模型通常由几个组件组成: 自然语言理解(NLU)、对话框状态跟踪(DST)、对话框策略(Dialog Policy)和自然语言生成(NLG)。它们以流水线的方式组合在一起，如图所示。NLU、DST和NLG模块通常在组合之前单独进行训练，而dialog policy则在组合系统中进行培训。 ​ 2. 在 end-to-end的方法中，对话系统以端到端方式进行训练，而不指定每个单独的组件。通常端到端方法的训练过程被表述为：在给定对话上下文和后端知识库的情况下生成的系统应答。 Main Challenges归结上述的分析，总结起来就是任务型对话系统中的三个关键问题： Data Efficiency： 但是在面向任务的对话系统中，特定于域的数据通常很难收集并且注释起来很昂贵。 因此如何解决数据源匮乏we,提高数据使用效率是主要的挑战之一。 Multi-turn Dynamics：与开放域对话相比，面向任务的对话的核心特征是它在多轮策略中强调goal-driven。 在每个轮次，系统动作都应与对话历史保持一致，并应引导后续对话获得更大的任务奖励。但是基于RL的方法，构建训练环境开销较大以及当前奖励的定义也不够完善。此前的研究提出了许多解决方案，以解决多轮交互式训练中的这些问题来学习更好的策略，包括基于模型的 planning，连续的奖励估计和端到端的策略学习。 Knowledge Integration：任务型对话系统必须查询知识库（KB）以检索一些实体以生成响应。 在pipeline方法中，KB查询主要是根据DST的结果构造的。 那么与pipeline模型相比，端到端方法没有了模块化的组件，因此需要细粒度的注释和领域专业知识。 由于没有构造查询的显式状态表示形式，因此这种查询过程存在很多问题。 研究现状Data efficiency与开放域对话系统的研究不同，面向任务的对话系统是数据驱动方法，通常需要细粒度的注释来学习特定域中的对话模型，所以我们需要对话行为（dialog act）和状态标签（state labels), 但是通常很难在特定域中获得大规模的带注释的语料库，另外细粒度的标注也需要大量的人力资源。因此目前针对该挑战提出的一些方法： Dialog Transfer Learning， Unsupervised Methods， User Simulation。 Dialog Transfer Learning 对于低资源问题： 在面向任务的对话系统中一般应用转移学习，通过将知识从源任务转移到目标任务来解决低资源问题。 例如，当旅馆域中的数据有限时，如何将餐馆预订系统改为旅馆预订。在这种情况下，这两个域的本体相似，共享许多对话动作和槽位。 在这种情况下，迁移学习可以大大减少这种适应所需的目标数据量。除域级别的迁移外，知识还可以在许多其他维度上进行迁移，包括个性化和跨语言迁移。 主要应用的算法：多任务学习，参数共享，模型共享，元学习。 Multi-domain dialog state tracking using recurrent neural networks.提出通过在多个域的数据集上进行多任务学习来学习DST模型，跨域传递知识，从而提高所有任务的性能。 Goal-oriented chatbot dialog management bootstrapping with transfer learning. 提出直接从源域模型共享参数来初始化目标模型。 Cross-domain dialogue policy transfer via simultaneousspeech-act and slot alignment. 提出对于没有共享slot的领域间，建立状态转移方程和映射函数。 个性化迁移： Personalizing a dialogue system with transfer reinforcement learning（ACL2018）.所有的用户都有一个通用的Q函数，每个特定的用户都有一个个性化的Q函数。当转移到一个新的用户，只需要少量的数据获得个性化的Q-function。 Fine grained knowledge transfer for personalized task-oriented dialogue systems. Personalizing dialogue agents via meta-learning. 此外基于元学习的方法在解决低资源的问题上也有所应用， Meta-learning for low-resource natural language generation in task-oriented dialogue systems. 通过在NLG上应用MAML算法，使得模型可以在低资源环境下获得较好的结果。 2. Unsupervised Methods对于缺少细粒度标注的问题： 生成对抗网络（GAN）: 对话策略学习中的一个关键问题是设定奖励进行监督，在实际应用中就需要建立能够提供奖励信号的奖励估计模型。 此处无监督方法的应用在于，将对话策略视为生成器，并将奖励函数视为判别器，可以采用生成对抗网络（GAN）以无监督的方式学习奖励函数。 Adversarial learning of task-oriented neural dialog model. Unsupervised dialogue spectrum generation for log dialogue ranking. 变分自动编码器（VAE）: 在大多数研究中，对话系统的本体集是由专家构建的。目前的最新的研究进展是从未标记的语料库中自动学习对话结构来协助人类专家进行此过程。 Unsupervised dialog structure learning. 提出通过变分自动编码器（VAE）的方法来学习对话过程的有限状态机。 使用未经中间注释的原始对话语料库预先训练了基于VAE的对话模型,然后根据潜在变量可以发现几个对话状态。 基于预训练的方法: 在预训练方法中使用无监督的预训练任务，例如 mask language modeling (MLM) 和 next sentence prediction (NSP)，从大规模的未标记语料库提取广泛的语言特征，再应用到对话任务中。 A transfer learning approach for neural network based conversational agents. Towards the use of pretrained language models for task-oriented dialogue systems. 3. User Simulation用监督的方法构建用户模拟器，提供基于RL的对话策略模型不限次数的用于训练交互的环境，缓解data-hungry 的问题。 Sequence to sequence modeling for user simulation in dialog system. Neural user simulation for corpus-based policy optimisation of spoken dialogue system. Deep dyna-q: Integrating planning for task-completion dialogue policy learning. Multi-turn Dynamics在开放域对话系统中，研究更多地集中在生成合理，一致且语义丰富的响应上，以最大程度地提高用户参与度。 面向任务的对话系统，则重点放在完成特定任务上这个对话过程可以表述为⻢尔可夫决策过程(MDP):在每个时间步长，通过采取某些对话动作，系统从当前状态过渡到了新的下一状态。因此，强化学习通常用于解决对话系统中的MDP问题。 最新研究主要集中在以下主题上:(1) 规划对话策略学习以提高样本效率;(2)奖励稀疏性问题;(3)端到端系统的对话策略学习。 Planning for Dialog Policy Learning 在应用RL的训练过程中，使用用户模拟器并不能完全模拟真实的对话行为，因此存在的偏差也会影响训练效果。目前研究的解决方法是在RL训练阶段，学习实际用户行为的同时，使用环境模型进行规划来交替训练对话策略。 在Deep Dyna-Q(DDQ)模型中有一个基于真实用户训练的world model捕捉环境动态, planing则是指对话策略则是通过直接RL(实际用户)和模拟RL(world model)交替来进行训练的。 ​ 基于planning的另一种方法是决策时间的规划，仅展望未来几个有限的步骤，并将这些步骤作为策略模型的附加函数来做出决策，而不是等所有对话结束再获得明确的奖励。 Towards end-to-end learning for efficient dialogue agent by modeling looking-ahead ability. Hierarchical text generation and planning for strategic dialogue. 2. Reward Estimation在基于RL的对话模型中，奖励对于策略学习至关重要。 定义奖励函数的一种典型方法是在对话成功时分配较大的正向奖励，并在每轮次中分配较小的负奖励，以鼓励对话轮次尽可能短。但是实际应用中，有时无法有效地估算奖励，或者规则的奖励信号与真实用户主观给出的奖励信号有时不一致。为了解决上述问题，目前的研究中尝试的方法有： 一种方法是使用带注释的数据进行离线学习： Predicting user satisfaction in spoken dialog system evaluation with collaborative filtering. 通过将对话句子和中间注释作为输入特征，将奖励学习表述为有监督的回归或分类任务。 可以从人工标注获得带注释的奖励。 但是由于输入特征空间很复杂，需要手动注释，开销较大。 还有另一项工作是使用在线学习进行奖励估算: Reward estimation for multi-domain task-oriented dialog. EMNLP2019 主要思想将预测的状态动作与实际做比较，提供不确定性的度量来作为奖励信号。此外还有采用对抗学习来进行对话奖励的估算。 3. End-to-end Dialog Policy Learning端到端的对话训练方法则不再拆分为各个模块，而是对整个系统进行建模。 与开放域对话建模类似，以单词级别作为输入，将策略学习和回复生成结合在一起作为解码器。但是与开放域对话相比，任务型对话的策略学习对完成任务更重要，将两个操作结合在一起会相互影响性能。 因为对于特定的训练示例，损失可能来自策略学习的error或回复语言生成的error之一，但参数的反向传播会影响两者。因此一些工作尝试用潜在变量模型，从端到端框架中分离策略和语言的学习： Hierarchical text generation and planning for strategic dialogue.提出用潜在变量模型将对话表达的语义从其语言实现中分离出来。 此外端到端系统学习另一个存在的问题是很难进行显式的知识库查询，比较熟悉的端到端对话系统模型 Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence .它的解决办法提出了一种两阶段的CopyNet，在解码之前产生一个明确的状态表示, 具体是首先对用户的输入生成belief span表示当前对话状态，然后通过belief span来查询知识库，再生成系统响应。 一些研究还提出了一种复合状态表示形式，用于软KB查询: Sequence-to-sequence learning for task-oriented dialogue with dialogue state representation Knowledge Integration任务型对话系统的另一个主要问题是如何将领域知识集成到对话模型中，如图所示。 ​ 在pipeline框架中，是通过根据DST的结果从知识库中查询知识，然后在NLG过程中，将查询到的实体集成到自然语言响应中，因此DST的效果影响了KB的抽取。传统的神经方法将DST定义为分类任务，但只能处理本体中的slot值，很难解决OOV问题，并且很难扩展到新域。 最近的研究尝试用端到端的对话系统解决该问题，但也存在诸多挑战，因为与pipeline方法不同，没有显式的对话状态表示形式来生成显式的知识库查询。 以下介绍的最新的进展包括：（1）DST的生成方法（2）端到端系统中的知识集成。 Generative DST CopyNet和 Pointer network DST跟踪每一个对话轮次中结构化的对话状态表示，目前的研究也是直接将自然语言作为DST输入而不使用NLU， 避免了NLU模块的累积误差。早期的方法通常采用分类方法解决DST问题： Neural belief tracker:Data-driven dialogue state tracking. 通过⻔控机制对系统request和inform信息进行了显式建模。 但是这些方法只能处理该领域本体词汇表中预定义的slot值。 上面提到的Sequicity模型使用带有两阶段CopyNet的seq2seq模型，同时生成belief span和回复响应，应用复制网络解决OOV的问题。 另外还有研究提出，使用指针网络来提取之前未定义的slot值： An end-to-end approach for handling unknown slot values in dialogue state tracking. 2. End-to-end Knowledge Integration由于端到端方法中没有显式的结构化对话状态表示。 因此，目前的研究希望通过使用模型的中间潜在表示来和进行知识库交互，端到端的进行训练。主要应用 CopyNet和端到端memory networks，通过结合注意力机制将知识集成到对话系统中。 A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. Entity-consistent end-to-end task-oriented dialogue system with kb retriever. Sequence-to-sequence learning for task-oriented dialogue with dialogue state representation. 一些最新的对话数据集比较 未来关注点： Pre-training Methods for Dialog Systems. A transfer learning approach for neural network based conversational agents. Hello, it’s gpt-2–how can i help you? towards the use of pretrained language models for task-oriented dialogue systems. ACL2019 ​ Domain Adaptation Zero-Shot Dialog Generation with Cross-Domain Latent Actions End-to-end Modeling Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence .","link":"/2020/03/20/Recent Advances and Challenges in Task-oriented Dialog System/"},{"title":"Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models","text":"Naacl 2019 Abstract 为对话定义action space并通过强化学习来优化其决策的过程是一个长期以来的挑战。通常的做法是使用手工来定义对话动作或者词表作为动作空间，两者都有各自的局限性。本文提出了一种潜在动作框架，将端到端agent的动作空间作为潜在变量，并应用非监督方法，从数据中归纳出自己的动作空间。综合实验研究了连续型和离散型的动作类型以及两种基于随机变分推理的优化方法。结果表明，在DealOrNoDeal和MultiWoz对话框上，所提出的潜在动作比以往的词级策略梯度方法取得了更好的性能改进。 Introduction 本文提出的模型 Latent Action Reinforcement Learning (LaRL)，克服了端到端对话模型中基于word-level RL的限制，以无监督的方式结合了传统模块化方法的优点。关键的想法是开发一种可以自己发现对话动作的E2E模型。这些动作必须具有足够的表达能力，以捕获复杂领域中的回复 (即具有表示大量动作的能力)，从而将话语层面的决策过程与自然语言生成分离开来。然后，任何RL方法都可以应用到这个诱导的动作空间中来代替词级别的输出。所以本文提出了一种提供潜在变量的对话框架，研究了从对话数据中诱导潜在动作空间的几种方法。 本文还进一步提出 (1)一种新的训练目标，其性能优于对话生成中使用的典型证据下界;(2)在解码器中集成离散潜在变量的注意机制，以更好地对长响应建模。 本文在DealOrNoDeal (Lewis et al.， 2017)和MultiWoz (Budzianowski et al.， 2018)两个数据集进行了实验。结果表明，在学习对话策略时，LaRL明显比word level RL更有效，而且不会产生不可理解的语言。本文的模型在MultiWoz上实现了18.2%的改进，并在DealOrNoDeal上发现了新颖多样的谈判策略。 除了较强的实证改进外，本文的模型分析还展示了一些新的见解，例如减少潜在动作空间中的暴露偏差至关重要，离散的潜在动作比连续的更适合作为RL agent的动作空间。 本文的工作不同于以往的工作有两个原因:(1)以前的工作中潜在的动作只是辅助的，小规模的，大多是在监督或半监督的环境中学习的。本文研究的是潜在变量的无监督学习，学习的变量具有足够的表达能力，能够独立捕捉整个动作空间。(2)据本文所知，本文的工作是对对话系统中使用潜在变量进行RL策略优化的首次全面研究。 Baseline端到端的系统响应生成可视为一个基于条件的生成任务，该任务使用神经网络的编解码器对条件分布$p(x|c)$进行建模，其中$c$为观察到的对话上下文，$x$为系统对上下文的响应。对话上下文可以是原始对话历史或者文本上下文. RL训练通常分为2步: 基于监督学习的预训练，和基于强化学习策略梯度算法.具体来说,监督学习这一步来最大化对数似然值 ($\\theta$是模型参数):然后接下来的RL这一步使用策略梯度算法，更新与任务目标相关的模型参数。假设有一个agent可以与之交互的环境，并且在对话的每个轮次$t$处都有一个轮次奖励$r_t$。本文得到在模型参数$\\theta$下，预期的折扣回报$J(θ)= E[\\sum^T_0 \\gamma ^t r_t]$, 折现因子$\\gamma$在[0,1]之间，$T$为对话的长度(总轮数)。 通常在奖励r上减去参数b来降低策略梯度的方差$$Rt=E[\\sum^{T-t}{k = 0} \\gamma ^t (r{t+k}-b)]$$. Word-level Reinforcement Learning如图1所示，world-level RL将每个输出单词视为一个action step，其策略梯度为其中$U_t$为第t轮回复中的token数量，j为回复中的单词index。显然Eq 2的动作空间非常大，即|V| (词表size)，学习视野较长，即$TU$。之前的工作发现直接应用该式会导致解码器不收敛。常用的解决方法是按一定比例交替使用带监督学习的Eq 2。将这个比率表示为RL:SL=A:B，这意味着, 对于每A次策略梯度更新，运行B次监督学习进行更新。对于只使用策略梯度而不涉及监督学习的情况，则RL:SL=off。 Model Latent Action Reinforcement Learning本文提出的LARL框架如图2所示，在回复生成过程中引入了一个潜在变量$z$。则现在的条件分布$p (x|c) = p (x|z) p (z|c)$生成的过程为:(1) 给定一个对话上下文 $c$, 首先依照$p_{θe}(z|c)$抽样一个潜在动作 $z$(2) 基于$p{θd} (x|z)$，即基于z抽样生成的响应x, 其中$p{θe}$是对话编码器网络, $p{θ_d}$是响应的解码器网络。在上述设置下，LaRL将潜在变量$z$作为其动作空间，而不是响应 $x$中输出单词。之后可以在潜在动作空间中应用reinforce:与Eq 2相比，LaRL的不同之处在于: 将范围从$TU$缩短到$T$。 潜在动作空间设计为低维，比V小得多。 策略梯度只更新编码器$θ_e$, 而解码器$θd$保持不变。这些属性减少了对话策略优化的难度，并将高层决策与自然语言生成分离开。$给定上下文$c$, $p{θe}$ 负责选择最好的潜在动作, 而$p{θ_d}$仅负责将$z$转换为表层形式的单词。本文的公式也为实验各种类型的模型学习方法提供了一个灵活的框架。在本文中，本文主要关注两个关键方面:潜在变量$z$的类型, 以及在有监督的预训练阶段, 优化学习$z$的方法。 Types of Latent Actions以往的研究中使用了两类潜在变量:连续各向同性高斯分布(Serban et al.， 2017b)和多元分类分布(Zhao et al.， 2018)。这两种类型都与本文的LaRL框架兼容，定义如下： Gaussian Latent Actions高斯潜在动作遵循$M$维多元对角协方差矩阵的高斯分布, 即$z\\sim N(\\mu ,\\sigma^2I )$。编码器$p_{θe}$由两部分构成:一个上下文编码器$F$, 用神经网络编码对话上下文$c$为一个向量表示$h$, 用一个前馈网络$pai$将$h$映射为$\\mu$和$\\sigma$ 。流程定义如下：然后将得到的$z$作为解码器的初始状态用于最后响应的生成。使用$p\\theta(z|c) = N(z; \\mu ,\\sigma^2I)$来计算Eq 3中的策略梯度更新. Categorical Latent Actions分类潜在动作是$M$个独立的$k$向分类随机变量。每个$z_m$都有自己的token embedding，将潜在的符号映射到向量空间$E_m \\epsilon R^{K×D}$，其中$m \\ epsilon [1,M]$， D为embedding size。因此 M个潜在动作可以表示指数级的，$K^M$个，不同且唯一的组合， 这使得它具有足够的表现力，可以在复杂的领域中建模对话动作。类似于高斯潜在动作，我们有与高斯潜在动作不同, 分类潜在动作矩阵编码后得到大小为 $R^{M×D}$ 的矩阵, 而解码器的初始状态是一个大小为 $R^D$ 的向量。以前的工作将潜在的emdedding相加，来集成这个矩阵和编码器, 表示作为求和融合。这种方法的一个限制是，它可能会丢失每个潜在维度中的细粒度顺序信息，并且在涉及多个对话动作的长响应方面存在问题。因此，本文提出一种新的方法，Attention Fusion，来结合分类潜在的动作与解码器。 将注意力机制(Luong et al.， 2015)应用于潜在的动作。设i为解码时的步长索引。然后有: Optimization Approaches 现在给定一个训练数据集{x, c}, 常规的优化方法是通过最大化其变分下界的随机变分推理（Full ELBO） 补充：在ELBO其中q是关于z的一个任意概率分布。 对任意分布q的选择来说，L提供了似然函数的一个下界。 越好地近似p(z∣x,c)的分布q(z∣x，c)，得到的下界就越紧。 将推断问题看作是找一个分布q使得L最大的过程。 $q_{\\gamma }(z|x, c)$ 为需要训练的神经网络，来逼近 $q(z|x, c)$ 和$q(z|c)$ 和 $p (x|z)$ 的后验分布。本文认为上式有局限性，解码器只看到 $z$ 从 $q(z|x, c)$ 采样，但未经历过从 $p_\\theta(z|c)$ 采样, 所以提出了以下优化函数（Lite ELBO）：本质上，简化目标将后验网络设置为与编码器相同，即 $q_\\gamma(z|x, c) = p_{θe} (z|c)$, 但是这也使得潜在空间缺少正则化，实验中表明，如果只最大化$p{p_{(z|c})}(x|z)$ 会存在过拟合。因此添加了额外的正则化项, 鼓励后验与先验分布能够相似, β是一介于0和1之间的超参数。将分类潜在动作的 $p(z)$ 设为平均值，即 $p(z) = 1/K$ ；将高斯潜在动作的先验设为 $N(0,I)$ . Experiment Dataset DealOrNoDeal是一个谈判数据集，包含基于2236个不同场景的5805个对话。本文提出252个测试环境场景，并从训练集中随机抽取400个场景进行验证。从混乱度(PPL)、奖励、一致性和多样性四个方面对研究结果进行了评价。PPL帮助确定哪个模型产生的与人更相似的响应，同时奖励和一致性评估模型的谈判强度。多样性表示该模型是发现了一种新的语篇级策略，还是只是重复了单调响应。 Multiwoz是一个插槽填充数据集，它包含10438个对话, 涉及6个不同的域中。8438个对话用于训练，各1000个用于验证和测试。由于该数据集之前没有用户模拟器，为了与之前的技术水平进行公平的比较，本文将重点放在中提出的对话-上下文-文本生成任务上。该任务要求在对话中的每个系统轮次生成响应对话。结果从三个方面进行评估:BLEU，Inform Rate和成功率。BLEU score检查响应层次词汇相似性，而Inform rate 和成功率度量模型是否在dialog级别是否回答问题并提供所有请求的信息。 Conclusion最后，本文为E2E dialogue agent中的RL提出了一个潜在变量的动作空间。 并对两种对话任务进行了评估，结果表明，本文的模型取得了优异的性能，并创造了一个MultiWoz最新的成功率。广泛的分析使我们能够深入了解如何正确地训练潜在变量，这些变量可以作为agent的动作空间，能够以无监督的方式创建抽象的动作。","link":"/2019/05/21/Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models/"},{"title":"Switch-based Active Deep Dyna-Q Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning","text":"D3Q的一种新的改进方法。 题目：Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning 作者：Yuexin Wu, Xiujun Li, Jingjing Liu, Jianfeng Gao, Yiming Yang 来源：AAAI 2019 链接： https:// arxiv.org/pdf/1811.0755 0.pdf IntroductionProblem基于强化学习的任务型对话agent在训练时通常需要大量真实的user experience. Dyna-Q算法扩展了Q-learning，引入worldmodel的概念，尝试用 world model 产生的simulated experience来有效促进agent的训练。但是Dyna-Q的质量取决于world model的质量，或者进一步说，取决于在学习过程中使用的real experience和simulatedexperience的比例。本文引入一个switcher 去动态的决定在不同的训练阶段是用真实的还是模拟的experience.同时本文不同于以往的随机生成模拟经验，而采用了active sampling策略，生成状态动作空间中未被agent完全探索的模拟经验。本文也是首次将active learning应用到任务型对话上。 BackgroundDyna算法：Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。Dyna learns a model from real experience, learn and plan value function(policy) from real and simulated experience.(model-based) Dyna 算法 该方向的思路历程： Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning 优点：只使用少量的真实用户交互数据，基于集成规划的方法针对任务完成型对话对对话策略进行学习。 缺点：在使用大量的模拟数据时，因为在开始阶段agent对低质量的experience不敏感，但在后期则会有很大影响，因为agent对噪声更敏感了（之前的解决方法是设置参数K，来控制planning的次数）。并且生成模拟经验时是通过均匀的抽样usergoal，就存在有没被探索的状态动作空间。 Chris：Deep Dyna-Q: 任务型对话策略学习的集成规划 2. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning 3. Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy LearningLearning. Model基于Switch-DDQ的对话系统分为6个模块： 基于LSTM的NLU模块，识别对话意图，提取相关的slot state tracker, 用于追踪对话状态 dialogue policy, 基于当前的对话状态选取下一个对话动作 基于模型的NLG模块，将对话动作转为自然语言 World model, 用于生成模拟的user action和奖励 基于动态选择的user goal 一个基于RNN的 Switcher 具体Switch-DDQ模型分为四个步骤： (1) Direct Reinforcement Learning，即agent与真实用户交互，收集真实对话经验，完善对话策略;(2) Active planning, agent与simulator进行交互，利用模拟经验改进策略(2) World model learning，应用收集的真实对话经验进行world model的学习(4) Switcher training: switcher是通过真实和模拟的经验来学习和改进的进一步解释： Direct Reinforcement Learning 在这个步骤中基于真实对话经验，应用DQN来学习对话策略。在每一步中，agent通过当前的对话状态 ， 通过最大化价值函数，选择要执行的动作 。然后agent会接收到奖励 ,以及用户的反馈 ，更新当前状态到 .存储经验 到经验池 。(line 12 in Algorithm1)Planning的过程也是基于相同的Q-learning算法，只是planning使用的是模拟的对话经验。 Active Planning based on World Model Active user goal sampling module在planning的阶段，world model可以选择性的生成模拟经验–在agent未被完全探索的状态动作空间。假设我们已经从human-human的对话数据中收集了大量的user goal, 然后把usergoal分成不同类别，有不同的限制条件和难度。在训练过程中，当监控验证集上agent的表现时,我们可以通过对话的成功率来收集每一类usergoal对agent性能改进的影响，通过这样的信息来指导world model如何来选择user goal。 - 基于当前agent的表现，对话失败率 ![f_i ](https://www.zhihu.com/equation?tex=f_i+) 越高更意味着于注入了更难的情况（包含了更多需要学习的有用的信息） - 被估计为不稳定的类别（因为它被抽样的数量 ![n_i](https://www.zhihu.com/equation?tex=n_i) 小）可能有较高的失败率，因此值得分配更多的训练实例去降低不确定性。 关于user goal的选择概率 符合高斯分布，假设有 类user goal，每个类别在验证集的失败率为 , 其取样的数量为 ，总体的取样数量为, 设方差为作为 不确定性的度量, 上式则表示每一个user goal i被选中的概率 ，选择具有最大的 作为选择的user goal。 Response generation module 用真实的对话数据来训练world model ，使其能够产生模拟的对话经验。在每个对话回合中，在world model输入当前对话状态 和上一轮的agent action ，从而生成用户的回复动作 、奖励 和一个表示对话是否结束的信号 。网络结构如图所示：两个 shared layer，三个 taskspecific layer用来完成分类任务。 SwitcherSwitcher基于一个二分类器，使用LSTM模型。假定一组对话是由一个序列的对话轮次构成的： 是对话的轮次数，Q-learning中采用 作为一个训练样本，可以从一个对话中的两个连续对话轮次中提取。设计Switcher可以turn-based或者dialogue-based,因为基于轮次比基于对话数据量更大，所以本文选择turn-based，则对于给定的一组对话，对它的每一轮的质量进行打分，然后将每一轮的分数取平均来衡量这组对话的质量，如果对话层次的得分低于阈值，agent则转换成和realuser来交流。 每一轮次的打分，需要把前面的轮次都考虑进去。给定一个对话轮次 ,它的历史信息为，用LSTM在隐藏层去编码历史信息 ，最后通过一个sigmoid层输出基于轮次层次的质量分数。 将交叉熵作为Score(.)训练的损失函数，其中 和 是分别存放真实经验和模拟经验的经验池。 由于 和 中存储的经验会在对话训练过程中发生变化，所以switcher的分数函数也会随之更新，从而自动调整在不同训练阶段的planning的执行情况。 ExperimentDataset本文用到的数据集是电影订票场景中的原始会话数据，如表1所示，表1由11个对话动作和16个slot组成。数据集总共包含280个注释对话，每个对话的平均长度约为11轮。 Comparison 在电影票预订领域，对Switch-DDQ进行了两种评估:模拟和人工评估，在模拟评估中使用了开源的面向任务的用户模拟器(Li et al.2016)。为了测试Switch-DDQ的性能，文中使用了不同版本的任务完成型对话agent。 对于每个agent，文中以成功率、平均奖励和平均轮数展示实验的效果。如图所示，Switch-DDQ在前100个epoch后，在交互轮数较少的情况下，始终能够获得较高的成功率。 从实验结果也可以看出DDQ(5)所采用的策略，虽然在训练的早期帮助更快的更新策略网络，但是在后期由于使用了低质量的训练实例影响了性能，所以DDQ(5)无法获得与DQN类似的性能。然而，在Switch-DDQ中并不会出现这种情况，它没有参数K,因为实际和模拟的经验比例是由switcher模块自动控制。 Conclusion本文主要在上一篇论文DDQ的基础上引入了 一个Switcher, Switch-DDQ能够自适应地从真实用户或world model中选择要使用的数据源，提高了对话策略学习的效率和鲁棒性。Switch-DDQ也可以看作是一种基于模型的通用RL方法，并且很容易扩展到其他RL问题。 Reference [1] Su, S.-Y.; Li, X.; Gao, J.; Liu, J.; andChen, Y.-N. 2018. Discriminativedeep dyna-q: Robust planning for dialogue policy learning. arXiv preprintarXiv:1808.09442. [2]Peng, B.; Li, X.; Gao, J.; Liu, J.; Wong, K.-F.; and Su, S.-Y. 2018.Deep Dyna-Q: Integrating planning for task-completion dialogue policylearning. In ACL [3]Li, X.; Chen, Y.-N.; Li, L.; Gao, J.; and Celikyilmaz, A. 2017. End-to-end task-completion neural dialogue systems. arXiv preprint arXiv:1703.01008. [4]Schatzmann, J.; Thomson, B.;Weilhammer, K.; Ye, H.; and Young, S. 2007.Agenda based user simulation for bootstrapping a pomdp dialogue system. InHuman Language Technologies 2007: The Conference of the North American Chapterof the Association for Computational Linguistics; Companion Volume, ShortPapers, 149–152. Association for Computational Linguistics. [5] Chris：Deep Dyna-Q: 任务型对话策略学习的集成规划 ps: 欢迎大家关注我的知乎~ –&gt;戳这里&lt;–","link":"/2019/03/04/Switch-based-Active-Deep-Dyna-Q-Efficient-Adaptive-Planning-for-Task-Completion-Dialogue-Policy-LearningLearning/"},{"title":"Task-Oriented Conversation Generation Using Heterogeneous Memory Networks","text":"来源；ACL2019 链接： https://www.aclweb.org/anthology/P19-1226.pdf Sih: 端到端对话生成任务中，用不同结构memory进行堆叠, 来处理不同来源的信息，应用context-aware memory和 context-free memory ，且context-aware memory加入了门机制。 DiDi provide paper. Introduction现有的问题如何将外部知识整合到端到端的任务型对话生成任务中是目前研究的重点, 但目前存在的问题是, 现有的memory network 在利用不同来源的异构信息时表现并不好。 创新点本文提出了一个异构记忆网络Heterogeneous Memory Networks (HMNs), 在端到端对话生成中利用两种不同的记忆网络 context-aware memory和 context-free memory 建模对话历史和外部知识，希望用编码历史对话的表征来查找到外部知识中的表征，且在 context-aware memory中加入了门机制。 Model Encoder主要的idea：context-aware memory with gate. Encoder端采用context-aware memory 将对话历史编码为上下文向量，具体做法： 输入：对于对话的历史信息，其每个单词由token，轮次，说话人身份三个部分组成： $[(hello, t1, user),(may, t1, sys),(I, t1, sys),(help,t1, sys), (you, t1, sys)]$ 每个单词先通过embedding look up转换成词向量，再将每个元组中的所有词向量相加，作为context-aware memory的输入，希望最后得到的输出是上下文向量 $c$。 则 k 跳的memory cells 为 $n^k = cat[n^k_1,n^k_2,…,n^k_l ]$ , 每个cell之间还要通过一个采用双向GRU的门控机制。 输入的上下文表示为$n^k = [\\underset{n^k}{\\rightarrow},\\underset{n^k}{\\leftarrow}]$， 正向过程表示为： 然后对query $qc^k$和memory cell $n^k$ 进行attention操作： 输出：将上下文向量$u^k$ 再与query相加，得到k 跳之后encoder的输出 $oc^k$. Decoder主要的idea： 堆叠两个不同的memory 解码器包含提出的HMNs和一个RNN controller, 其中HMNs是context-aware memory和context-free memory两种memory的堆叠，controller来控制HMNs查询的过程。具体做法是将对话历史加载到context-aware memory中，而知识库三元组被加载到context-free memory中，HMNs接受encoder端的输出作为输入，然后先遍历context-aware memories， 利用它最后一跳的输出$u^k$将用来查询context-free memory。 一些中间解释： 输入：Encoder输出的上下文向量 $c$ 输出：在每个时间步中，HMNs会生成三个量： $oc^1$， $P_{his}$, $P_{kb}$, $P_{vocab}$ 1）context aware memory 中对对话历史编码的第一跳的输出 $oc^1$ 2）两个memory中最后一跳分别得到的两个注意力权重：对话历史的单词分布 $P_{his}$ 和知识的分布 $P_{kb}$。将上下文向量$oc^1$ 和隐藏层$h_t$拼在一起，来得到预测词表分布的概率$P_{vocab}$。 然后采用Word Selection Strategy 从 ${P_{vocab},P_{kb}, P_{his}}$的三个分布中选择一个单词: 如果单词在$P_{kb}, P_{his}$ 的分布概率高，并且不指向哨兵位置上，则直接比较每个单词的概率，并选择较高的那个。 如果其中一个词指向哨兵位置，模型将在另一个词表中选择概率最高的单词。最后，如果两个词表都指向哨兵位置，将从$P_{vocab}$ 中选择这个词。 本文采用copy机制从memory中来copy单词，用 $P_{kb}$和$P_{his}$来计算生成目标词的概率。如果目标单词没有出现在输入的句子当中，则指向memory中的最后的位置，哨兵标记。 Controller采用GRU作为一个controller，接受encoder输出的上下文向量 $c$ 作为初始的隐藏状态 $h_0$。在每个时间步长中，用先前的生成词的词向量 $E(g_{t-1})$和上一个的隐藏状态$h_{t-1}$作为输入，来控制HMNs的查询过程。 Loss在每一个时间步长，优化的损失函数是该时间步长 $t$ 下对目标单词预测的三个概率分布( ${P_{vocab},P_{kb}, P_{his}}$)的负对数似然求和： ExperimentDataset Key-Value Retrieval dataset dialog bAbI tasks DSTC 2 Evaluation:BLEU, F1 score, Per-response accuracy and Per-dialog accuracy. 做了四个对比实验： SEQ2SEQ ：一层LSTM做编解码器。 SEQ2SEQ + Attention Mem2Seq HMNs with context-free only (HMNs-CFO)：来测试 context aware的性能 Result analysis 文中认为该模型work的一个原因是，一个memory很难学习好来自不同的来源的分布。因此文中采用的分别对对话历史和知识用不同memory进行编码，可以得到两种比一般分布更好的分布，也说明了使用历史记忆生成的查询向量在知识库内存中检索信息是合理的。 HMN表现优于HMN-CFO，表明建立更好的对话历史记录的表示方式有益于知识的推理，并且能够通过改进查询向量来帮助改善context free memory。 堆栈式存储网络体系结构和使用context aware memory来加载历史信息可以提高检索知识和生成语句的性能。 模型缺点：不能解决large scare knowledge的问题。 Conclusion文章提出了一个能够在端到端对话系统中整合异构信息的模型，具体是将异构memory network应用于对话历史和知识库的建模。提出的上下文感知记忆网络(context-aware memory networks)在学习对话历史的分布和检索知识方面表现较好。提供了在端到端面向任务的对话系统中高效使用各种结构化数据的可能性，而无需任何额外的标签和模块训练。 Sih: 用不同结构memory处理不同来源的信息并且进行堆叠值得借鉴。 Reference ：[1] Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension. ACL2019 https://www.aclweb.org/anthology/P19-1226.pdf [2] Multi-Level Memory for Task Oriented Dialogs. NAACL2019 https://arxiv.org/pdf/1810.10647.pdf [3] End-To-End Memory Networks. NIPS 2015 https://arxiv.org/abs/1503.08895 [4] Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue EMNLP 2109 https://liusih.github.io/2019/09/25/Multi-task%20Learning%20for%20Natural%20Language%20Generation%20in%20Task-Oriented%20Dialogue/","link":"/2019/11/20/Task-Oriented Conversation Generation Using Heterogeneous Memory Networks/"},{"title":"Visual Dialogue 总结","text":"CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog NAACL 2019 链接： https://arxiv.org/pdf/1903.03166.pdf 摘要：可视对话是一种多模态任务，使用对话历史作为上下文，以图像为基础回答一系列问题。它是视觉、语言、推理和基础方面的挑战。然而，在大型真实数据集上单独研究这些子任务是不可行的，因为它需要对所有图像和对话框的“状态”进行代价高昂的完整注释。我们开发了一个大型诊断数据集，用于研究可视化对话框中的多轮推理。具体地说文章构建了一个基于CLEVR dataset 图像的场景图的对话语法。 其中可视对话的所有方面都得到了完整的标注。 总共包含了5个10轮对话框的实例，用于大约85k个cleveland图像，总共有425万对问答对。我们使用CLEVR-Dialog来测试标准可视对话模型的性能; 特别是视觉上的共参考分辨率(作为共参考距离的函数)。这是对可视对话模型的第一次分析，没有这个数据集是不可能的。文章希望CLEVR-Dialog的发现将有助于开发未来的可视化对话模型。数据集和代码将公开。 Image-Question-Answer Synergistic Network for Visual Dialog CVPR2019 链接： https://arxiv.org/abs/1902.09774 摘要: 图像、问题(结合对话历史)和相应的答案是可视化对话的三个重要组成部分。经典的可视对话系统集成了图像、问题和对话历史搜索或生成最佳匹配的答案，因此，这种方法明显忽略了答案的作用。本文设计了一种新颖的图像-问答协同网络来评价问答在精确的视觉对话中的作用。我们将传统的单阶段解决方案扩展为两阶段解决方案。在第一个阶段，根据候选答案与图像和问题对的相关性对它们进行粗略的评分。然后，在第二阶段，通过与图像和问题的协同作用，对正确率较高的答案进行重新排序。在可视化对话v1.0数据集上，提出的协同网络增强了判别式可视化对话模型，实现了57.88% \\%的归一化折现累积增益。一个生成的可视化对话模型也显示了很好的改进。 Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog ACL 2019 Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, Jianfeng Gao 链接： https://arxiv.org/abs/1902.00579 摘要：本文提出了一种新的视觉对话模型——递归双注意网络(ReDAN)，利用多步推理的方法来解决图像的一系列问题。在对话的每个问答环节中，ReDAN通过多个推理步骤逐步推断出答案。在推理过程的每一步中，根据图像和之前的对话历史，对问题的语义表示进行更新，并在后续的步骤中使用递归细化的表示进行进一步的推理。在VisDial v1.0数据集上，提出的ReDAN模型获得了64.47%的NDCG评分。推理过程的可视化进一步证明了ReDAN可以通过迭代细化找到上下文相关的视觉线索和文本线索，从而一步一步地得到正确的答案。 Sequential Attention GAN for Interactive Image Editing via Dialogue CVPR2019 Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, Jianfeng Gao 链接：https://arxiv.org/abs/1812.08352 摘要：在本文中，我们介绍了一种新的任务——通过会话语言进行交互式图像编辑，用户可以通过多回合的自然语言对话引导agent对图像进行编辑。在每个对话轮中，代理接受来自用户的源图像和自然语言描述作为输入，并在文本描述之后生成一个新图像。为此任务引入了两个新数据集，Zap-Seq和DeepFashion-Seq。本文提出了一种新的顺序注意生成对抗网络(SeqAttnGAN)框架，该框架利用神经状态跟踪器对每个对话轮中的源图像和文本描述进行编码，生成与前面图像和对话上下文一致的高质量新图像。为了更好地实现区域特定的文本到图像的生成，我们还在模型中引入了注意机制。在这两个新的数据集上的实验表明，所提出的SeqAttnGAN模型在基于对话框的图像编辑任务上优于现有的SOTA方法。详细的定量评估和用户研究也表明，我们的模型在图像生成方面比SOTA基线更有效，无论是在视觉质量还是文本到图像的一致性方面。 StoryGAN: A Sequential Conditional GAN for Story Visualization CVPR2019 Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao 链接： https://arxiv.org/abs/1812.02784 摘要：我们提出了一个新的任务，叫做故事可视化。给定一个多句的段落，通过为每个句子生成一个图像序列来可视化故事。与视频生成不同，故事可视化不太关注生成图像(帧)中的连续性，而是更关注动态场景和角色之间的全局一致性——这是任何单一图像或视频生成方法都无法解决的挑战。因此，我们提出了一个新的基于顺序条件GAN框架的故事到图像序列生成模型StoryGAN。我们的模型是独特的，因为它由一个动态跟踪故事流的深层上下文编码器和两个在故事和图像级别的鉴别器组成，以增强图像质量和生成序列的一致性。为了评估模型，我们修改了现有的数据集，以创建CLEVR-SV和Pororo-SV数据集。从经验上看，StoryGAN在图像质量、上下文一致性度量和人类评估方面都优于最先进的模型。 Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation CVPR2019 Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa 链接: https://arxiv.org/abs/1903.02547 摘要：我们提出了带有回溯(FAST)导航器的边界感知搜索，这是一个用于动作解码的通用框架，在Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson et. al. (2018) 挑战中实现了最先进的结果。给定一个自然语言指令和一个以前从未见过的环境的真实感图像视图，该代理的任务是尽可能快地从源导航到目标位置。虽然目前所有的方法都使用波束搜索来做出局部的行动决策或对整个轨迹进行评分，但我们的方法在探索一个未被观测的环境时平衡了局部和全局信号。重要的是，这让我们可以贪婪地行动，但在必要时使用全球信号进行回溯。将快速框架应用于现有的最先进的模型，获得17%的相对增益，绝对6%的成功率加权路径长度(SPL)。 Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation CVPR2019 Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang 链接：https://arxiv.org/abs/1811.10092 摘要：视觉语言导航(VLN)是在真实的三维环境中，通过导航一个具体的代理来执行自然语言指令的任务。在本文中，我们研究如何解决这一任务的三个关键挑战:跨模态接地，病态反馈和泛化问题。首先，我们提出了一种新的增强交叉模态匹配(RCM)方法，该方法通过增强学习(RL)来实现局部和全局的交叉模态接地。特别地，匹配批评家被用来提供一个内在的奖励来鼓励指令和轨迹之间的全局匹配，推理导航仪被用来在局部视觉场景中执行跨模态接地。对VLN基准数据集的评估表明，我们的RCM模型在SPL上的性能显著优于现有方法的10%，达到了最新的性能水平。为了提高学习策略的通用性，我们进一步介绍了一种自我监督模仿学习(SIL)方法，通过模仿自己的过去来探索未知的环境。我们证明SIL可以近似于一个更好和更有效的策略，这极大地减小了可见和不可见环境之间的成功率性能差距(从30.7%到11.7%)。 LARGE-SCALE ANSWERER IN QUESTIONER’S MIND FOR VISUAL DIALOG QUESTION GENERATION ICLR 2019 链接： https://arxiv.org/pdf/1902.08355.pdf 摘要： Answerer in Questioner’s Mind (AQM)是近年来提出的一种面向任务的对话系统的信息理论框架。AQM从提出一个问题中获益，当它被问到的时候，这个问题可以最大限度地获取信息。 然而，由于其本质上是显式计算信息增益， 当解空间很大时，AQM有一定的局限性。为了解决这个问题，我们建议使用AQM+来处理大规模的问题，并提出一个与当前对话上下文更一致的问题。我们对我们的方法进行了评估。 这是一个具有挑战性的面向任务的可视化对话问题，其中候选类的数量接近10K。我们的实验结果和消融研究表明，在合理的近似下，AQM+的性能显著优于最先进的模型。特别地，所提出的AQM+在对话过程中减少了60%以上的错误，而比较算法减少了不到6%的错误。基于我们的研究结果，我们认为AQM+是一种通用的面向任务的对话算法，可以应用于非yes-or-no响应。","link":"/2019/06/09/Visual Dialog 总结/"},{"title":"jupyter的安装和配置","text":"1.安装jupyter pip install jupyter 如果安装报错，则安装 sudo apt-get install libsqlite3-dev 2.生成配置文件： jupyter notebook --generate-config (该文件在 /home/username/.jupyter/jupyter_notebook_config.py) 3.生成密码： #ipython (或者Python） from notebook.auth import passwd passwd() Enter password: #在此处输入密码 Verify password： #确认输入密码 然后会生成： Out：‘sha1:5ab...............' #为生成的密码，复制一下，后面会用到 4.修改配置文件 ~/.jupyter/jupyter_notebook_config.py vim ~/.jupyter/jupyter_notebook_config.py 此处需要更改的： c.NotebookApp.ip='*’ #此处默认为localhost,更改为任意ip c.NotebookApp.password = u'sha1:5ab..............' #此处为之前自行设置的密码，粘贴到此处 c.NotebookApp.open_browser = False c.NotebookApp.port =8888 #该处端口号可任意设置，8889，9999等，默认为8888 #ps:注意！！！！一定要去掉前面的注释，才可以有用。 保存退出 esc shift+: wq 启动jupyter jupyter notebook 然后打开浏览器输入： 1.如果是远程登录linux服务器： https://服务器地址:8889/ (此处8889为之前设置的端口号） 2.如果是本机： https://localhost:8889/","link":"/2018/09/25/jupyter的安装和配置/"},{"title":"w2v词向量模型的训练和使用","text":"0.数据处理针对wiki 数据集： 下载opencc把繁体中文转化为简体中文opencc(windows版本)下载地址：http://download.csdn.net/download/adam_zs/10213686 运行命令： opencc -i wiki.zh.fanti.text -o wiki.zh.jianti.text -c t2s.json jieba分词 a. 中文 def cut_words(sentence): # print sentence return \" \".join(jieba.cut(sentence)).encode('utf-8') b. 英文 引入日志配置 import logging logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 引入数据集 #some error here `# raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"] 切分词汇 sentences= [s.encode('utf-8').split() for s in sentences] 1.训练 step1: train a. #LineSentence model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count()) #min_count 最好设置为1，5的话容易匹配不到词典 b. #Text8Corpus sentences=word2vec.Text8Corpus(u'wiki_seg.txt') model=gensim.models.Word2Vec(sentences, , size=200min_count=3,) step2:save #保存.model a model.save(outp1) # outp1 可以为.model,.vect,.txt,.bin #保存.model b model.model.wv.save_word2vec_format(outp2, binary=False) #保存 #保存 c model.save('wiki.model') #.model model.save_word2vec_format('wiki.model.bin', binary=True) #.bin model.model.wv.save_word2vec_format(outp2, binary=False) #.bz step 3 基于模型继续训练 model=gensim.models.load('wiki.model') model.build_vocab(new_sentences, update=True) model.train(x, total_examples=model.corpus_count, epochs=model.iter) model.save('wiki.model') ps:#中间步骤 调用Word2Vec创建模型实际上会对数据执行两次迭代操作，第一轮操作会统计词频来构建内部的词典数结构，第二轮操作会进行神经网络训练，而这两个步骤是可以分步进行的，这样对于某些不可重复的流（譬如 Kafka 等流式数据中）可以手动控制： model = gensim.models.Word2Vec(iter=1) # an empty model, no training yet model.build_vocab(some_sentences) # can be a non-repeatable, 1-pass generator model.train(other_sentences) # can be a non-repeatable, 1-pass generator 2.载入 loadmodel = gensim.models.Word2Vec.load('wiki.model') #载入 .model模型 model1 = word2vec.Word2Vec.load_word2vec_format('wiki.model.bin', binary=True) #载入bin model=gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True) #载入 .bin模型 3.测试a.前几个最相关的词和相关性 res= model.most_similar（‘女孩’） b.两个词之间的相关性 model.similarity('dogs','you') c. 直接获取向量 model['电脑'] 4.Word2Vec 参数- min_count model = Word2Vec(sentences, min_count=10) # default value is 5 在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。 - size size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。 model = Word2Vec(sentences, size=200) # default value is 100 - workers workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用： model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization","link":"/2018/09/25/w2v词向量模型的训练和使用/"},{"title":"医疗对话系统总结","text":"Sih对目前医疗诊断领域任务型对话的总结。 End-to-End Knowledge-Routed Relational Dialogue System for Automatic Diagnosis 2019 AAAI https://arxiv.org/abs/1901.10623v2 Task-oriented Dialogue System for Automatic Diagnosis 2018 ACL https://www.aclweb.org/anthology/P18-2033 Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learning 2018 AAAI http://infolab.stanford.edu/~echang/aaai-2018-context.pdf Abstract本文提出了一个自动医疗诊断对话系统，通过与患者对话，收集患者自我报告之外的其他症状，自动做出诊断。现有的对话系统主要依赖于数据学习，无法对额外的专用知识图谱（expert knowledge graph）进行编码。在这项工作中提出了一个端到端知识路由相关对话系统 （KR-DS），将丰富的医学知识图谱整合到话题转换中，使其与NLU，NLG相互协作。本文使用基于Knowledge-routed的dqn网络 （krdqn） 来管理主题转换，它集成了一个编码不同症状和症状-疾病对之间关系的关系细化分支和一个用于主题决策的知识路由图分支。在公共医疗对话数据集上的大量实验表明，本文提出的KR-DS模型明显优于目前最先进的方法（诊断准确率超过8%）。另外，本文从在线医学论坛收集了一个新的医疗数据集用于验证实验结果。 Introduction1. 医学对话系统的难点 对话系统询问的症状应与潜在疾病相关，并与医学知识相一致。然而当前面向任务的对话系统高度依赖于复杂的belief tracker和纯数据驱动的学习, 由于这样的诊断缺乏医学知识的支撑所以它无法直接适用于自动诊断。 2. 现有的自动诊断系统和其缺点 Task-oriented Dialogue System for Automatic Diagnosis(2018 ACL) 率先构建了一个用于自动诊断的对话系统，将对话系统转换为马尔可夫决策过程，并通过reinforcement learning训练对话策略。缺点：这项工作只是利用DQN网络通过数据驱动学习来管理主题转换(确定应该询问哪些症状)，而且结果复杂且重复。此外，本工作仅针对对话管理进行对话状态跟踪和策略学习，使用基于模板的自然语言处理模型，无法很好地匹配实际的自动诊断场景。 3. 本文的解决方法 本文提出了一个完整的端到端知识路由关系（KR-DS）自动诊断的对话系统，结合医学知识图谱和基于症状-疾病关系进行话题转换的对话管理（DM），并使其与NLU和NLG结合，如图1所示。主要提出两个branch：knowledge-routed graph branch和 relational refinement branch。一般来说，医生根据医学知识和诊断经验（medical knowledge and diagnosis experience）来确定疾病类型。受此启发，本文主要提出两个branch：knowledge-routed graph branch和 relational refinement branch。 Relational refinement branch 从历史诊断数据（historical iagnostic data）中学习不同症状和症状-疾病对之间（symptom-disease pairs）的关系，从而细化由basic DQN生成的粗略结果。 Knowledge-routed graph branch 通过基于条件概率的医学知识路由图谱的先验信息来帮助决策。这两个分支通过knowledge guiding 和 relation encoding产生更加合理的决策结果。 4. 本文主要贡献： 本文进行关系建模（relation modeling）来处理相关的症状和疾病之间的关系，还进行了图谱推理（graph reasoning）来指导利用已有的医学知识进行策略学习。并且进一步介绍了一个面向终端任务的自动诊断对话系统和一个新的医疗对话系统数据集。 Adding: 医疗对话是如何进行的？ Proposed Work本文提出的对话系统（krds）如图1所示，作为一个面向任务的对话系统，它包括NLU,DM,NLG。DM根据当前对话状态执行主题转换，agent学习通过询问症状来诊断疾病的任务，最后应告知患者疾病类型。此外本文也使用了用户模拟器来生成基于user goal的对话。 Policy Learning with KR-DQN Natural Language Understanding 中文医疗诊断数据集 在该诊断系统中，假设有$M$种disease，$N$种symptom. 4种agent action:request+symptom, inform+disease, thanks, closing.所以agent的action space的大小$D$可以表示为：$D = num_{greeting}+M +N$. 4种user action:request+disease, confirm/deny/not-sure+symptom, thanks, closing 4种symptom states：positive, negative, not-sure and not mentioned ,在symptom vectors里表示为1, - 1, -2, 0 NLU和RL进行联合训练。 Policy Learning with KR-DQN Basic DQN Branch.先利用一个Basic DQN分支来生成一个粗略的动作结果，如公式1所示。MLP以状态$s_t$作为输入，输出一个粗糙的动作结果，先有一个粗略的DQN结果再对结果进行优化。 $$a_t^r = MLP(s_t)$$ Relational Refinement Branch关系模块可以通过从一组其他元素的聚合信息来影响单个元素(例如，其他症状及疾病)。由于关系权重（relation weights）是由任务目标驱动自动学习的，因此关系模块可以对元素之间的依赖关系建模。因此本文设计了一个relational refinement module, 通过引进一个关系矩阵$R \\epsilon R^{D\\times{D}}$ 来表示所有动作间的依赖关系。Basic DQN 预测的动作是$a_t^r$, 再乘以学习到的关系矩阵R得到新的action $a^f_t \\epsilon R^D$, 该关系矩阵是不对称的，表示有向加权图。细化的动作向量$a^f_t$的每一个元素为初始预测动作$a_t^r$的加权和，其中权值表示元素之间的依赖关系.$$a^f_t = a^r_t ·R$$关系矩阵$R$由数据集的统计量进行条件概率的初始化。每个条目$R_{ij}$表示单位$x_j$以单位$x_i$为条件的概率。关系矩阵通过反向传播学习捕获动作之间的依赖关系。实验表明，这种初始化方法优于随机初始化，因为先验知识可以指导关系矩阵的学习。 Knowledge-routed Graph Branch（与其他对话系统的不同之处：以user的self report 作为对话的基础）设计思路：在拿到患者的自我报告时，医生首先要对患者可能存在几种候选疾病有一个大致的了解。随后，通过询问这些候选疾病（diease）的显著症状（symptoms），医生排除了其他候选病，直到确定诊断。受此启发，本文设计了一个知识路由图谱（knowledge-routed graph）模块来模拟医生的思维过程。首先计算症状和疾病之间的条件概率作为有向医学知识图谱的权值，其中有两种类型的节点，如图2所示。edges只存在于疾病和症状之间。每条edge都有两个weights，其中疾病到症状的条件概率为$P (dis|sym)$，症状到疾病的条件概率为$P (sym|dis)$. 在与病人交流的过程中，医生可能会有几种候选疾病。我们将候选疾病概率表示为与观察到的症状相对应的疾病概率。症状先验概率$P_{prior}(sym)$通过以下规则计算: 对于提及到的症状，患有该症状设置为1，未患有该症状设置为-1 其他症状（不确定或未提及）的概率设置为从数据集中统计计算出来的先验概率。那么这些症状的概率 $P_{prior}(sym)$乘以条件概率$P(dis|sym)$得到疾病概率$P(dis)$，公式为:$$P (dis) = P (dis| sym)·P_{prior}(sym)$$考虑到候选疾病，医生经常根据他们的医学知识询问一些值得注意的症状来确诊, 因此由疾病概率$P(dis)$与条件概率矩阵$P (sym|dis)$相乘得到症状概率$P (sym)$:$$P (sym) = P (sym|dis)·P (dis) $$将疾病概率$P(dis)$和症状概率$P (sym)$连接在一起，得到 knowledge-routed action的概率$a^k_t\\epsilon D$。有了这三个action $a_t^r, a_t^f , a_t^k$，然后把三个动作相加得到$a_t$, 作为KR-DQN在当前状态$s_t$下的最终结果。 $$a_t = sigmoid(a_t^r) + sigmoid(a_t^f ) + a^k_t$$ Experiement医疗对话数据集： MZ dataset： （Baidu拇指医生）710个user goal，66个症状，4类疾病。 本文提出的数据集 DX dataset: 527个对话， 41种症状， 5类疾病 评价指标： 使用正确诊断率作为对话的准确性。 匹配率，来评估症状对话系统请求的有效性，成功匹配意味着系统询问用户隐含存在的症状，否则就是失败匹配。具体对话样例（与baseline的比较） Sih conclusion 对话逻辑较为简单，主要是对知识建模，知识推理的过程。 可以将此方法尝试用于multi domain dialog。 利用知识图谱对知识建模值得借鉴。 编码不同症状和症状-疾病对之间的关系，Relation modeling也是值得借鉴的。","link":"/2019/05/12/医疗领域的任务型对话系统/"},{"title":"Reinforcement Learning中的 AC 和 A3C 总结","text":"Sih对AC，A3C的总结。 Actor Critic在RL中的位置： RL 按照value和policy分类： Value-Based：是预测某个state下所有Action的期望价值（Q值），之后通过选择最大Q值对应的Action执行策略，适合仅有少量离散取值的Action的环境 Policy-Based：是直接预测某个state下应该采取的Action，适合高维连续Action的环境，更通用 按照model-based和model-free来分类： model-based：根据state和采取的action预测接下来的state，并利用这个信息训练强化学习模型（知道状态的转移概率） model-free：不需要对环境状态进行任何预测，也不考虑行动将如何影响环境，直接对策略或action的期望价值进行预测，计算效率非常高 回顾：Value-Based &amp; model-free神经网络近似动作-值函数：使用参数 $\\theta$ 来对动作-值函数进行近似$Q(s,a,\\theta)$ ​ One-step Q-learning: $$L_i(\\theta_i) = E(r + \\gamma \\cdot max_{a’}Q(s’,a’;\\theta_{i-1}) - Q(s,a;\\theta_i))^2$$ ​ n-step Q-Learning： $$Q_{Target} = r_t + \\gamma r_{t+1} + … +\\gamma ^{n-1}r_{t+n-1} + \\gamma^n max_{a’}Q(s’,a’;\\theta_{i-1}^-)$$ one-step缺点：得到一个奖励r仅仅直接影响了得到该奖励的状态总做对（s,a）的值。其他state action pair的值仅仅间接的通过更新value Q(s,a)来影响，这使得学习过程缓慢，因为许多更新都需要传播一个reward给相关进行的states和actions。 n-step优点：一个奖励r直接影响先前n个state action pair，学习更有效。 tips: 动作-值函数： $$Q^\\pi(s,a) = E[R_t|s_t = s,a]$$ 状态价值函数： $$V^\\pi(s) = E[R_t|s_t = s ]$$ Policy-Based &amp; model-free直接将策略参数化：$\\pi(a|s;\\theta)$ ，通过迭代更新$\\theta$，使总奖励期望 $E[R_t]$ 梯度上升 AC一句话概括 Actor Critic 方法: 强化学习的分类可以分为以值函数为中心的和以策略为中心的算法，交叉部分就是actor-critic。 结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率(即策略). Actor-Critic的形象解释为：演员在演戏的同时有评论家指点。这样演员就会演得越来越好。同时专家也在不断进步，也就是对演员的动作评价的更加的准确，共同进步，使得效果越来越好。 基于Actor-Critic策略梯度学习分为两部分内容： Critic：参数化动作价值函数 $Q_w(s, a)$ Actor：按照Critic部分得到的价值引导策略函数参数 $\\theta $ 的更新。 Critic做的事情：策略评估，他要告诉个体，在由参数 $\\theta$ 确定的策略 $\\pi_{\\theta}$ 到底表现得怎么样。 Actor Critic 方法的优势: 可以进行单步更新, 比传统的 Policy Gradient 要快.（sih: PG是回合更新） Actor Critic 方法的劣势: 取决于 Critic 的价值判断, 但是 Critic 难收敛, 再加上 Actor 的更新, 就更难收敛. 为了解决收敛问题, Google Deepmind 提出了 Actor Critic 升级版 Deep Deterministic Policy Gradient. 后者融合了 DQN 的优势, 解决了收敛难的问题. Deep Deterministic Policy Gradient (DDPG) 就是以 Actor Critic 为基础. 具体算法 ① 中，$\\pi(a|s;\\theta)$ 表示在st,θ的情况下选择动作atat的概率。概率的对数乘以该动作的总奖励$R_t$，对$θ$求梯度，以梯度上升的方式更新$θ$。该公式的意义在于，奖励越高的动作, 越努力提高它出现的概率。 但是在某些情况下，每个动作的总回报Rt都不为负，那么多有的梯度值都大于等于0，此时每个动作出现的概率都会提高，这在很大程度上减缓了学习的速度，而且使得梯度的方差也很大。因此需要对$R_t$ 使用某种标准化操作来降低梯度的方差。 ② 中，可以让$R_t$减 去一个基线b(baseline)，b通常设为$R_t$ 的一个期望估计，即$b_t(s_t) \\approx V^\\pi(s_t)$，通过求梯度更新θ，总奖励超过基线的动作的概率会提高，反之则降低，同时块还可以降低梯度方差（证明略）。这种方式被叫做Actor-Critic体系结构，其中策略π是actor，基线$b_t$是critic。 ③中，$R_t - b_t(s_t)$可以使用动作优势函数$A^\\pi(a_t,s_t) = Q^\\pi(a_t,s_t) - V^\\pi(s_t)$代替，因为$R_t$ 可以视为$A^\\pi(a_t,s_t) = Q^\\pi(a_t,s_t) - V^\\pi(s_t)$的估计，基线$b_t(s_t)$视为$V^\\pi(s_t)$的估计。 A3CA3C-Asynchronous Advantage Actor-Critic 将AC放到多个线程中同步训练，可以有效的利用计算机资源，提升训练效用。简单概括，就是：它会创建多个并行的环境，让多个拥有副结构的agent同时在这些并行环境上更新主结构中的参数。并行中的agent们互不干扰，而主结构的参数更新受到副结构提交更新的不连续性干扰，所以更新的相关性被降低，收敛性提高。 A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，它提供了一种通用的异步的并发的强化学习框架，这个框架不止可以用于A3C，还能应用于其他的强化学习算法。 A3C的更新公式有两条，一条梯度上升更新策略$\\pi$ 的参数，如前面介绍的actor-critic结构： $$\\nabla_{\\theta’} log \\pi (a_t|st;\\theta’)A(s_t,a_t;\\theta’,\\theta’_v)$$ 其中，$A(s_t,a_t;\\theta’,\\theta’_v)$是优势函数的估计（算法中表示为$R-V(s_i;\\theta’_v))$): $$A(s_t,a_t;\\theta’,\\theta’_v)$$ $$=\\sum_{i=0}^{k-1}\\gamma^ir_{t+i}+\\gamma^kV(s_{t+k};\\theta’_v) -V(s_t;\\theta’_v)$$ $\\theta’$是策略$\\pi$ 的参数，$\\theta’_v$是状态值函数的参数。k是可以变化的，上届由n-step决定，即n。 在实际操作中，论文在该公式中加入了策略$\\pi$ 的熵项$\\beta\\nabla_{\\theta’}H(\\pi(s_t;\\theta’)))$，防止过早的进入次优策略。 另一条公式是使用TD方式梯度下降更新状态值函数的参数，即算法中的$\\partial (R-V(s_i;\\theta’_v))^2 / \\partial\\theta’_v$. 未完待续… Reference： [1] A3C 总结 https://zhuanlan.zhihu.com/p/32596470 [2] Asynchronous Methods for Deep Reinforcement Learning [3] A3C阅读笔记https://ldgyyf.cn/2019/08/05/%E8%AE%BA%E6%96%87/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/DRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%B9%9D%EF%BC%89%E4%B9%8BA3C%E7%AE%97%E6%B3%95/","link":"/2019/11/09/强化学习中的A3C/"},{"title":"Today's User simulator","text":"Dialogue SimulationSimulation-Based Evaluation 为什么用User simulator: 通常，RL算法需要与用户交互才能学习。但是，在招募的用户或实际用户上运行RL可能是昂贵的，甚至是有风险的。解决这一挑战的一种方法是构建一个模拟用户，与RL算法交互几乎不需要任何成本。本质上，User simulator试图模拟真实用户在对话中的行为: a. 跟踪对话状态，b.并与RL对话系统进行对话 一般的User simulator: such as deterministic vs. stochastic content-based vs. collaboration-based static vs. non-static user goals during the conversations 用户模拟器可以在 dialogue-act level 或在句子级别上操作 可以是基于规则也可以是基于模型从真实的对话数据中学到的。 Agenda-Based Simulation ex:As an example, we describe a popular hidden agenda-based user simulator developed by Schatzmann and Young (2009), as instantiated in Li et al. (2016d) and Ultes et al. (2017c). 每个对话模拟从一个随机生成的用户目标开始，对话管理器不知道这个目标。一般来说，用户目标由两部分组成:inform-slots 包含大量的槽值对，作为用户想要强加于对话的约束;request-slots 是用户最初不知道其值的槽，将在对话期间填写。 此外，为了使用户目标更加实际，还添加了领域特定的约束，以便某些slot需要出现在用户目标中。例如，要求用户知道在电影领域中需要多少张票是有意义的 在对话过程中，模拟用户维护一个堆栈数据结构，称为user agenda。议程中的每个agenda都对应着用户要实现的一个未决意图，它们的优先级由议程堆栈的“后进先出”(first-in-last-out)操作隐式地决定。因此，agenda提供了一种方便的方式编码的历史的对话和用户的“心理状态”。对用户的模拟可以归结为在每次对话结束后，当更多的信息被披露时，如何维护议程。机器学习或专家定义(expert-defined)的规则可用于在堆栈更新(stack-update)过程中设置参数。 Model-Based Simulation Another approach to building user simulators is entirely based on data. ex: (Eckert et al., 1997; Levin et al., 2000; Chandramohan et al., 2011). Asri et al. (2016) 与agenda-based的方法类似，模拟器还会以随机生成的用户目标和约束作为开始。这些是在谈话中是固定的。 在每一个回合中，用户模型都会输入一系列在对话中收集到的上下文，并输出下一个动作。具体来说，对话的语境包括： 最近的机器动作， 机器信息与用户目标不一致， 约束状态和 请求状态。 有了这些上下文，使用LSTM输出下一个用户话语。在实践中，通过结合基于规则和基于模型的方法来创建用户模拟器，可以表现的更好。 Further Remarks on User Simulation 目前的用户模拟器自身的评估也不明朗，实际一些对话策略会因为过拟合一个特定的对话模拟器而不能适用于真实的用户。 用户模拟器和人之间的差距，是优化基于用户模拟器获得的对话策略的最大的限制。 最近推出的一个更大的评估环境的语料库–AirDialogue Reference:[1]Li, X., Lipton, Z. C., Dhingra, B., Li, L., Gao, J., and Chen, Y.-N. (2016d). A user simulator fortask-completion dialogues. CoRR abs/1612.05688 [2]Li, X., Chen, Y.-N., Li, L., Gao, J., and Celikyilmaz, A. (2017d). End-to-end task-completion neuraldialogue systems. In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP), pages 733–743.[3] Gao,Michel Galley,Michel Galley, Neural Approaches to Conversational AI (2018) ​","link":"/2018/11/08/用户模拟器小结 Today's user simulator/"},{"title":"Curriculum Learning and self-paced learning","text":"1. Curriculum LearningBengio提出了课程学习的概念，来自于人类从易到难的学习过程：将数据看作模型所学习的内容的话，我们自然需要一个合理的课程(curriculum)来指导模型学习这些内容的方式。在课程学习中，数据被按照从易到难的顺序逐渐加入模型的训练集，在这个过程中训练集的熵也在不断提高(包含更多的信息)。 A model is learned by gradually including from easy to complex samples in training so as to increase the entropy of training samples. 课程学习的核心问题是得到一个ranking function，该函数能够对每一条数据样本给出其learning priority (学习该样本的优先程度)。一个确定的ranking function便能够确定一个“课程”，有更高rank的数据会被较早地学习。 一般特定问题的ranking function是靠直觉来定义的，这样的定义方法可能会存在偏差，不够科学，所以引出了self-paced learning。 2. Self-paced learningKumar在2010年提出self-paced learning，通过模拟认知机制，通过先学习简单的、普适性的知识，然后逐渐增加难度，学习更复杂、更专业化的知识课程学习。自步学习从选取最简单的数据子集开始，逐渐加入复杂的数据，从而减少熵值，训练出潜在的权重参数。基本思路是利用损失大小与难易程度之间的对偶关系进行对所学样本的加权。这种加权格式类似于引入隐含变量后的估计推断格式和EM算法格式，使得模型对于数据分布的学习更加稳健 SPL 通过在损失函数中引入变量 来表示样本是否被选择： SPL将curriculum设计为学习的目标函数中的一个正则化项，SPL的优化目标即优化得到一组模型参数w与权重向量v，使得所有数据的加权Loss之和与负的权重向量v的L1范数最小(也存在同时使用L1L2范数的变体[3])。其中 λ 是一个控制参数，反映了当前学习所进行到的阶段(learning pace)， 是调整步速的关键变量， 越小，表明模型还在训练的初始阶段，训练相对简单的样本，随着 增大，模型学习越来越多的样本。 直观来解释一下权重向量v，最常用的是binary (0/1)的形式：其实就是是否选择该数据进入训练/验证集。note that在Loss项前乘了因子vi，所以对于第i条数据，若vi=0则其产生的Loss对于该步的模型参数w更新没有影响。 Reference Bengio, Yoshua, et al. “Curriculum learning.” Proceedings of the 26th annual international conference on machine learning. ACM, 2009. Kumar, M. Pawan, Benjamin Packer, and Daphne Koller. “Self-paced learning for latent variable models.” Advances in Neural Information Processing Systems. 2010. https://zhuanlan.zhihu.com/p/55720313 https://zhuanlan.zhihu.com/p/54025612","link":"/2019/11/01/课程学习Curriculum Learning/"}],"tags":[{"name":"Curriculum Learning","slug":"Curriculum-Learning","link":"/tags/Curriculum-Learning/"},{"name":"NLU","slug":"NLU","link":"/tags/NLU/"},{"name":"Paper overview","slug":"Paper-overview","link":"/tags/Paper-overview/"},{"name":"Multi-task Learning","slug":"Multi-task-Learning","link":"/tags/Multi-task-Learning/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Curiosity","slug":"Curiosity","link":"/tags/Curiosity/"},{"name":"reward","slug":"reward","link":"/tags/reward/"},{"name":"Data Augmentation","slug":"Data-Augmentation","link":"/tags/Data-Augmentation/"},{"name":"meta-learning","slug":"meta-learning","link":"/tags/meta-learning/"},{"name":"Dialogue system","slug":"Dialogue-system","link":"/tags/Dialogue-system/"},{"name":"Latent Variable","slug":"Latent-Variable","link":"/tags/Latent-Variable/"},{"name":"Memory Network","slug":"Memory-Network","link":"/tags/Memory-Network/"},{"name":"Visual Dialog","slug":"Visual-Dialog","link":"/tags/Visual-Dialog/"},{"name":"jupyter","slug":"jupyter","link":"/tags/jupyter/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"},{"name":"Actor-Critic","slug":"Actor-Critic","link":"/tags/Actor-Critic/"},{"name":"A3C","slug":"A3C","link":"/tags/A3C/"},{"name":"User Simulator","slug":"User-Simulator","link":"/tags/User-Simulator/"},{"name":"Self-paced Learning","slug":"Self-paced-Learning","link":"/tags/Self-paced-Learning/"}],"categories":[{"name":"Curriculum Learning","slug":"Curriculum-Learning","link":"/categories/Curriculum-Learning/"},{"name":"NLU","slug":"NLU","link":"/categories/NLU/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Text Classification - Multi-task Learning","slug":"Text-Classification-Multi-task-Learning","link":"/categories/Text-Classification-Multi-task-Learning/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/categories/Reinforcement-Learning/"},{"name":"Dialogue system","slug":"Dialogue-system","link":"/categories/Dialogue-system/"},{"name":"NLG","slug":"NLG","link":"/categories/NLG/"},{"name":"Curiosity","slug":"Reinforcement-Learning/Curiosity","link":"/categories/Reinforcement-Learning/Curiosity/"},{"name":"Visual Dialog","slug":"Visual-Dialog","link":"/categories/Visual-Dialog/"},{"name":"jupyter","slug":"jupyter","link":"/categories/jupyter/"},{"name":"word2vec","slug":"word2vec","link":"/categories/word2vec/"},{"name":"Multi-task Learning","slug":"NLG/Multi-task-Learning","link":"/categories/NLG/Multi-task-Learning/"}]}